[
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_0_20.mp4",
    "text": " Today I'm super excited to talk about a tool that has been a game changer in how we manage an automated cloud infrastructure. Terraform! If you already know Terraform, let me know how you like it in the comments below. I'd love to hear your experience. Now if you're new to Terraform or just curious to learn more about what it can do,"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_20_40.mp4",
    "text": " you're in the right place, so let's get into it. So what is Terraform? And simple terms Terraform is an open source infrastructure as code tool. It allows you to define Cloud infrastructure like servers, databases, and networking using code instead of clicking around in the UI."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_40_60.mp4",
    "text": " creating resources manually, you will write just a script and Terraform does the heavy lifting then for you. Let's talk about the problems of Terraforms offs, right? So imagine you're building an Azure-based data pipeline. Without Terraform, you have to manually create services like data factories."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_60_80.mp4",
    "text": " signups and analytics in storage accounts. This takes time, introduces inconsistencies, and can lead to errors, right? Because it's you that's doing it. Terraform changes the game. When you ride your infrastructure as code, it's reusable and guarantees the same results every time you run it. Whether you're the..."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_80_100.mp4",
    "text": " deploying your pipeline in a test environment or scaling it in production. You can trust that Terraform gives you exactly what you defined, so there are not really surprises. How does Terraform work? It's quite straightforward. You write in your infrastructure definitions in a simple configuration language called"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_100_120.mp4",
    "text": " HCL or HashiCorp configuration language. If you worked with Yamal before you will find HCL very familiar. It's a human readable and easy to learn format. Once you've written your configuration, you use commands like Terraform plan to preview changes and Terraform apply to deploy them."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_120_140.mp4",
    "text": " Plus, it works seamlessly with other providers like AWS, GCP, Databricks, Snowflick, and so on and so on. So it's not just Azure. One of the most powerful features of Terraform is its ability to manage the lifecycle of your infrastructure using its state file."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_140_160.mp4",
    "text": " Terraform tracks every resource it's managing. You need to make changes, Terraform will update only what's necessary. Want to delete and recreate your entire infrastructure? Terraform can do that too. Just update the configuration or use commands like Terraform taint for specific resources."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_160_180.mp4",
    "text": " If flexibility is perfect for testing, disaster recovery or scaling up your infrastructure. Now let's talk about collaboration. When your terraform configurations are stored in a version control system like GitHub, teamwork becomes basically seamless. Two Android colleagues can work together on the same..."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_180_200.mp4",
    "text": " Infrastructure code, review changes through pull requests and track the history of the modifications. And here's where it gets even better. There are form integrates perfectly with GitHub actions, enabling full CI-CD pipelines. This means you can automatically validate, test and deploy your infrastructure changes as part of"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_200_220.mp4",
    "text": " If you're a development workflow, it's a complete solution for efficient automated infrastructure management. If you're wondering how to get started with Terraform, I've got something that might just be perfect for you. Over at my academy at LearnDataEngine.com, I've created a hands-on project called Azure Pipelines"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_220_240.mp4",
    "text": " Terraform. It's designed to guide you through building a real-world automated data pipeline on Azure, basically step by step. This hands-on training is all about real-world skills. You'll extract data from APIs, process it using Azure tools, and even implement architectures like the Lakehouse and"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_240_260.mp4",
    "text": " medallion architecture. By the end you'll not only understand terraform but also have a complete project to showcase your skills plus, we'll dive into setting up CISTD pipelines using terraform and GitHub so your entire workflow from code to deployment is automated and efficient."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_260_280.mp4",
    "text": " Terraform isn't just an automation, it's about helping you focus on what really matters. Solving problems, creating value, if you're ready to lever up your skills, head over to learndataengineering.com and check out the hands-on project. Let's build something amazing. Alright, so..."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_280_300.mp4",
    "text": " Don't forget to like and subscribe if you found this helpful and don't forget to share your experience with Terraform in the comments. I love to hear how you're using it. Until next time, bye!"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_300_311.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_0_20.mp4",
    "text": " Right from the start, Software Development comprised two different departments. The development team that develops the plan, designs, and builds the system from scratch, and the Operation Team for testing and implementation of whatever is developed. The Operations Team gave the development team feedback on any bugs that needed fixing, and any rework."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_20_40.mp4",
    "text": " required. Inveribly, the development team would be idle awaiting feedback from the operations team. This undoubtedly extended timelines and delayed the entire software development cycle. There would be instances where the development team moves on to the next project, while the operations team continues to provide feedback for the previous code."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_40_60.mp4",
    "text": " weeks or even months for the project to be closed and final code to be developed. Now, what if the two departments came together and worked in collaboration with each other? What if the wall of confusion was broken? And this is called the DevOps approach. The DevOps symbol resembles an infinity sign suggests"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_60_80.mp4",
    "text": " that it is a continuous process of improving efficiency and constant activity. The DevOps approach makes companies adapt faster to updates and development changes. The teams can now deliver quickly, and the deployments are more consistent and smooth. Though there may be communication challenges, DevOps manages a streamlined flow between the teams."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_80_100.mp4",
    "text": " and makes the software development process successful. The DevOps culture is implemented in several phases with the help of several tools. Let's have a look at these phases. The first phase is the planning phase, where the development team puts down a plan, keeping in mind the application objectives that are to be delivered to the customer. Once the"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_100_120.mp4",
    "text": " plan is made, the coding begins. The development team works on the same code and different versions of the code are stored into a repository with the help of tools like Git and Merge when required. This process is called version control. The code is then made executable with tools like Maven and Gradle in the build stage. After the code is successful,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_120_140.mp4",
    "text": " built. It is then tested for any bugs or errors. The most popular tool for automation testing is Selenium. Once the code has passed several manual and automated tests, we can say that it is ready for deployment and is sent to the operations team. The operations team now deploys the code to the working environment. The most"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_140_160.mp4",
    "text": " Those prominent tools used to automate these phases are Ansible, Docker, and Kubernetes. After the deployment, the product is continuously monitored, and Nagios is one of the top tools used to automate this phase. The feedback received after this phase is sent back to the planning phase, and this is what forms the core of the DevOps lifecycle."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_160_180.mp4",
    "text": " that is the integration phase. Jenkins is the tool that sends the code for building and testing. If the code passes the test, it is sent for deployment, and this is referred to as continuous integration. There are many tech giants and organizations that have opted for the DevOps approach. For example, Amazon, Netflix, Walmart,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_180_200.mp4",
    "text": " Facebook, and Adobe. Netflix introduced its online streaming service in 2007. In 2014, it was estimated that a downtime for about an hour would cost Netflix $200,000. However, now Netflix can cope with such issues. They opted for DevOps in the most fantastic way."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_200_220.mp4",
    "text": " Netflix developed a tool called the Simeon Army that continuously created bugs in the environment without affecting the users. This chaos motivated the developers to build a system that does not fall apart when any such thing happens. So, on this note, here is a quiz for you. Match the DevOps tool with the phase it is used in."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_220_240.mp4",
    "text": " A. B. C. D. None of the above. Today, more and more companies lean towards automation. With the aim of..."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_240_260.mp4",
    "text": " reducing its delivery time and the gap between its development and operations teams. To attain all of these, there's just one gateway. DevOps! If you enjoyed this video, if you did, a thumbs up would be really appreciated. Here's your reminder to subscribe to our channel, and to click on the bell icon for more on the latest technologies and"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_260_279.mp4",
    "text": " trends. Thank you for watching and stay tuned for more from SimpliLearn."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_0_20.mp4",
    "text": " Meet Emma, a graphic designer working on a new project. One day, her colleague mentions a tool that helps create designs, images, and text using AI. Intrigued, Emma wonders how AI can create something from scratch. Her curiosity grows, and she decides to dive deeper into this tool."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_20_40.mp4",
    "text": " new technology called Generative AI. Generative AI refers to a type of artificial intelligence designed to create new content, such as text, images, music, and videos. Unlike traditional AI, which analyzes or categorizes data, Generative AI produces original content based on patterns learned."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_40_60.mp4",
    "text": " from vast data sets. Essentially, it generates new unique material. These models are often trained on large amounts of data and use sophisticated algorithms to mimic human creativity. Tools like Chad G.P.T. or Dolly can create art, write essays, or simulate conversations by generating output"
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_60_80.mp4",
    "text": " based on user prompts. Generative AI has a wide range of applications, content creation, tools like GPT-4 generate text, logposts, stories, and essays from simple prompts. Art and design, AI models such as Dolly, generates unique images and designs based on text descriptions."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_80_100.mp4",
    "text": " Transforming Creativity in Art. Music in Audio. AI can compose music or replicate voices, offering new possibilities for musicians and audio engineers. Healthcare. Generative AI simulates disease progression or creates synthetic medical data, helping doctors gain fast-food."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_100_120.mp4",
    "text": " insights for research. Let's take image generation as an example to explain how generative AI works. Data collection and learning. AI models like Dolly are trained on large data sets of images paired with text descriptions. These data sets teach the model to recognize different objects, colors, styles,"
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_120_140.mp4",
    "text": " and how to associate text with corresponding images. The more data the AI learns from, the better it can generate accurate and diverse images based on user prompts. Neural networks and transformers. When Emma inputs a prompt like a cat wearing sunglasses, the transformer model processes the text."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_140_160.mp4",
    "text": " Recognizing words like cat and sunglasses and links them to images that learn from during training, transformers help the AI decide how to combine these elements into a coherent image. Tokens in context. The text input, such as a cat wearing sunglasses, is split into smaller parts called tokens. The AI process is a very important thing."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_160_180.mp4",
    "text": " processes each token and understands their relationship. For instance, it knows the sunglasses should be placed on the cat, creating a contextually accurate image. Feedback mechanism. Generative AI models improved through feedback. After generating an image, users provide feedback on the accuracy or quality."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_180_200.mp4",
    "text": " of the output. If M is generated image shows the sunglasses floating beside the cat, she can mark it as incorrect. The model uses this feedback to improve future image generations. Reinforcement Learning Reinforcement Learning further enhances the AI's ability. The model is rewarded when it generates accurate images."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_200_220.mp4",
    "text": " and corrected when it makes mistakes. For example, when MED describes a sunset and the AI produces a vibrant sunset image, it receives positive reinforcement. Over time, this method refines the model's ability to generate better images. Data science and AI models. Data scientists curate"
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_220_240.mp4",
    "text": " the training data and define the parameters that help the AI generate accurate images. The more varied the data set, the more versatile the AI becomes in generating diverse types of content. Advanced models use billions of parameters, which are settings that guide the AI in processing data and generating outputs. Generating original content."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_240_260.mp4",
    "text": " It's trained the model can generate original images. For example, in the might describe a futuristic city-skate, and the AI would produce a unique image based on what it learned. The generated image isn't just a copy of past data, but an entirely new creation, showcasing the AI's ability to combine learn patterns and creativity."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_260_280.mp4",
    "text": " Now, let's have a quick, fun quiz on what we have learned so far. What does generative AI primarily do? A, analyze data. B, generate new content. C, store data. Make sure to let us know your answer in the comments section below for a chance to win an Amazon voucher."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_280_300.mp4",
    "text": " If you are interested in learning AI, then make sure to check out Simply Learn's Generative AI program in association with top universities. We hope that you enjoyed this video and found it informative and exciting. If yes, then we would appreciate a thumbs up. A gentle reminder to get subscribed to Simply Learn and click that bell icon to never miss any update."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_300_301.mp4",
    "text": " from Simply Learn."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_0_20.mp4",
    "text": " Hi, welcome back LearnGender2AI in 5 minutes. So today we talk about how Gender2AI model works. So here let's try to understand how Gender2AI models are working."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_20_40.mp4",
    "text": " So here you will find users and roles in multiple domains, be it a banking domain, in students domain, manufacturing domain, any kind of domain, a people that means users, roles will be having a lot of doubts, questions, issues, concerns. They should be able to solve."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_40_60.mp4",
    "text": " solve their problems. So how are they going to solve problems with the help of gendered way? This is where they prepare prompts in natural language. So they can prepare any type of prompt. They can randomly generate prompts. But to prepare prompts there is some technology."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_60_80.mp4",
    "text": " There is some methodology, there are some techniques. That is where we call it as a prompt engineering. Thus just with this prompt engineering, we have a lot of new job opportunities. These prompt engineering prompts will be conjured by gender-rear models. That is where we talk about..."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_80_100.mp4",
    "text": " about foundation models which are already trained models. Here we don't need to train any model like machine learning. Here you get already trained models, pre-trained models. These are called foundation models. These genie models will consume the prompts given by the users on different kind of roles and generate."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_100_120.mp4",
    "text": " new content, new text, new video, new audio, new code, image. This is the working model of any generative AI engine. Right. So as I told you, foundation models, these are pre-trained models and massive amounts of data sets. Data sets."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_120_140.mp4",
    "text": " will contain huge amount of data that huge amount of data is gathered on social media, wiki media, blogs. So this information will be passed to the models and models will get trained on this data and supplied to you to prepare and apply."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_140_160.mp4",
    "text": " consume prompts and generate new content for you. So, our generator via models we work based on the transformer architectures. In further sessions we are going to discuss what is the transformer transformer architecture. So, the first released models are large language models."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_160_169.mp4",
    "text": " These works only on text data, but now the latest model is multi-model model. These models will work on text image, audio and..."
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_0_20.mp4",
    "text": " Hi, welcome back. Learns in day to day in 5 minutes. Today we will talk about types of foundation models. There are many foundation models supplied by different companies, open a hugging phase, Google, there are many GPT."
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_20_40.mp4",
    "text": " models. So here we are going to talk about these GPD models all GPD models are going to work based on transformer architecture. There are task specific models, the generative GIA models there will be using generative adversarial network variational"
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_40_60.mp4",
    "text": " Art and Coder diffusion models and flow models, these are all the different foundation models. So, what is the next expectations of human being? We have discussed what is artificial intelligence, discussed what is machine learning, discussed what is deep learning, what is in the"
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_60_71.mp4",
    "text": " So, for all good we understood complete what is artificial intelligence, what is the next expectations of human being?"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_0_20.mp4",
    "text": " Hey everyone! Today we're diving into a Gentik AI, smart AI that doesn't just respond, it acts on its own. From automating tasks to changing how we use the internet, this tech is the future. Let's get started! So, what exactly is a Gentik AI?"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_20_40.mp4",
    "text": " In simple terms, its artificial intelligence with a goal-oriented mind of its own. Unlike traditional AI that only responds to prompts, agentic AI can plan, decide, and execute tasks by itself. Imagine asking an AI to book a vacation, and it handles searching flights, comparing hotels, and even e-"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_40_60.mp4",
    "text": " emailing you the itinerary. That's the power of agents. They're intelligent, autonomous, and multi-step problem solvers. You might wonder, why are we talking about Agentic AI now? Well, three big reasons. First, AI today isn't just about text. It's."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_60_80.mp4",
    "text": " model. That means it can understand images, videos, voice, and more. Second, large language models like GPT40, Clawed 3, and Gemini have made it possible for AI to grasp complex tasks. And third, there's growing demand for automation at scale, whether you're a business or an..."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_80_100.mp4",
    "text": " individual. All this sets the perfect stage for AgentikaI to thrive. AgentikaI is already being used in real-world scenarios. In software development, tools like AlphaVolve can generate code on their own. In customer support, smart agents handle queries and escalate issues."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_100_120.mp4",
    "text": " without human help. Google is even testing AI agents inside search that can complete tasks for you. And for personal productivity? These agents can organize your calendar, send reminders, do research, all without you lifting a finger. Now let's talk pros and cons."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_120_140.mp4",
    "text": " The benefits are clear, agentic AI saves time, scales efficiently, and reduces errors. But it's not all smooth sailing. There are real concerns, reliability, cybersecurity risks, job disruption, and of course, ethical boundaries. What happens if an agent makes the wrong decision?"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_140_160.mp4",
    "text": " Who's responsible? These are questions we must tackle as the tech advances. You don't need to be a huge company to start using Agentic AI. As a content creator, you could use an AI agent to help write scripts, edit videos, and even manage comments. As a developer, look!"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_160_180.mp4",
    "text": " code platforms let you build your own agents. For businesses, internal help desks and customer support can be powered by AI. The possibilities are endless and accessible. Thanks for watching. Don't forget to like, share, and subscribe for more tech tips and"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_180_183.mp4",
    "text": " updates. See you in the next video."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_0_20.mp4",
    "text": " To understand AI agents, let's first look at how they differ from traditional software. Traditional software is built using programming languages and follows a fixed set of instructions. It takes an input, processes it in the same way every time, and produces a predictable"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_20_40.mp4",
    "text": " output. It can interact with external systems like databases, file systems, or APIs, but always in a deterministic predefined way. Compare that to generative AI applications powered by large language models like chat GPT, Cloud, Gemini, or D."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_40_60.mp4",
    "text": " These are advanced systems that take input in natural language or even in other data types like voice images or structured data. They pass that input through a neural network and generate a response. And like traditional programs, they don't always act the same."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_60_80.mp4",
    "text": " way. They're not deterministic. These one-shot interactions are incredibly useful, but they come with major limitations. Their knowledge is frozen at the time they were trained, and they can't interact with their employment the way traditional software does. That's where"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_80_100.mp4",
    "text": " AI agents come in by giving LLM's access to external systems and data, agents and lock a much wide range of real-world applications. In simple terms, an AI agent is a digital system that operates autonomously within an environment."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_100_120.mp4",
    "text": " It usually performs three core functions, perceived environment by accessing data, sensors, or inputs, decide using an internal reasoning engine to plan actions toward a goal, act by using tools to perform tasks."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_120_140.mp4",
    "text": " the real world. This is called the Perceive Decide Act Loop, Inc. with smart robot vacuum. It perceives the world using sensors. It decides how to navigate a room and it acts by moving and vacuuming. The idea of agent is new. In fact, it was a major"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_140_160.mp4",
    "text": " topic in the 90s and early 2000s, but back then we didn't have reasoning models that were smart enough to handle complex problems. LLMs changed that. They unlocked new levels of reasoning, planning and context in the standing. So how do modern agents work? At its cost."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_160_180.mp4",
    "text": " or an AI agent allowed generative AI models to interact with external tools. This interaction is achieved by an orchestration or engine component that manages the agents instructions and goals, tools calling and optionally gives it access."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_180_200.mp4",
    "text": " to short and long term memory. These tools allow the agent to interact with their environment. For example, they can read and write data from files or databases, search to web or interact with online forms, call APIs, access code basis to generate or update software."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_200_220.mp4",
    "text": " communicate with physical devices like camera, smart sensor, other hardware. Now, when should we use agent? Agents are ideal when we need autonomy, complex reasoning, tool use and adaptability. Think of these processes, for example. Custom"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_220_240.mp4",
    "text": " support sales funnels, employees hiring, etc. These are examples of complex use cases that can't be fully automated by classical approaches. They require a level of smart that are better suited for AI agents. In the industry, you framework..."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_240_260.mp4",
    "text": " like Lank Chain and Lank Graph, AutoJane, Cruoii and Pidentic AI make it easier to build agents. This is very exciting, but it's not without challenges. L&Ms are powerful but unpredictable. They can hallucinate facts or f"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_260_280.mp4",
    "text": " misuse tools in ways that cause system failures. LLMs are computationally expensive and multi-step planning increases both runtime and cost if an agent takes 20 steps to solve a problem that costs adds up very quickly. Given agents access to real systems."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_280_300.mp4",
    "text": " DataBases, Devices, User Accounts, Introduces serious risks. What if an agent deletes critical data or leaks private information? Ultimately, agents will become seamless assistance embedded in apps, workflows and physical devices."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_300_320.mp4",
    "text": " We just need to make sure they don't cause harm along the way."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_320_322.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_0_20.mp4",
    "text": " Hey what's up everybody? Today I'm diving into something super exciting, a genetic AI and how it's going to transform our world. First things first, what's a genetic AI? Well it's a type of artificial intelligence designed to act with a certain level of autonomy. Unlike traditional AI which follows pre-programmed..."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_20_40.mp4",
    "text": " instructions. Agenetic AI can make decisions on its own to achieve specific goals. Think of it as having a digital assistant that doesn't just follow orders, but can actually think about the best way to help you. Now why is this so game changing? Let's break it down. Imagine factories where a Genetic AI optimizes production."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_40_60.mp4",
    "text": " lines reduces downtime and ensures quality control. Your AI assistant doesn't just follow a set schedule. It adapts in real time to changing conditions, making the entire process more efficient. In mining, agentech AI can manage autonomous equipment, ensuring safety and maximizing resource extraction."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_60_80.mp4",
    "text": " That's the kind of seamless experience we're talking about, but it goes beyond just efficiency. In the defense sector, agente AI can be used for surveillance, threat detection and even autonomous drones, making operations safer and more effective. We're talking about saving lives and enhancing security, and what about other industrial applications?"
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_80_100.mp4",
    "text": " A gentigay eye can revolutionize how industries operate. These systems can communicate with each other to optimize processes, reduce waste and increase overall productivity. Imagine never having to deal with unexpected downtime. Sounds like a dream, right? Moving to other sectors, a gentigay eye can revolutionize."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_100_120.mp4",
    "text": " how companies operate. From automating mundane tasks, to making strategic decisions based on real-time data, this tech can free up human workers to focus on more creative and complex problems. It's like having the smartest, most efficient team member you could ever imagine. But with great power comes great responsibility."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_120_140.mp4",
    "text": " As we embrace a GENTIC AI, we also need to consider the ethical implications. How do we ensure these systems act in our best interest? What about privacy and security concerns? These are questions we need to address as we move forward. So, in a nutshell, a GENTIC AI isn't just another tech buzzword. It's a revolutionary..."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_140_160.mp4",
    "text": " advancement, etc. to impact every facet of our lives from home and health care to transportation and business. The future is closer than we think and it's going to be incredible. Alright, that's it for today. If you found this video insightful, give it a thumbs up and don't forget to hit that subscribe button for more tech deep dives. Thanks for watching and I'll see you next time."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_160_163.mp4",
    "text": " catch you in the next one."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_0_20.mp4",
    "text": " Hi there. Let me show you how data breaks changes data engineering by giving you everything you need in one AI-infused intelligent platform. And the one governance layer, the Data Intelligence Platform handles it all. Streaming in Chess-Tune."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_20_40.mp4",
    "text": " transformation and orchestration, add a fraction of the typical development time and on top of serverless compute. As you might know, any new project starts with data in Chesschen, in Chesting Data from Sources, such as Cloud Storage, Message Buses, Databases, or ERMs only takes me..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_40_60.mp4",
    "text": " minutes. Lakeflow Connect seamlessly and continuously imports your data from popular business applications such as Salesforce, Workday and Google Analytics. With Lakeflow Connect, it just takes a few clicks and your data appears as fully synchronized tables in your catalog."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_60_80.mp4",
    "text": " ready to be used across the organization. And of course, there are other options too, like getting datasets or even ML models from Databricks Marketplace and partners. After interesting it, our data is now available and governed by Unity Catalog. So we can start transforming the raw data and extract..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_80_100.mp4",
    "text": " meaningful information. In the past, this required writing complex Python code and applying heart to understand optimization techniques. But now we have the Data Intelligence platform, which is infused with AI that understands your interactions with the data, it's metadata."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_100_120.mp4",
    "text": " and your code. Databricks Assistant saves your team tons of development time and keeps everyone following best practices. Here I created a single materialized view that automatically maintains fresh pre-computed data, making it ideal for dashboards. With Delta Life Tanks."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_120_140.mp4",
    "text": " You scale these transformations into robust production grade pipelines, written in simple SQL or Python. Let's look at a data life tables pipeline in action. Here, in the pipeline view, you can see multiple tables and transformation steps. It provides a clear visualization of your data flow."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_140_160.mp4",
    "text": " Also, data reliability is guaranteed by continuous data quality monitoring and status tracking in real time. Best of all, your pipeline adapts to your needs. You can easily switch between batch and streaming to match the pace of your business. Well, you have now seen..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_160_180.mp4",
    "text": " how Databricks Streamlines individual tasks. But these steps are for only building blocks of a larger workflow. This is where Databricks workflows comes in, a fully serverless orchestration engine that orchestrates everything in the lake house and seamlessly coordinates all the required steps."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_180_200.mp4",
    "text": " such as triggering D9 gesture or transformation pipelines, orchestrating ML model updates, or automating dashboard refreshes for a business team. To summarize, think of the D9 Intelligence platform as you want to stop platform for D9 engineering."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_200_220.mp4",
    "text": " You can enchest data with Lakeflow Connect, transform it using simple SQL pipelines with Delta Life tables, and orchestrate every step in your data journey with Databricks workflows, all on one secure and serverless platform. Are you ready to transform your data engineering?"
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_220_230.mp4",
    "text": " Start today at dtabrics.com slash trial. Thanks for joining me in this data engineering overview."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_0_20.mp4",
    "text": " Data lineage is the history of how data is ingested, transformed and stored within a data pipeline. This is powerful information that you can leverage to make informed decisions about your architecture."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_20_40.mp4",
    "text": " One simple way to gather this information would be to interview the data engineers across your pipeline and ask which data sources they read from, what they do with the data, and where the data goes next. Then you could map out a data flow diagram like this toy example. Data is pulled in from sources here into some stores."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_40_60.mp4",
    "text": " like this data lake and this bucket, transformed by an ETL and loaded into a data warehouse. This is then used to populate a set of dashboards. One potential use case is risk modeling."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_60_80.mp4",
    "text": " For example, we can ask the question, what's affected if this store goes down? Following through the diagram, we see a knock on effect through the data warehouse and onto this dashboard. Now, the obvious downside to collecting your lineage data by interviews is that it's expensive time-consuming error pro."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_80_100.mp4",
    "text": " an out of date almost as soon as you're done collecting it. So maintaining it at a useful level of granularity would take an unreasonable amount of time. Clearly, we need to automate our data lineage. Digital tools, such as DBT, offer static analysis of data pipelines."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_100_120.mp4",
    "text": " These pass queries to map the flow of data through a system. This requires that all data movement and transformation takes place within the tool. So, if you were copying this CSV to this book outside the platform, it would be invisible to the analysis. Some platforms search a data..."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_120_140.mp4",
    "text": " bricks offer runtime automated lineage. These maintain a metadata store which records links between services when processes move data. This works well for common use cases but may not support niche operations. And again any operations carried out on"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_140_160.mp4",
    "text": " platform will be lost. Another option is data tagging. If all data transformers in your architecture conform to some schema then they can read in and extend data lineage metadata alongside transforming the data. So this ingest"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_160_180.mp4",
    "text": " could tag everything it ingests with the source system. Then this ETL could add further tags to show that it read data from this store, transformed it, and stored it in this data warehouse. This is handled automatically in services like Microsoft Pervue, but could also be done manually. Now it's..."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_180_200.mp4",
    "text": " that each transformer in the pipeline would need to update the metadata which can be expensive to implement. However, this granular detail means that by the time we reach figures in this dashboard, we know exactly where each one came from and how it was processed. Now we have"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_200_220.mp4",
    "text": " a granular up-to-date data lineage. Let's look at some other use cases. First, migration planning. Can I rename a table in the data warehouse? I met a data tells us which teams would need to be informed and whether some data is completely unused. Next,"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_220_240.mp4",
    "text": " compliance and provenance. How can we verify the sorting correctness of this data? Being able to easily obtain a full list of data transformations applied is an order to the dream. And this is especially important to our clients in highly regulated industries like health and finance. Finally, root cause."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_240_260.mp4",
    "text": " analysis. Let's say this graph is wrong. Why? For a given data point we can see its complete journey through our pipeline, so for example we could detect if our ETL was not deduplicating correctly. This allows critical issues to be investigated and resolved as quickly as possible."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_260_280.mp4",
    "text": " Ultimately, maintaining data lineage is essential to understanding the flow of data through a business. The best approach depends on the tooling you employ. If supported, static analysis will yield good results for minimal effort."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_280_300.mp4",
    "text": " look into whether your platform offers runtime automated lineage. If neither of these is possible, then data tagging may be the best way forwards. As ever, you'll need to consider the realities of your existing architecture with what you want data lineage to achieve for you."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_300_306.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_0_20.mp4",
    "text": " Data is everywhere, but you already know that. But do you know who's responsible for all of this data? Well, it's the data engineer. There are lots of roles like the data analysts, data scientists and machine learning engineer that use data. But for them to be able to use this data and actually make it useful, somebody needs to build a system to collect"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_20_40.mp4",
    "text": " and store it. And that is where a data engineer comes in. The thing is that it kind of works like a pyramid. The data engineer leads the foundation and if the foundation isn't good, the entire thing is going to collapse. Now hopefully the data engineer of your company is as good as the ones that build the pyramids because they've been around quite a while. So what are the key responsibilities of"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_40_60.mp4",
    "text": " of data engineers. Well, the first thing is data collection. And this basically means that data engineers will create pipelines to store data from various sources. It could be a company database, the data might be available somewhere else, but the purpose here is to get it all into one place where we can really understand it and store it more properly. And I actually,"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_60_80.mp4",
    "text": " it useful. And that brings us on to the second part, which is data storage. It is really important that data is stored securely and in a matter that makes it accessible for other people in the organization that want to use this data, for example, to make important decisions and conclusions. Now we can't always store and use the data right away, so we may have to transform this data."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_80_100.mp4",
    "text": " data. And this is also a key responsibility of data engineers. They make clean and transform the raw format into a format that can actually be made useful for the company. It is also all about data availability. And you want data to be available to data scientists and to analysts and machine learning engineers for whatever you need it for. Now let's talk about the skills and tools that"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_100_120.mp4",
    "text": " you'll actually need to become a data engineer. Some of the key tools that you should know are SQL or SQL, used for managing databases, more specifically relational databases, and in general, a strong understanding of database systems and different forms of architecture. It is also helpful to be strong in a program in language, like Python, which is widely used as well."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_120_140.mp4",
    "text": " Now one thing that's becoming more and more important in data engineering is the use of cloud as it's becoming more popular in companies in general. Large data sets are the new deal and therefore we need to know how to deal with them. For that reason it's great to know a cloud solution, for example AWS or Google Cloud Platform or Microsoft Azure. The most popular ones are AWS"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_140_160.mp4",
    "text": " and Microsoft Azure, but there are also a lot of other good alternatives and some of the other tools commonly used are Apache Hadoop for processing large data sets and also Spark for data processing. I recommend this website where you can actually see the skills that our employers are looking for directly on the site and the data is scraped directly from real job listings so that you"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_160_180.mp4",
    "text": " actually know what they're looking for on the real job market. Not just what some random guy on YouTube is telling you, although I do try to base my opinions on real data. Now, how do you break into data engineering? Well, there are of course many, many ways just like with any job and it depends on your prior experience or education or lack of education. Many data engineers will have a bad."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_180_200.mp4",
    "text": " degree at least in, for example, computer science or a related field, but also other degrees like masters can be pretty common as well. Some people go directly into data engineering, whereas other people go from another data role or some other technical role and then transition into data engineering. This might also be better option if you don't have a strong education."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_200_220.mp4",
    "text": " an old background or a solid degree, as it can be quite difficult to get your first job without a degree in data engineering if you're starting out. All in all, I think data engineering is a very exciting field and it's only going to grow in the future and I wish you all the best starting your career in data engineering or perhaps you're more interested in data science or data analytics. Thanks."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_220_231.mp4",
    "text": " watching this video please drop a like and subscribe if you did enjoy it and also somewhere on the screen here there should be another video that I think you might be interested in or at least a YouTube algorithm think so so check it out."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_0_20.mp4",
    "text": " Right from the start, Software Development comprised two different departments. The development team that develops the plan, designs, and builds the system from scratch, and the Operation Team for testing and implementation of whatever is developed. The Operations Team gave the development team feedback on any bugs that needed fixing, and any rework."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_20_40.mp4",
    "text": " required. Inveribly, the development team would be idle awaiting feedback from the operations team. This undoubtedly extended timelines and delayed the entire software development cycle. There would be instances where the development team moves on to the next project, while the operations team continues to provide feedback for the previous code."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_40_60.mp4",
    "text": " weeks or even months for the project to be closed and final code to be developed. Now, what if the two departments came together and worked in collaboration with each other? What if the wall of confusion was broken? And this is called the DevOps approach. The DevOps symbol resembles an infinity sign suggests"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_60_80.mp4",
    "text": " that it is a continuous process of improving efficiency and constant activity. The DevOps approach makes companies adapt faster to updates and development changes. The teams can now deliver quickly, and the deployments are more consistent and smooth. Though there may be communication challenges, DevOps manages a streamlined flow between the teams."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_80_100.mp4",
    "text": " and makes the software development process successful. The DevOps culture is implemented in several phases with the help of several tools. Let's have a look at these phases. The first phase is the planning phase, where the development team puts down a plan, keeping in mind the application objectives that are to be delivered to the customer. Once the"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_100_120.mp4",
    "text": " plan is made, the coding begins. The development team works on the same code and different versions of the code are stored into a repository with the help of tools like Git and Merge when required. This process is called version control. The code is then made executable with tools like Maven and Gradle in the build stage. After the code is successful,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_120_140.mp4",
    "text": " built. It is then tested for any bugs or errors. The most popular tool for automation testing is Selenium. Once the code has passed several manual and automated tests, we can say that it is ready for deployment and is sent to the operations team. The operations team now deploys the code to the working environment. The most"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_140_160.mp4",
    "text": " Those prominent tools used to automate these phases are Ansible, Docker, and Kubernetes. After the deployment, the product is continuously monitored, and Nagios is one of the top tools used to automate this phase. The feedback received after this phase is sent back to the planning phase, and this is what forms the core of the DevOps lifecycle."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_160_180.mp4",
    "text": " that is the integration phase. Jenkins is the tool that sends the code for building and testing. If the code passes the test, it is sent for deployment, and this is referred to as continuous integration. There are many tech giants and organizations that have opted for the DevOps approach. For example, Amazon, Netflix, Walmart,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_180_200.mp4",
    "text": " Facebook, and Adobe. Netflix introduced its online streaming service in 2007. In 2014, it was estimated that a downtime for about an hour would cost Netflix $200,000. However, now Netflix can cope with such issues. They opted for DevOps in the most fantastic way."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_200_220.mp4",
    "text": " Netflix developed a tool called the Simeon Army that continuously created bugs in the environment without affecting the users. This chaos motivated the developers to build a system that does not fall apart when any such thing happens. So, on this note, here is a quiz for you. Match the DevOps tool with the phase it is used in."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_220_240.mp4",
    "text": " A. B. C. D. None of the above. Today, more and more companies lean towards automation. With the aim of..."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_240_260.mp4",
    "text": " reducing its delivery time and the gap between its development and operations teams. To attain all of these, there's just one gateway. DevOps! If you enjoyed this video, if you did, a thumbs up would be really appreciated. Here's your reminder to subscribe to our channel, and to click on the bell icon for more on the latest technologies and"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_260_279.mp4",
    "text": " trends. Thank you for watching and stay tuned for more from SimpliLearn."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_0_20.mp4",
    "text": " Ever wonder how Netflix updates its apps without crashing? Or how Instagram adds new features almost daily? Behind every successful app update is a powerful approach called Dev Ups. Today, we'll break down exactly how modern companies deliver software updates so smoothly."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_20_40.mp4",
    "text": " and reliably. Before DevOps, software development was divided into two separate teams. Developers wrote the code, and operations teams deployed and maintained it. This separation created major problems. Long delays between updates, frequent conflicts between"
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_40_60.mp4",
    "text": " between teams, lots of deployment errors, slow recovery from problems, and frustrated users waiting for fixes. Companies needed a better way to build and deliver software. This is where DevOps comes in. Instead of keeping teams separate, DevOps."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_60_80.mp4",
    "text": " combines development and operations into a single, smooth process. Here's how it works. First, we start with planning and coding. Developers write new features or fix bugs in small, manageable pieces. Next comes the most important part, automation. Before..."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_80_100.mp4",
    "text": " For any code goes live, it passes through a series of automated checks. Code quality tests, security scans, performance tests, integration checks. If everything passes, the code automatically moves to deployment. But here's the clever part."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_100_120.mp4",
    "text": " happens gradually. New code rolls out to a small group of users first. Then, slowly expands if everything works well. The whole time, we're monitoring everything. Application performance, error rates, user experience, and system health. This creates a continuous cycl..."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_120_140.mp4",
    "text": " of improvement, where feedback directly influences the next updates. To make DevOps work, you need the right tools at each stage. Let's break down the most important ones. First, is Git. It's a version control system that tracks every change made to the code."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_140_160.mp4",
    "text": " Think of it as a highly organized record keeper. It stores every version of your code, tracks who made each change, and lets teams works together without overriding each other's work. When something breaks, you can easily see what changed and who changed it. Next is Docker. It packages..."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_160_180.mp4",
    "text": " applications and everything it needs to run into a single unit called a container. This solves a huge problem in software development, making sure your application runs the same way everywhere. No matter where you run the container on your laptop, a test server, or in production, it behaves exactly the same way."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_180_200.mp4",
    "text": " Then we have two types of automation tools, Jenkins or GitHub Actions. These are automation servers that handle repetitive tasks. Every time someone updates the code, these tools automatically build the application, run all the tests, check for security issues,"
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_200_220.mp4",
    "text": " to test environments, and if everything passes, deploy to production. Finally, Kubernetes. This is what we call a container orchestration platform. It manages your application and production by automatically scaling when more users are active, healing itself when something fails."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_220_240.mp4",
    "text": " Managing updates without downtime, balancing the workload across servers, and monitoring application health. What does all this mean in practice? With DevOps, companies can now release updates multiple times per day. Catch problems before they affect you."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_240_260.mp4",
    "text": " users, fix issues in minutes instead of days, scale applications instantly when needed, and keep system secure and reliable. For users, this means more frequent app updates, fewer crashes and bugs, new features arrive"
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_260_280.mp4",
    "text": " better app performance and quicker problem resolution. Let's see how companies use DevOps in the real world. Take Netflix. They can update their app while millions are streaming. When they added, are you still watching feature? They first tested it with just 1% of users."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_280_300.mp4",
    "text": " Monitor the response and gradually roll it out to everyone. Or your banking app. It updates security features weekly without ever losing your data. It's like changing a car's tires while driving. Even gaming companies like Fortnite can add new features while millions are playing and roll back instantly."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_300_320.mp4",
    "text": " something goes wrong. That's the power of DevOps. Constant improvement without disruption. Now you know the secret behind those smooth app updates. Modern software delivery isn't about working harder. It's about working smarter with the right processes and tools."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_320_338.mp4",
    "text": " Next time, your favorite app updates, you'll know there's an entire DevOps system working behind the scenes to ensure everything runs perfectly. Thanks for watching! If you want to learn more about modern technology, hit subscribe for new videos every week."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_0_20.mp4",
    "text": " Alright, let's talk about something that has completely changed the ways software is built and delivered. Devops. Now I know what are you thinking? Oh no, another complicated tech birth world. But trust me by the end of this video, you will not only understand what DevOps is, but you will also see why every modern company needs it."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_20_40.mp4",
    "text": " And don't worry, I am going to explain it with a real world story, so it will actually make sense for you. So imagine you and your friends are organizing a college event. Big annual fest."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_40_60.mp4",
    "text": " happens in almost all the colleges. So one team is a planning team and we can call them developers. Another team is setting up the venue, doing all the lights and all sound stuff. We can call them operations. Now let's say the event team plans an awesome concert and sends the plan to the venue team at the last minute. But guess what?"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_60_80.mp4",
    "text": " The venue team was ready. The stage is in setup, the lights are broken, there is no sound system. So what will happen? Chaos, right? The event will get delayed, everyone blames each other, this will be the situation. And the audience will..."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_80_100.mp4",
    "text": " get frustrated. And this is how software development used to work before. Developers would write code, build new feature and then throw it to IT team saying, hey, now it's your job. But when the app is crashed or when it didn't work, the IT team would be like, oh well, this is not my problem. You have it in the back."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_100_120.mp4",
    "text": " bad code. It's your problem. So it will be like a blame game situation and it will lead to slow releases, frustrated user, clients and that was the old way. Now the solution for this. Imagine instead of even team and when you team working without each other they will work together from the start."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_120_140.mp4",
    "text": " They will communicate constantly, the venue team starts setting up as soon as the event team finalizes the plan, they test the sound, light, stage, before hand, before the event day and make sure that everything is working perfectly. Now everything runs smoothly and the audience is happy and the event is a success."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_140_160.mp4",
    "text": " So, this is exactly what DevOps does in the tech world. DevOps means developers and operations. So, developers and operations work together instead of separately. It is about collaboration. No more blame games. Dev and IT team should work as a team. It is about automation."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_160_180.mp4",
    "text": " Task like testing development happens automatically. It is about faster and more reliable releases. Apps are updated quickly without breaking. And trust me, this makes the software development faster. It makes the software development smoother and much less stressful. But how does this dev-off-"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_180_200.mp4",
    "text": " actually work, how does this happen in real life if I say that there are three main things which DevOps team will do. Number first continuous integration we call it CI. So think of this like a constantly telling small part of event setup. So like problems are caught early. So in tech means that developer keep."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_200_220.mp4",
    "text": " emerging updates and you know the system checks for the issue automatically and it's like constantly making sure that the food and like the food is fresh the lights are working the sound is clear so everything we check every r or something okay process will be very smooth and comes c i cd pipeline this is the heart of DevOps it's"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_220_240.mp4",
    "text": " It's like a smooth assembly line where all these checks and updates happens automatically without human intervention. So here's how it works. I'll tell you. Number first, developers write new code. Number second, the CI part automatically tests the code for the bugs. If there is any bug or anything. So if the code passes that..."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_240_260.mp4",
    "text": " test, the CD pipeline deploys it to the production. Production means you'll see in the live server where the clients are using it. For the users to see is basically. And then the system monitors the app real time to catch any errors or any bug earlier. So basically CI CD pipeline is like a robot chef who makes"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_260_280.mp4",
    "text": " sure that everything runs smoothly without any human mistake and it gets the app to user quickly. And this is why big companies like Netflix, Amazon, Google, every company like these runs on DevOps. And it keeps everything running faster smoothly and the users are"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_280_300.mp4",
    "text": " happy, always happy. But then why should you care about DevOps? That's the question I'm a developer why should I care about DevOps? If you're just starting in tech you might be wondering like that but I'll tell you why companies love DevOps engineer. It's one of the highest paying tech jobs."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_300_320.mp4",
    "text": " It makes your work easier. No more it will be like it works on my machine excuses. You don't have to be hard code for this. DevOps is more about smart automation than writing a complex algorithm. So whether you are a developer, whether you are a system admin,"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_320_340.mp4",
    "text": " whether you are someone who wants to work in tech. Learning DevOps will open a lot of opportunities for you. So next time if someone says DevOps, just remember it's like running a big event smoothly with a good teamwork and automation. So no last minute panics, no blame games, just a fast and a smooth"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_340_356.mp4",
    "text": " for everyone. And that my friend is DevOps. Thank you so much for watching this video till the end. I hope you like this video. If you really like it share it with your friend who are looking for the possibilities and make sure to drop a comment about how the video was. I'll see you in the next one."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_0_20.mp4",
    "text": " We are predicting the company name by providing the input text. Hello all, welcome back to the Cloud and Data Science. As we all know, large language models is the hard topic in today's industry. In this video, we are going to see how to integrate LAN chain with large language models."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_20_40.mp4",
    "text": " using Azure OpenAI or OpenAI. Langchain provides a framework for developing the applications using the Logge Language models. It provides easy to use abstraction for working with Logge Language models. Logge Language model provides various components, various modules."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_40_60.mp4",
    "text": " to consume the language models using Azure OpenAI or OpenAI. The following are the modules as we see in the screen, model interface. So it provides an IOW interface to integrate and interact with the language models. Azure through Azure OpenAI"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_60_80.mp4",
    "text": " or WAPIN AI. The other important feature is data connection. Currently, large language models are trained on the generic data. So it provides interface to interact integrate with application specific data, domain specific data so that we can get more domain space."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_80_100.mp4",
    "text": " weak accurate predictions and results. And lying chain also provides construct to for the sequence of the calls and it provides memory to store the state between runs of a chain. Also it provides"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_100_120.mp4",
    "text": " to log these steps in between the chain. So the two important features I like one is domain specific data to integrate with domain specific data and the other one is state maintenance between the chains of the calls and sequence of the calls."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_120_140.mp4",
    "text": " As I mentioned, these are the important components in the Lang chain. It provides prompts and once we got the response output, it provides output process to parse the their X data and it also provides various example selectors and it provides documents."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_140_160.mp4",
    "text": " load us to load the documents and text splitters to split the text and it provides memory to store the state between the chain of calls and it provides various models access to various models and it also provides vector stores to store the data in the form of vectors and"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_160_180.mp4",
    "text": " and it provides retrievers to retrieve the application specific data or domain specific data. These are the some of the use cases specific to the chains. It helps to build the agents, chatbots and question answering systems. And it provides ablur data analysis."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_180_200.mp4",
    "text": " And it also provides natural language APIs to integrate and interact with various natural language models. And it also provides a provision to evaluate the results and it also provides way to extract the data and to summarize the data. These are some of the use cases specific to the language."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_200_220.mp4",
    "text": " Now let us see how to use this Lang chain to build our first LLM application. As we seen the screen there are four steps. First we need to install the Lang chain using the pip install command and second step is"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_220_240.mp4",
    "text": " we need to install the open API again using pip install open API, open a and then once we have the open AI key, we need to get our own open AI key after registering with open AI, then we will initialize the open AI LLM class to start our predictions. We will provide the"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_240_260.mp4",
    "text": " input parameter to the LM class and get the output as a result. So let us see those four steps in action. Go to that your VS code or any editor. So first step as I mentioned we have to install the"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_260_280.mp4",
    "text": " Langchain once we have the Langchain installed then we have to install the OpenA Then once we have the OpenA then Third step is we need to get the key. How do we get the OpenA key? Simply you have to go and register for the OpenA and there you can create new secret key"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_280_300.mp4",
    "text": " copy that new secret key and then paste that new secret key here. Once you have the key then all you need to do is you need to import that open-A wrapper from the Langchen. Langchen provides the wrapper to the open-A. You can also use Azure Open-A once you have"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_300_320.mp4",
    "text": " Now that Openier app, then you can initialize the large language model class, LLM class, and then you can call predict method, LLM dot predict. In this example, what we are predicting is what would be a good company name for a company that makes colorful socks, that is the input parameter to the predict method."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_320_340.mp4",
    "text": " nautotis feedfull of fun this is the output, this is the company name. So, this is the simple application what we are doing is we are predicting the company name by providing the input text using the large language model and by using the lang chain"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_340_355.mp4",
    "text": " interface to integrate with the WAPINYA. Thanks for watching. In the next video, we are going to see more in detailed steps of use cases. Thanks for watching. Please don't forget."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_0_20.mp4",
    "text": " Welcome back to BraveHub. In today's video, I'm going to teach you the basics of Leng Chain in under 5 minutes. If you're looking to quickly get started with this powerful tool for building language model applications, you're in the right place. Let's jump in. Leng Chain is a framework designed to simplify the development of applications."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_20_40.mp4",
    "text": " powered by large language models, LLMs, it's perfect for creating chatbots, knowledge-based systems, and other AI-driven applications. What makes Lengchains stand out is its ability to chain together multiple components, like prompts, memory and tools, making the development process much more efficient. First,"
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_40_60.mp4",
    "text": " First, let's install Leng Chain. Open your terminal and run the following command. Pipp install Leng Chain. This command will install Leng Chain and its dependencies. Once that's done, you're ready to start building. Let's create a simple Leng Chain that combines a prompt with an LLM to generate text. Here,"
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_60_80.mp4",
    "text": " Use how it's done. In this example, we're creating a Leng Chain that takes a name as input, and generates a short poem about that person, using OpenAI's GPT3. The LM Chain object combines the prompt with the LLM, making it easy to generate customized content. One of the powerful features of Leng Chain."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_80_100.mp4",
    "text": " is its ability to add memory to your chains, allowing your application to remember context across interactions. Let's see how that works. Here, we're adding simple memory to our length chain, enabling the application to retain information between runs. This is useful for building more interactive and context-aware applications."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_100_120.mp4",
    "text": " LangChain also supports integrating tools like web search, calculators and databases, making your applications even more powerful. Let's add a simple tool to our chain. In this example, we're using a simple calculator tool within our LangChain. The chain now not only generates text, but can also perform calculation."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_120_140.mp4",
    "text": " making it a more versatile solution. And that's a quick overview of Leng Chain in under 5 minutes. We covered how to install Leng Chain, create a simple chain, add memory, and integrate tools. Leng Chain is incredibly powerful, and I encourage you to explore its full capabilities. If you found this video helpful,"
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_140_157.mp4",
    "text": " Please give it a thumbs up, subscribe for more quick tutorials, and hit the notification bell so you never miss an update. Got questions or ideas for future videos? Drop them in the comments below. Thanks for watching, and see you in the next video."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_0_20.mp4",
    "text": " Hi everyone and welcome back to TechWorld with Preet. In today's video we are diving into a fascinating question. What is Langchin and why should you use it? Langchin is an exciting tool that's gaining momentum in the AI space and by the end of"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_20_40.mp4",
    "text": " this video, you'll not only understand what it is, but also have a strong sense of how it fits into modern AI workflows. Let's get started. Coming back to question what is Langchene and why do we use it in the first place? What exactly is it doing for us?"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_40_60.mp4",
    "text": " Well, answering that question is a big trickier than it seems. So to really understand it, we are going to walk through a short journey across this video and the next few. To make things more concrete, let's look at a real world use case. Have you ever wondered how some apps let you interact with large top?"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_60_80.mp4",
    "text": " documents, like asking questions about the contents of a PDF. We'll start by looking at a public web app called pdf.ai. It's not something I built, someone else did, but it's a great example. Take a look at web app. Next we will understand how this app works."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_80_100.mp4",
    "text": " and we can use that knowledge to better understand Langchain. Let me show you a quick demo. First, sign into PDF.ai with your Google account. Once you logged in, you will be taken to the upload page where you can upload your PDF file."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_100_120.mp4",
    "text": " Icept uploaded a 277 page document about the cracking the technical interview to pdf.ai. On the left you can see the document and on the right I can type questions about it."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_120_140.mp4",
    "text": " design an algorithm to find all pairs of integers within an array which some to a specified value. After a brief pause the app gives me an answer. If you check closely it's getting the information directly from the PDF. What's even cooler is that it shows the exact section where the"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_140_160.mp4",
    "text": " info came from. For example, it says the answer is from page 91. If we search for page 91 in the PDF, we can see the same details as shown in the chat response. So, how does this magic happen? Let's take"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_160_180.mp4",
    "text": " Make a look behind the scenes to understand how an app like this is built. Here's what's happening behind the scenes. Text Extraction. The app reads the PDF and extracts the text, breaking it into smaller sections, embedding scenes generation. Each section is converted."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_180_200.mp4",
    "text": " into vector embeddings, a mathematical representation of text meaning. Question handling. When you ask a question, it's converted into an embedding and the system compares it to the embeddings from the document to find relevant sections. AI response. Finally, check GPT-O."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_200_220.mp4",
    "text": " a similar model processes the relevant sections and generates a response. Next question comes in mind what Lanchine can do. Lanchine is a powerful tool that helps developers build smart AI apps and this image shows all the cool things."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_220_240.mp4",
    "text": " it can do. Let's break it down. Checkports. You can build checkports that talk like humans using real-time data and memory. Agents. These are AI tools that can decide what to do next, like searching the web or calling APIs. Interacting with APIs. Langching can cause..."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_240_260.mp4",
    "text": " connect with other services and fetch real-time info using APIs. Code understanding, it can help explain or work with code, like debugging or code review. Quarring tabular data, Lanching can understand and answer questions from data tables like Excel Sheets."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_260_280.mp4",
    "text": " You can test how well your AI app is performing, extraction. It can pull out specific information from documents or files. Question answering using docs, ask questions. And it finds answers from your files or documents, summarization. It reads long."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_280_300.mp4",
    "text": " content and gives you a short meaningful summary. Why is Lanchine so popular? Think of Lanchine like a kitchen helper for developers. Imagine you're a chef and someone has already chopped the veggies, measured the spices and got everything ready for you."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_300_320.mp4",
    "text": " to do is cook the meal. Lanchine does the same thing for developers. It takes care of all the hard setup. So you can just focus on building your AI app. So that's Lanchine in a nutshell. A framework that turns the complex into the simple making it easier to"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_320_340.mp4",
    "text": " build powerful lm applications. In upcoming videos we'll go deeper into Lengen's features, build real-world projects and even talk about bringing these apps to production. If you found this helpful don't forget to like share and subscribe to TechWorld with breed. Let's get started."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_340_344.mp4",
    "text": " Keep learning and building amazing things together. See you next."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_0_20.mp4",
    "text": " Let's talk about Langchain. What is it? How does it work? Why should you care? And how you can get started today? All of that and more in two minutes. Start the timer. Langchain is an open-source framework designed to make it easier for developers to build powerful applications that interact with language models."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_20_40.mp4",
    "text": " like GPT-4, and Thropix-Clawed, Cohear Models, and Beyond. Before Lengchain building applications with language models often required a lot of custom code to handle tasks like memory management, prompt optimization, and interacting with external data sources. Lengchain simplifies this by offering modular tools that handle these challenges"
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_40_60.mp4",
    "text": " more efficiently. In practice, Lengchain lets you define different components, like memory management and API connections that interact seamlessly with language models. Think of it like Lego for developers. You can snap together the pieces you need to create custom workflows that perfectly fit your application. Now that's great in all, but why should you care?"
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_60_80.mp4",
    "text": " Well first, it's incredibly flexible. Unlike other frameworks, Lengchains modular design lets developers easily integrate various components, whether it's language models, APIs, or custom logic into one cohesive application. This flexibility means you can quickly assemble anything from a chatbot to a document summarizer without reinventing the wheel."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_80_100.mp4",
    "text": " Second, Lengchain makes handling contacts much easier. One of the biggest challenges with language models is their inability to remember long conversations for large data chunks. Lengchain provides built-in memory management tools that allow your applications to maintain conversations stay over time or dynamically retrieve relevant documents. Third, Lengchain"
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_100_120.mp4",
    "text": " It's a pain-shine when it comes to real-time data integration. It works smoothly with external services like databases, APIs, and cloud platforms, enabling your applications to pull in live information and provide contextually accurate responses. This makes it ideal for building systems that need up to the minute data or complex external interactions."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_120_140.mp4",
    "text": " If you're still not convinced, here are a few additional reasons why you may want to dive into Lengcheng. Reason number one, accessibility. Whether you're a beginner looking to explore language models or a seasoned developer building advanced AI applications, Lengcheng's modular structure adapts to your needs. You can start simple and grow your applications complexity as you learn more."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_140_160.mp4",
    "text": " Reason number two, support. Length chain boasts an active and growing community, offering plenty of documentation, tutorials and support. Whether you're debugging, learning, or sharing your own use cases, you're not alone. There's a whole community of developers ready to help. And finally, Reason number three, job prospects. AI and machine learning engineers are enough."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_160_176.mp4",
    "text": " high-demand. With salaries averaging over $128,000 in the US, a cordon is hit for cruder. Ready to dive in? Check out Zero to Mastery's Lane Chain Bite Size course. Will you learn to build state-of-the-art large language model-powered applications?"
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_0_20.mp4",
    "text": " What exactly is Generative AI? When new content is created by artificial intelligence, it's called Generative AI. This could involve generating texts and images, as well as videos, music or voices. To do this, you describe in a chat dialogue box what you want the AI to be."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_20_40.mp4",
    "text": " create. This description is called a prompt. The Generative AI tools provide answers to all sorts of questions, summarise complex information and generate diverse ideas quickly. Depending on how they are used, they can create short stories, paintings, pieces of code, or even"
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_40_60.mp4",
    "text": " musical compositions. The foundation for this creation lies in large amounts of data that the AI system accesses to identify patterns and similarities. The content produced by the AI is new. It's often impressive and challenging to distinguish from things humans have made."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_60_80.mp4",
    "text": " A generative AI can also be misused. In so-called deep fakes, AI is utilized to produce images or videos that seem real. AI-generated texts are also tough to recognize as machine-made. Moreover, the AI can provide answers that sound..."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_80_100.mp4",
    "text": " correct but are actually incorrect. This is called hallucinating. The quality of what's created depends on both the quality of the data used and the quality of the prompts given. To effectively utilise generative AI, we need to learn how to guide the tools with meaningful prompts."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_100_120.mp4",
    "text": " and use them thoughtfully. Generative AI holds immense potential and can help us in many ways, such as serving as a writing or learning partner. However, the AI should do the hard work and humans should be responsible for the facts."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_120_122.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_0_20.mp4",
    "text": " Agendic AI represents a significant evolution in artificial intelligence that focuses on autonomous decision making and action-taking capabilities. Let's break down this concept in simple terms. What is Agendic AI? Agendic AI refers to artificial intelligence systems capable of..."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_20_40.mp4",
    "text": " of autonomous action and decision making. Unlike traditional AI that follows strict rules or generative AI that creates content, agentic AI can independently pursue goals with minimal human supervision. Think of agentic AI as a factory manager rather than a factory worker."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_40_60.mp4",
    "text": " On AI agent, might perform one specific task, like a factory worker on an assembly line, Agentic AI orchestrates multiple processes and makes decisions about how to achieve broader goals, like the factory manager overseeing operations. Key characteristics of Agentic AI. Agentic AI."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_60_80.mp4",
    "text": " systems are distinguished by four fundamental characteristics. They can make decisions and take actions without constant human oversight. They focus on achieving long-term objectives rather than just completing specific tasks. They learn from experiences and adjust their strategies based on changing conditions."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_80_100.mp4",
    "text": " They anticipate needs and take initiative rather than just reacting to commands. Here are the key differences between Agendic AI and other forms of AI. Agendic AI versus traditional AI. Traditional AI systems operate based on predefined rules and algorithms requiring"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_100_120.mp4",
    "text": " explicit instructions for every task. They're reactive and limited to specific domains. If traditional AI is like a calculator that can only perform these SAT calculations you input, Agendic AI is like a financial advisor who can analyze your finances, set goals, and make investment to see..."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_120_140.mp4",
    "text": " on your behalf. Agentech AI vs Generative AI. Generative AI, like chat GPD, creates content such as text, images, and videos based on prompts. It's primarily focused on creation and requires human guidance. It Generative AI."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_140_160.mp4",
    "text": " is like a highly skilled artist who can paint whatever you describe. A GEDDIC AI is like a home renovation contractor who not only designs or kitchen but also orders materials, coordinates workers, and completes the project with minimal direction from you. Generative AI focuses on creating while a GEDDIC AI focuses"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_160_180.mp4",
    "text": " on doing real world examples of Agenic AI. They act as your personal assistant. In other words, an AI that can book your flight, schedule transportation to a hotel, and make dinner reservations with a single command. AI agents and call centers can analyze customers"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_180_200.mp4",
    "text": " sentiment, review order history, access company policies, and respond to customer needs simultaneously. When it comes to supply chain management, they act as systems that can forecast a man fluctuations and proactively adjust inventory levels to prevent shortages. In multimedia creation, the agent"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_200_220.mp4",
    "text": " Heligates research, text generation, image selection, and design to other AI systems to create a complete multimedia report. In scientific discovery, they act as an AI agent that can identify new materials or drug compounds, select optimal suppliers, and even order necessary materials."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_220_240.mp4",
    "text": " Agenic AI represents the next frontier in AI development, with many experts viewing it as a crucial step toward artificial general intelligence AGI. As these systems continue to evolve, they promise to transform how we work, solve problems, and interact with technology."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_240_252.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_0_20.mp4",
    "text": " Hello everyone, welcome back to the IMITZTookChannel and today we are going to dive into the fascinating"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_20_40.mp4",
    "text": " This video we are going to cover how chain AI model 4 and what are their capability. You might be heard about what is chain AI and you might be know what is the Google part and what is the chain AI. These model have a lot of curiosity among the people but they can also be confusing when we talk about the chain AI. So when we talk about the chain AI, we are going to be able to see the chain AI model"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_40_60.mp4",
    "text": " the genitive AI, many people think it is a magical model that write a human language such as chaggitp 3.5 or 4 model and if you know about the Gemini which is for the Google. These models were popular and they were different approach and they are different. In this video I will"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_60_80.mp4",
    "text": " we provide a clear and simple explanation what the real life example to help you to understand what is Genetics, Genetics AI is all about. So let's get started. So Genetics AI fall under a broader category of the artificial intelligence known as a machine learning. You might be already familiar with the"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_80_100.mp4",
    "text": " machine learning, where computer learn performed task to without being a specific program. They are two-minute type of machine learning with a super-wide machine learning and so on, super-wide machine learning. Super-wide machine learning is involved training models on the liberal data to make a prediction or a classification. On the other hand, we have"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_100_120.mp4",
    "text": " have unscrupized machine learning deal with the finding the pattern on the structures in unlabeled data. Now, let's talk about the generative AI. In generative AI, they are two main type of model. One is the discriminative model and another is the generative models. So, discriminative models are 4."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_120_140.mp4",
    "text": " focus on the distance between the categories or classes. By generating model, our focus on generating on the new data. Now let's example, let's say we have a generative model train on the cat images or let's say dog images. When given a new output, so..."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_140_160.mp4",
    "text": " on the description of a CAD, what the CAD slide, what see a bit full of the CAD, the disability to create new content when it is said to be a Genitive AI model apart. Now it is discussed some real example of the Genitive AI. So this model can be used for the images."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_160_180.mp4",
    "text": " generations, text analysis and we ever viewed generation and sector. Companies are leveraging the genitives knowledge for the various tasks such as the content creations, data argumentations, data sentiment analysis, even the genitings of personalized recommendations. Now if you look at the core, it's the"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_180_200.mp4",
    "text": " are have heavily using to converting to the shifting to the gen A because they are having different problem now the gen A I can solve their problem by using this large LAM models. So genitive A I have been involved over time some architecture more complex like G24 or"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_200_220.mp4",
    "text": " vast amount of data to generate divers on high quality content. So in this conclusion the AI goals are really good emotions in the innovations and creativity by understanding these models. So let us explore this video sphere. So in the upcoming big guy one."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_220_240.mp4",
    "text": " to launch a bootcamp based on the chain and where we have a top faculty discussing what is the genitive and how you can implement the genia into this one. In upcoming 2 or 3 years this genia will be changed differently in the quadrits in the real life example. So you must know what is the genitive and how you can implement the genia into this one."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_240_260.mp4",
    "text": " a janeai and how to implement janeai into this work. So if you want to know about the janeai and the machine learning you can check out this follow link in the bio to check the description and the website page where we have a different course on the machine learning and janeai and the data engineering courses. So I hope you found this video very useful and"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_260_276.mp4",
    "text": " And if you need more content along the data science, and don't forget to comment down below, and don't forget the heat on the icon button, and stay updated on the latest video. Thanks everyone for the watch."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_0_20.mp4",
    "text": " Hi there. Let me show you how data breaks changes data engineering by giving you everything you need in one AI-infused intelligent platform. And the one governance layer, the Data Intelligence Platform handles it all. Streaming in Chess-Tune."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_20_40.mp4",
    "text": " transformation and orchestration, add a fraction of the typical development time and on top of serverless compute. As you might know, any new project starts with data in Chesschen, in Chesting Data from Sources, such as Cloud Storage, Message Buses, Databases, or ERMs only takes me..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_40_60.mp4",
    "text": " minutes. Lakeflow Connect seamlessly and continuously imports your data from popular business applications such as Salesforce, Workday and Google Analytics. With Lakeflow Connect, it just takes a few clicks and your data appears as fully synchronized tables in your catalog."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_60_80.mp4",
    "text": " ready to be used across the organization. And of course, there are other options too, like getting datasets or even ML models from Databricks Marketplace and partners. After interesting it, our data is now available and governed by Unity Catalog. So we can start transforming the raw data and extract..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_80_100.mp4",
    "text": " meaningful information. In the past, this required writing complex Python code and applying heart to understand optimization techniques. But now we have the Data Intelligence platform, which is infused with AI that understands your interactions with the data, it's metadata."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_100_120.mp4",
    "text": " and your code. Databricks Assistant saves your team tons of development time and keeps everyone following best practices. Here I created a single materialized view that automatically maintains fresh pre-computed data, making it ideal for dashboards. With Delta Life Tanks."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_120_140.mp4",
    "text": " You scale these transformations into robust production grade pipelines, written in simple SQL or Python. Let's look at a data life tables pipeline in action. Here, in the pipeline view, you can see multiple tables and transformation steps. It provides a clear visualization of your data flow."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_140_160.mp4",
    "text": " Also, data reliability is guaranteed by continuous data quality monitoring and status tracking in real time. Best of all, your pipeline adapts to your needs. You can easily switch between batch and streaming to match the pace of your business. Well, you have now seen..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_160_180.mp4",
    "text": " how Databricks Streamlines individual tasks. But these steps are for only building blocks of a larger workflow. This is where Databricks workflows comes in, a fully serverless orchestration engine that orchestrates everything in the lake house and seamlessly coordinates all the required steps."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_180_200.mp4",
    "text": " such as triggering D9 gesture or transformation pipelines, orchestrating ML model updates, or automating dashboard refreshes for a business team. To summarize, think of the D9 Intelligence platform as you want to stop platform for D9 engineering."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_200_220.mp4",
    "text": " You can enchest data with Lakeflow Connect, transform it using simple SQL pipelines with Delta Life tables, and orchestrate every step in your data journey with Databricks workflows, all on one secure and serverless platform. Are you ready to transform your data engineering?"
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_220_230.mp4",
    "text": " Start today at dtabrics.com slash trial. Thanks for joining me in this data engineering overview."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_0_20.mp4",
    "text": " This is the most comprehensive cursor tutorial on this planet. So make sure you watch till the end to build your own dream start up using cursor. You will ask who is this tutorial for? This tutorial is specifically designed for someone who is a complete non-coder or a coder. Or anyone who wants to start using cursor to build their own apps, web apps, mobile apps or literally anything."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_20_40.mp4",
    "text": " they've dreamed of. If code has always felt scary, this is exactly what you need to finally break that fear and start building. Why do you need this tutorial? Because this will help you build your dream startup without needing to write code manually. Gursar plus the right prompts is a game changer and you will learn this shortly. What skills will you gain?"
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_40_60.mp4",
    "text": " Let me be honest the one skill I can guarantee after completing this tutorial is you'll be able to build anything Using cursor and I mean it anything just mark my words and give this tutorial your full attention one simple request from me to you Only the top 1% of people will complete this full tutorial So make sure you're in that 1% if you're still watching this video right now you've all"
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_60_80.mp4",
    "text": " already pass most of the people who quit too early, stay till the end and you will walk away knowing how to build your own apps without writing a single line of code. Alright, enough of the talking. Let's jump in and start building something awesome together. Okay. So the first thing you need to do is download cursor, go to Google and search download cursor. This is the official website you want to visit. You can download cursor."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_80_100.mp4",
    "text": " for Mac by clicking the download button. If you are using Windows or Linux, you can check the other platform options. Make sure you are downloading the latest version, which is 0.48 because that's what I'll be using in this tutorial. Once downloaded, just open cursor. When you open cursor, you'll see the main interface, the cursor dashboard, from here, we'll create our first project."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_100_120.mp4",
    "text": " Click on Open Project and then create a new folder. Let's name it something like cursor tutorial 101. Open this folder inside cursor and there you go. Your first project is now set up. On the left hand panel you'll see the file explorer. Here you can add files, folders and refresh the workspace. There's also a search part to find files or get integration."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_120_140.mp4",
    "text": " tab which we won't be covering today and the extension section. Cursor is based on VS code which was originally developed by Microsoft as a coding ID. Now on the right hand side you'll see the chat window. This is where the magic happens. There are three interaction modes. One, agentic mode for guided workflows and full builds. Two, ask mode to ask questions about..."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_140_160.mp4",
    "text": " your codebase, three manual edit where you highlight code and ask for changes. You'll also find passchats and notepads where you can plan features, leave notes or outline new ideas. To get started, we'll build a basic to do app using HTML, CSS and tailwind. For non-corders, HTML is a markup language that structures your web page."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_160_180.mp4",
    "text": " and tailwind CSS is for styling. Now I'll ask cursor created to do app using HTML and tailwind CSS. Let's build it step by step. Make sure you select Argentic mode. I'm using Gemini 2.5 Pro with thinking mode enabled. But you can also use claw 3.7 on it. Even the free plan on cursor. Let's see."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_180_200.mp4",
    "text": " you build projects, but it comes with a two week trial and a 2000 completion limit. The pro plan is $20 per month and gives you unlimited completions plus faster responses. Now cursor is generating the app. It's planning its next move and starts by building index.html. Also, cursor also creates app.js, but don't stress about the code."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_200_220.mp4",
    "text": " Just follow along and we'll edit it together. Except all the changes cursor suggests. Now right click the file and select open with live server. You should see your basic to do app. Try it out. Add a new task. Delete a task. All of this was created from a single prompt. Awesome, right? Let's say you want to change the style of a button. Highlight the button."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_220_240.mp4",
    "text": " code and send it to the chat. For example, make the button yellow with a stylish hover effect. Cursor understands the request, shows you the current code, then updates it with the new styles, accept the changes, save the file and refresh the page to see the updated button design. Boom! Yellow button with hover styling. Now let's take it up a notch."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_240_260.mp4",
    "text": " will copy a UI design from Figma and build the exact same layout, go to Figma and search to do app UI design web template, open any nice looking template, pick the perfect component you like, let's say a task card, right click and copy as PNG, then go back to cursor and paste it directly into the chat window."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_260_280.mp4",
    "text": " cursor now has the image context. Now we will use the prompt. Use this design to create my to do app layout step-by-step. Cursor breaks it down, plans the design structure, creates a dark theme, adds components styling, and modifies both index.html and app.as to reflect the new design, except all changes, save, refresh the browser, your app now mirrors the face."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_280_300.mp4",
    "text": " my design with a fresh stylish UI. If you don't want to manually accept changes each time, you can enable auto run mode from cursor settings. But I recommend turning it on only when you're confident about the prompts. For beginners, keep it manual so you have control. That was a complete walkthrough from downloading cursor to building a full app to editing code and copying UI from a design tool."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_300_316.mp4",
    "text": " If you want to learn more advanced stuff like building full stack apps, integrating AI or working with MCP tools, make sure to subscribe to the channel, join the school community, and drop a comment if you want more tutorials like this one. With this knowledge, you can start building real apps using cursor, even if you have never written a line."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_0_20.mp4",
    "text": " Hello world, it's Siraj and let's learn about a popular new deep learning framework called PyTorch. The name is inspired by the popular Torch deep learning framework which was written in the Lua programming language. Learning Lua is a big barrier to entry if you're just starting to learn deep learning and it doesn't offer the modularity necessary to"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_20_40.mp4",
    "text": " interface with other libraries like a more accessible language would. So a couple of AI researchers who were inspired by Torch's programming style decided to implement it in Python, calling it PyTorch. They also added a few other really cool features to the mix, and we'll talk about the two main ones. The first key feature of PyTorch is in PyTorch."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_40_60.mp4",
    "text": " imperative programming. An imperative program performs computation as you typed it. Most Python code is imperative. In this numpy example, we write four lines of code to ultimately compute the value for d. When the program execute c equals b times a, it runs the actual computation then and there, just like you told it to. In contrast,"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_60_80.mp4",
    "text": " in a symbolic program, there is a clear separation between defining the computation graph and compiling it. If we were to rewrite the same code symbolically, then when c equals b times a is executed, no computation occurs at that line. Instead, these operations generate a computation or symbolic graph. And then we can convert the graph into a function."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_80_100.mp4",
    "text": " function that can be called via the compile step. So computation happens as the last step in the code. Both styles have their trade-offs. Symbolic programs are more efficient since you can safely reuse the memory of your values for in-place computation. TensorFlow is made to use symbolic programming. Imperative programs are more flexible since..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_100_120.mp4",
    "text": " Python is most suited for them. So you can use native Python features like printing out values in the middle of computation and injecting loops into the computation flow itself. The second key feature of PyTorch is dynamic computation graphing as opposed to static computation graphing. In other words, PyTorch is defined by run."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_120_140.mp4",
    "text": " So at runtime, the system generates the graph structure. TensorFlow is defined and run, where we define conditions and iterations in the graph structure. It's like writing the whole program before running it, so the degree of freedom is limited. So in TensorFlow, we define the computation graph once, then we can execute that same graph many times. The great thing about this..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_140_160.mp4",
    "text": " is that we can optimize the graph at the start. Let's say in our model we want to use some kind of strategy for distributing the graph across multiple machines. This kind of computationally expensive optimization can be reduced by reusing the same graph. Static graphs work well for neural networks that are fixed size like feed-forward networks"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_160_180.mp4",
    "text": " or convolutional networks, but for a lot of use cases, it would be useful if the graph structure could change, depending on the input data, like when using recurrent neural networks. In this snippet, we're using TensorFlow to unroll a recurrent network unit over word vectors. To do this, we'll need to use a special TensorFlow function called while loop. We have to use"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_180_200.mp4",
    "text": " special nodes to represent primitives like loops and conditionals because any control flow statements will run only once when the graph is built. But a cleaner way to do this is to use dynamic graphs instead, where the computation graph is built and rebuilt as necessary at runtime. The code is more straightforward since we can use standard 4 and"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_200_220.mp4",
    "text": " if statements. Any time the amount of work that needs to be done is variable, dynamic graphs are useful. Using dynamic graphs makes debugging really easy, since a specific line in our written code is what fails, as opposed to something deep under session.run. Let's build a simple two-layer neural network in PyTorch to get a feel for the syntax. We start by importing our framework."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_220_240.mp4",
    "text": " as well as the autograd package, which will let our network automatically implement back propagation, then we'll define our batch size, input dimension, hidden dimension, and output dimension. We'll then use those values to help define tensors to hold inputs and outputs, wrapping them in variables. We'll set requires gradients to false since we don't need to..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_240_260.mp4",
    "text": " to compute gradients with respect to these variables during backpropagation. The next set of variables will define our our weights. We'll initialize them as variables as well, storing random tensors with the float data type. Since we do want to compute gradients with respect to these variables, we'll set the flag to true. We'll define a learning rate, then we can begin our"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_260_280.mp4",
    "text": " training loop for 500 iterations. During the forward pass, we can compute the predicted label using operations on our variables. MM stands for Matrix Multiply and Clamp clamps all the elements in the input range into a range between min and max. Once we've matrix multiplied for both sets of weights to compute our prediction, we can count."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_280_300.mp4",
    "text": " calculate the difference between them and square it, the sum of all the squared errors, a popular loss function. Before we perform back propagation, we need to manually zero the gradients for both sets of weights, since the gradient buffers have to be manually reset before fresh gradients are calculated. Then we can run back propagation by simply calling the backward function on our..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_300_320.mp4",
    "text": " It will compute the gradient of our loss with respect to all variables we set requires gradient to true for. Then we can update our weights using gradient descent. And our outputs look great. Pretty dope. To sum up, PyTorch offers two really useful features, dynamic computation graphs and imperative programming. Dynamic computation graphs are built and re-"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_320_340.mp4",
    "text": " built as necessary at runtime and imperative programs perform computation as you run them. There is no distinction between defining the computation graph and compiling. Right now TensorFlow has the best documentation on the web for a machine learning library, so it's still the best way for beginners to start learning. It's best suited for..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_340_358.mp4",
    "text": " reduction use since it was built with distributed computing in mind. But for researchers, it seems like Pi Torch has a clear advantage here. A lot of cool new ideas will benefit and rely on the use of dynamic graphs. Please subscribe for more programming videos and for now, I've got to torch my hair."
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_0_20.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_20_40.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_40_60.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_60_80.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_80_100.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_100_120.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_120_140.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_140_160.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_160_180.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_180_200.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_200_220.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_0_20.mp4",
    "text": " Streamlit is a modern web framework for data science applications and the company found it around the framework psycho system. The development flow in Streamlit entails that every time you save some changes, Streamlit will be able to rerun your updated application which enables a quick feedback loop during development. The data flow in Streamlit apps ensures that the entire python..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_20_40.mp4",
    "text": " codebase will be run when changes are made to either the code or the state of the widgets in the UI. Srimlet also offers ways to modify these behaviors using callbacks, such as on-change and on-click. Behind the scenes, Srimlet makes use of caching to speed up the entire process. The Srimlet right method or SD right for short is one-"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_40_60.mp4",
    "text": " One of the most powerful streamlit features is for displaying and styling data. S.T.R.E.R.E.T can take different types of data as input and figure out the right way to display them. The supported data types include text strings, data frames, figures, charts, objects, models, and many more. Magic and magic commands enable developers to display almost anything."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_60_80.mp4",
    "text": " without having to even use an explicit command like SDRIGHT. Just type the variable name on a separate line and let's streamlets do the rest. Widgets help you input data and interact with your application. They include buttons, sliders, select boxes, text inputs, check boxes, and all the other widgets you'd expect to use in a web or mobile application. Creating widgets is as easy as..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_80_100.mp4",
    "text": " as just instantiating a python variable. This streamlit variable will hold your inputs, which can be used in other operations stored or displayed. The layout in streamlit applications can be easily customized using layout methods such as STSidebar to create a left panel sidebar for your controls, ST columns to place widget side by side."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_100_120.mp4",
    "text": " As the expanders to hide large content and SD progress that can be displayed during wrong running paths. Teams help you change the color scheme and the fonts within your applications. Streamlit comes with a default light theme and a dark theme, as well as the functionality to create and save your own custom teams. Caching allows your app to run faster when performing long running time..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_120_140.mp4",
    "text": " Caching is performed by wrapping functions with the add SC cache decorator which checks all the code and the input variables passed to that function in order to decide to run that function again or to skip execution. Pages for multi-page apps in streamlit are created by adding individual python scripts within a page directory. Next to the main page."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_140_160.mp4",
    "text": " of your application called mainpage.py. The Model in Streamlit relies on several pillars. Streamlit apps that are essentially Python scripts that run from top to bottom, scripts that are re-executed on each page load, the fact that Streamlit will draw its output live as the script is run, caching which is used for efficient and faster applications, scripts that are re-exec-"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_160_180.mp4",
    "text": " on each widget interaction and multi-page applications that are built from individual Python scripts stored in a pages folder. Components are third-party modules that extend what's possible with Streamlit. You can use the component gallery to find components, install them as regular Python packages or you can create and publish your own packages. To install Streamlit it's recommended you"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_180_200.mp4",
    "text": " set up a local Python environment on your computer using konda, pip, pip, and vm or virtual m. Or you might as well consider an a konda. First you have to make sure that you installed a Python version, either 3.7 or above, then you can install streamlit using pip install streamlit. And you can test your installation by simply running streamlit hello. After which a hello app"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_200_220.mp4",
    "text": " should appear in a new tab in your browser. To run Streamlit for a file, name say MyFile.py. Simply use the command Streamlit, run MyFile.py. To create a Streamlit app, we start a new Python file. We import Streamlit and the necessary packages at the top. We can then set the app title using SD title. When we need some data to work with, we'll use"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_220_240.mp4",
    "text": " the open source uber raw dataset. We create our load underscore data method and then we'll use the load data function to load 10,000 rows of data in a pandas data frame. We can also add caching to our load data function using the decorator at st.cache underscore data to improve the performance. We can also use the"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_240_260.mp4",
    "text": " to draw a histogram using our data. We can use STMAP to plot our data on a map. To filter the data using a slider, we can simply use ST slider. And finally, you can also use a button to toggle our data. We showed previously how we can create a single page app using Streamlit. But oftentimes, a multi-page app is needed. To build a multi-page app, we start with our entry point..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_260_280.mp4",
    "text": " file page, say, home.py. We then create a folder named pages where we add the rest of our pages. The command streamlet run home.py. We run our entire multi-page app and this is how we split our existing Uber dataset single page app into a multi-page app with a home page, a plotting demo page, a mapping page and a data frame page. So..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_280_300.mp4",
    "text": " We show the case to what streamlet is, what it can be used for and how it can be used. If you like this video about streamlet and you would want me to create a series of more advanced videos on streamlet, please like this video and subscribe to the channel. You should also click the notification bell if you want to be kept up to date with similar content. If there are any particular topics you'd want me to discuss in the future related to"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_300_303.mp4",
    "text": " data science or to stream the team particular, leave me a comment down below."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_0_20.mp4",
    "text": " Today I'm super excited to talk about a tool that has been a game changer in how we manage an automated cloud infrastructure. Terraform! If you already know Terraform, let me know how you like it in the comments below. I'd love to hear your experience. Now if you're new to Terraform or just curious to learn more about what it can do,"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_20_40.mp4",
    "text": " you're in the right place, so let's get into it. So what is Terraform? And simple terms Terraform is an open source infrastructure as code tool. It allows you to define Cloud infrastructure like servers, databases, and networking using code instead of clicking around in the UI."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_40_60.mp4",
    "text": " creating resources manually, you will write just a script and Terraform does the heavy lifting then for you. Let's talk about the problems of Terraforms offs, right? So imagine you're building an Azure-based data pipeline. Without Terraform, you have to manually create services like data factories."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_60_80.mp4",
    "text": " signups and analytics in storage accounts. This takes time, introduces inconsistencies, and can lead to errors, right? Because it's you that's doing it. Terraform changes the game. When you ride your infrastructure as code, it's reusable and guarantees the same results every time you run it. Whether you're the..."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_80_100.mp4",
    "text": " deploying your pipeline in a test environment or scaling it in production. You can trust that Terraform gives you exactly what you defined, so there are not really surprises. How does Terraform work? It's quite straightforward. You write in your infrastructure definitions in a simple configuration language called"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_100_120.mp4",
    "text": " HCL or HashiCorp configuration language. If you worked with Yamal before you will find HCL very familiar. It's a human readable and easy to learn format. Once you've written your configuration, you use commands like Terraform plan to preview changes and Terraform apply to deploy them."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_120_140.mp4",
    "text": " Plus, it works seamlessly with other providers like AWS, GCP, Databricks, Snowflick, and so on and so on. So it's not just Azure. One of the most powerful features of Terraform is its ability to manage the lifecycle of your infrastructure using its state file."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_140_160.mp4",
    "text": " Terraform tracks every resource it's managing. You need to make changes, Terraform will update only what's necessary. Want to delete and recreate your entire infrastructure? Terraform can do that too. Just update the configuration or use commands like Terraform taint for specific resources."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_160_180.mp4",
    "text": " If flexibility is perfect for testing, disaster recovery or scaling up your infrastructure. Now let's talk about collaboration. When your terraform configurations are stored in a version control system like GitHub, teamwork becomes basically seamless. Two Android colleagues can work together on the same..."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_180_200.mp4",
    "text": " Infrastructure code, review changes through pull requests and track the history of the modifications. And here's where it gets even better. There are form integrates perfectly with GitHub actions, enabling full CI-CD pipelines. This means you can automatically validate, test and deploy your infrastructure changes as part of"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_200_220.mp4",
    "text": " If you're a development workflow, it's a complete solution for efficient automated infrastructure management. If you're wondering how to get started with Terraform, I've got something that might just be perfect for you. Over at my academy at LearnDataEngine.com, I've created a hands-on project called Azure Pipelines"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_220_240.mp4",
    "text": " Terraform. It's designed to guide you through building a real-world automated data pipeline on Azure, basically step by step. This hands-on training is all about real-world skills. You'll extract data from APIs, process it using Azure tools, and even implement architectures like the Lakehouse and"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_240_260.mp4",
    "text": " medallion architecture. By the end you'll not only understand terraform but also have a complete project to showcase your skills plus, we'll dive into setting up CISTD pipelines using terraform and GitHub so your entire workflow from code to deployment is automated and efficient."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_260_280.mp4",
    "text": " Terraform isn't just an automation, it's about helping you focus on what really matters. Solving problems, creating value, if you're ready to lever up your skills, head over to learndataengineering.com and check out the hands-on project. Let's build something amazing. Alright, so..."
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_280_300.mp4",
    "text": " Don't forget to like and subscribe if you found this helpful and don't forget to share your experience with Terraform in the comments. I love to hear how you're using it. Until next time, bye!"
  },
  {
    "clip_path": "ui/clips/1pH1F6s5EQI_300_311.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_0_20.mp4",
    "text": " Right from the start, Software Development comprised two different departments. The development team that develops the plan, designs, and builds the system from scratch, and the Operation Team for testing and implementation of whatever is developed. The Operations Team gave the development team feedback on any bugs that needed fixing, and any rework."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_20_40.mp4",
    "text": " required. Inveribly, the development team would be idle awaiting feedback from the operations team. This undoubtedly extended timelines and delayed the entire software development cycle. There would be instances where the development team moves on to the next project, while the operations team continues to provide feedback for the previous code."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_40_60.mp4",
    "text": " weeks or even months for the project to be closed and final code to be developed. Now, what if the two departments came together and worked in collaboration with each other? What if the wall of confusion was broken? And this is called the DevOps approach. The DevOps symbol resembles an infinity sign suggests"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_60_80.mp4",
    "text": " that it is a continuous process of improving efficiency and constant activity. The DevOps approach makes companies adapt faster to updates and development changes. The teams can now deliver quickly, and the deployments are more consistent and smooth. Though there may be communication challenges, DevOps manages a streamlined flow between the teams."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_80_100.mp4",
    "text": " and makes the software development process successful. The DevOps culture is implemented in several phases with the help of several tools. Let's have a look at these phases. The first phase is the planning phase, where the development team puts down a plan, keeping in mind the application objectives that are to be delivered to the customer. Once the"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_100_120.mp4",
    "text": " plan is made, the coding begins. The development team works on the same code and different versions of the code are stored into a repository with the help of tools like Git and Merge when required. This process is called version control. The code is then made executable with tools like Maven and Gradle in the build stage. After the code is successful,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_120_140.mp4",
    "text": " built. It is then tested for any bugs or errors. The most popular tool for automation testing is Selenium. Once the code has passed several manual and automated tests, we can say that it is ready for deployment and is sent to the operations team. The operations team now deploys the code to the working environment. The most"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_140_160.mp4",
    "text": " Those prominent tools used to automate these phases are Ansible, Docker, and Kubernetes. After the deployment, the product is continuously monitored, and Nagios is one of the top tools used to automate this phase. The feedback received after this phase is sent back to the planning phase, and this is what forms the core of the DevOps lifecycle."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_160_180.mp4",
    "text": " that is the integration phase. Jenkins is the tool that sends the code for building and testing. If the code passes the test, it is sent for deployment, and this is referred to as continuous integration. There are many tech giants and organizations that have opted for the DevOps approach. For example, Amazon, Netflix, Walmart,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_180_200.mp4",
    "text": " Facebook, and Adobe. Netflix introduced its online streaming service in 2007. In 2014, it was estimated that a downtime for about an hour would cost Netflix $200,000. However, now Netflix can cope with such issues. They opted for DevOps in the most fantastic way."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_200_220.mp4",
    "text": " Netflix developed a tool called the Simeon Army that continuously created bugs in the environment without affecting the users. This chaos motivated the developers to build a system that does not fall apart when any such thing happens. So, on this note, here is a quiz for you. Match the DevOps tool with the phase it is used in."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_220_240.mp4",
    "text": " A. B. C. D. None of the above. Today, more and more companies lean towards automation. With the aim of..."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_240_260.mp4",
    "text": " reducing its delivery time and the gap between its development and operations teams. To attain all of these, there's just one gateway. DevOps! If you enjoyed this video, if you did, a thumbs up would be really appreciated. Here's your reminder to subscribe to our channel, and to click on the bell icon for more on the latest technologies and"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_260_279.mp4",
    "text": " trends. Thank you for watching and stay tuned for more from SimpliLearn."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_0_20.mp4",
    "text": " Meet Emma, a graphic designer working on a new project. One day, her colleague mentions a tool that helps create designs, images, and text using AI. Intrigued, Emma wonders how AI can create something from scratch. Her curiosity grows, and she decides to dive deeper into this tool."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_20_40.mp4",
    "text": " new technology called Generative AI. Generative AI refers to a type of artificial intelligence designed to create new content, such as text, images, music, and videos. Unlike traditional AI, which analyzes or categorizes data, Generative AI produces original content based on patterns learned."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_40_60.mp4",
    "text": " from vast data sets. Essentially, it generates new unique material. These models are often trained on large amounts of data and use sophisticated algorithms to mimic human creativity. Tools like Chad G.P.T. or Dolly can create art, write essays, or simulate conversations by generating output"
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_60_80.mp4",
    "text": " based on user prompts. Generative AI has a wide range of applications, content creation, tools like GPT-4 generate text, logposts, stories, and essays from simple prompts. Art and design, AI models such as Dolly, generates unique images and designs based on text descriptions."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_80_100.mp4",
    "text": " Transforming Creativity in Art. Music in Audio. AI can compose music or replicate voices, offering new possibilities for musicians and audio engineers. Healthcare. Generative AI simulates disease progression or creates synthetic medical data, helping doctors gain fast-food."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_100_120.mp4",
    "text": " insights for research. Let's take image generation as an example to explain how generative AI works. Data collection and learning. AI models like Dolly are trained on large data sets of images paired with text descriptions. These data sets teach the model to recognize different objects, colors, styles,"
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_120_140.mp4",
    "text": " and how to associate text with corresponding images. The more data the AI learns from, the better it can generate accurate and diverse images based on user prompts. Neural networks and transformers. When Emma inputs a prompt like a cat wearing sunglasses, the transformer model processes the text."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_140_160.mp4",
    "text": " Recognizing words like cat and sunglasses and links them to images that learn from during training, transformers help the AI decide how to combine these elements into a coherent image. Tokens in context. The text input, such as a cat wearing sunglasses, is split into smaller parts called tokens. The AI process is a very important thing."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_160_180.mp4",
    "text": " processes each token and understands their relationship. For instance, it knows the sunglasses should be placed on the cat, creating a contextually accurate image. Feedback mechanism. Generative AI models improved through feedback. After generating an image, users provide feedback on the accuracy or quality."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_180_200.mp4",
    "text": " of the output. If M is generated image shows the sunglasses floating beside the cat, she can mark it as incorrect. The model uses this feedback to improve future image generations. Reinforcement Learning Reinforcement Learning further enhances the AI's ability. The model is rewarded when it generates accurate images."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_200_220.mp4",
    "text": " and corrected when it makes mistakes. For example, when MED describes a sunset and the AI produces a vibrant sunset image, it receives positive reinforcement. Over time, this method refines the model's ability to generate better images. Data science and AI models. Data scientists curate"
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_220_240.mp4",
    "text": " the training data and define the parameters that help the AI generate accurate images. The more varied the data set, the more versatile the AI becomes in generating diverse types of content. Advanced models use billions of parameters, which are settings that guide the AI in processing data and generating outputs. Generating original content."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_240_260.mp4",
    "text": " It's trained the model can generate original images. For example, in the might describe a futuristic city-skate, and the AI would produce a unique image based on what it learned. The generated image isn't just a copy of past data, but an entirely new creation, showcasing the AI's ability to combine learn patterns and creativity."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_260_280.mp4",
    "text": " Now, let's have a quick, fun quiz on what we have learned so far. What does generative AI primarily do? A, analyze data. B, generate new content. C, store data. Make sure to let us know your answer in the comments section below for a chance to win an Amazon voucher."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_280_300.mp4",
    "text": " If you are interested in learning AI, then make sure to check out Simply Learn's Generative AI program in association with top universities. We hope that you enjoyed this video and found it informative and exciting. If yes, then we would appreciate a thumbs up. A gentle reminder to get subscribed to Simply Learn and click that bell icon to never miss any update."
  },
  {
    "clip_path": "ui/clips/NRmAXDWJVnU_300_301.mp4",
    "text": " from Simply Learn."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_0_20.mp4",
    "text": " Hi, welcome back LearnGender2AI in 5 minutes. So today we talk about how Gender2AI model works. So here let's try to understand how Gender2AI models are working."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_20_40.mp4",
    "text": " So here you will find users and roles in multiple domains, be it a banking domain, in students domain, manufacturing domain, any kind of domain, a people that means users, roles will be having a lot of doubts, questions, issues, concerns. They should be able to solve."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_40_60.mp4",
    "text": " solve their problems. So how are they going to solve problems with the help of gendered way? This is where they prepare prompts in natural language. So they can prepare any type of prompt. They can randomly generate prompts. But to prepare prompts there is some technology."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_60_80.mp4",
    "text": " There is some methodology, there are some techniques. That is where we call it as a prompt engineering. Thus just with this prompt engineering, we have a lot of new job opportunities. These prompt engineering prompts will be conjured by gender-rear models. That is where we talk about..."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_80_100.mp4",
    "text": " about foundation models which are already trained models. Here we don't need to train any model like machine learning. Here you get already trained models, pre-trained models. These are called foundation models. These genie models will consume the prompts given by the users on different kind of roles and generate."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_100_120.mp4",
    "text": " new content, new text, new video, new audio, new code, image. This is the working model of any generative AI engine. Right. So as I told you, foundation models, these are pre-trained models and massive amounts of data sets. Data sets."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_120_140.mp4",
    "text": " will contain huge amount of data that huge amount of data is gathered on social media, wiki media, blogs. So this information will be passed to the models and models will get trained on this data and supplied to you to prepare and apply."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_140_160.mp4",
    "text": " consume prompts and generate new content for you. So, our generator via models we work based on the transformer architectures. In further sessions we are going to discuss what is the transformer transformer architecture. So, the first released models are large language models."
  },
  {
    "clip_path": "ui/clips/IZnaX4x5d8c_160_169.mp4",
    "text": " These works only on text data, but now the latest model is multi-model model. These models will work on text image, audio and..."
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_0_20.mp4",
    "text": " Hi, welcome back. Learns in day to day in 5 minutes. Today we will talk about types of foundation models. There are many foundation models supplied by different companies, open a hugging phase, Google, there are many GPT."
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_20_40.mp4",
    "text": " models. So here we are going to talk about these GPD models all GPD models are going to work based on transformer architecture. There are task specific models, the generative GIA models there will be using generative adversarial network variational"
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_40_60.mp4",
    "text": " Art and Coder diffusion models and flow models, these are all the different foundation models. So, what is the next expectations of human being? We have discussed what is artificial intelligence, discussed what is machine learning, discussed what is deep learning, what is in the"
  },
  {
    "clip_path": "ui/clips/Fi47A_-wvuk_60_71.mp4",
    "text": " So, for all good we understood complete what is artificial intelligence, what is the next expectations of human being?"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_0_20.mp4",
    "text": " Hey everyone! Today we're diving into a Gentik AI, smart AI that doesn't just respond, it acts on its own. From automating tasks to changing how we use the internet, this tech is the future. Let's get started! So, what exactly is a Gentik AI?"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_20_40.mp4",
    "text": " In simple terms, its artificial intelligence with a goal-oriented mind of its own. Unlike traditional AI that only responds to prompts, agentic AI can plan, decide, and execute tasks by itself. Imagine asking an AI to book a vacation, and it handles searching flights, comparing hotels, and even e-"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_40_60.mp4",
    "text": " emailing you the itinerary. That's the power of agents. They're intelligent, autonomous, and multi-step problem solvers. You might wonder, why are we talking about Agentic AI now? Well, three big reasons. First, AI today isn't just about text. It's."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_60_80.mp4",
    "text": " model. That means it can understand images, videos, voice, and more. Second, large language models like GPT40, Clawed 3, and Gemini have made it possible for AI to grasp complex tasks. And third, there's growing demand for automation at scale, whether you're a business or an..."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_80_100.mp4",
    "text": " individual. All this sets the perfect stage for AgentikaI to thrive. AgentikaI is already being used in real-world scenarios. In software development, tools like AlphaVolve can generate code on their own. In customer support, smart agents handle queries and escalate issues."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_100_120.mp4",
    "text": " without human help. Google is even testing AI agents inside search that can complete tasks for you. And for personal productivity? These agents can organize your calendar, send reminders, do research, all without you lifting a finger. Now let's talk pros and cons."
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_120_140.mp4",
    "text": " The benefits are clear, agentic AI saves time, scales efficiently, and reduces errors. But it's not all smooth sailing. There are real concerns, reliability, cybersecurity risks, job disruption, and of course, ethical boundaries. What happens if an agent makes the wrong decision?"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_140_160.mp4",
    "text": " Who's responsible? These are questions we must tackle as the tech advances. You don't need to be a huge company to start using Agentic AI. As a content creator, you could use an AI agent to help write scripts, edit videos, and even manage comments. As a developer, look!"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_160_180.mp4",
    "text": " code platforms let you build your own agents. For businesses, internal help desks and customer support can be powered by AI. The possibilities are endless and accessible. Thanks for watching. Don't forget to like, share, and subscribe for more tech tips and"
  },
  {
    "clip_path": "ui/clips/G5Cw5FV6tec_180_183.mp4",
    "text": " updates. See you in the next video."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_0_20.mp4",
    "text": " To understand AI agents, let's first look at how they differ from traditional software. Traditional software is built using programming languages and follows a fixed set of instructions. It takes an input, processes it in the same way every time, and produces a predictable"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_20_40.mp4",
    "text": " output. It can interact with external systems like databases, file systems, or APIs, but always in a deterministic predefined way. Compare that to generative AI applications powered by large language models like chat GPT, Cloud, Gemini, or D."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_40_60.mp4",
    "text": " These are advanced systems that take input in natural language or even in other data types like voice images or structured data. They pass that input through a neural network and generate a response. And like traditional programs, they don't always act the same."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_60_80.mp4",
    "text": " way. They're not deterministic. These one-shot interactions are incredibly useful, but they come with major limitations. Their knowledge is frozen at the time they were trained, and they can't interact with their employment the way traditional software does. That's where"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_80_100.mp4",
    "text": " AI agents come in by giving LLM's access to external systems and data, agents and lock a much wide range of real-world applications. In simple terms, an AI agent is a digital system that operates autonomously within an environment."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_100_120.mp4",
    "text": " It usually performs three core functions, perceived environment by accessing data, sensors, or inputs, decide using an internal reasoning engine to plan actions toward a goal, act by using tools to perform tasks."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_120_140.mp4",
    "text": " the real world. This is called the Perceive Decide Act Loop, Inc. with smart robot vacuum. It perceives the world using sensors. It decides how to navigate a room and it acts by moving and vacuuming. The idea of agent is new. In fact, it was a major"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_140_160.mp4",
    "text": " topic in the 90s and early 2000s, but back then we didn't have reasoning models that were smart enough to handle complex problems. LLMs changed that. They unlocked new levels of reasoning, planning and context in the standing. So how do modern agents work? At its cost."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_160_180.mp4",
    "text": " or an AI agent allowed generative AI models to interact with external tools. This interaction is achieved by an orchestration or engine component that manages the agents instructions and goals, tools calling and optionally gives it access."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_180_200.mp4",
    "text": " to short and long term memory. These tools allow the agent to interact with their environment. For example, they can read and write data from files or databases, search to web or interact with online forms, call APIs, access code basis to generate or update software."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_200_220.mp4",
    "text": " communicate with physical devices like camera, smart sensor, other hardware. Now, when should we use agent? Agents are ideal when we need autonomy, complex reasoning, tool use and adaptability. Think of these processes, for example. Custom"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_220_240.mp4",
    "text": " support sales funnels, employees hiring, etc. These are examples of complex use cases that can't be fully automated by classical approaches. They require a level of smart that are better suited for AI agents. In the industry, you framework..."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_240_260.mp4",
    "text": " like Lank Chain and Lank Graph, AutoJane, Cruoii and Pidentic AI make it easier to build agents. This is very exciting, but it's not without challenges. L&Ms are powerful but unpredictable. They can hallucinate facts or f"
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_260_280.mp4",
    "text": " misuse tools in ways that cause system failures. LLMs are computationally expensive and multi-step planning increases both runtime and cost if an agent takes 20 steps to solve a problem that costs adds up very quickly. Given agents access to real systems."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_280_300.mp4",
    "text": " DataBases, Devices, User Accounts, Introduces serious risks. What if an agent deletes critical data or leaks private information? Ultimately, agents will become seamless assistance embedded in apps, workflows and physical devices."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_300_320.mp4",
    "text": " We just need to make sure they don't cause harm along the way."
  },
  {
    "clip_path": "ui/clips/krfXutyqPDY_320_322.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_0_20.mp4",
    "text": " Hey what's up everybody? Today I'm diving into something super exciting, a genetic AI and how it's going to transform our world. First things first, what's a genetic AI? Well it's a type of artificial intelligence designed to act with a certain level of autonomy. Unlike traditional AI which follows pre-programmed..."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_20_40.mp4",
    "text": " instructions. Agenetic AI can make decisions on its own to achieve specific goals. Think of it as having a digital assistant that doesn't just follow orders, but can actually think about the best way to help you. Now why is this so game changing? Let's break it down. Imagine factories where a Genetic AI optimizes production."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_40_60.mp4",
    "text": " lines reduces downtime and ensures quality control. Your AI assistant doesn't just follow a set schedule. It adapts in real time to changing conditions, making the entire process more efficient. In mining, agentech AI can manage autonomous equipment, ensuring safety and maximizing resource extraction."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_60_80.mp4",
    "text": " That's the kind of seamless experience we're talking about, but it goes beyond just efficiency. In the defense sector, agente AI can be used for surveillance, threat detection and even autonomous drones, making operations safer and more effective. We're talking about saving lives and enhancing security, and what about other industrial applications?"
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_80_100.mp4",
    "text": " A gentigay eye can revolutionize how industries operate. These systems can communicate with each other to optimize processes, reduce waste and increase overall productivity. Imagine never having to deal with unexpected downtime. Sounds like a dream, right? Moving to other sectors, a gentigay eye can revolutionize."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_100_120.mp4",
    "text": " how companies operate. From automating mundane tasks, to making strategic decisions based on real-time data, this tech can free up human workers to focus on more creative and complex problems. It's like having the smartest, most efficient team member you could ever imagine. But with great power comes great responsibility."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_120_140.mp4",
    "text": " As we embrace a GENTIC AI, we also need to consider the ethical implications. How do we ensure these systems act in our best interest? What about privacy and security concerns? These are questions we need to address as we move forward. So, in a nutshell, a GENTIC AI isn't just another tech buzzword. It's a revolutionary..."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_140_160.mp4",
    "text": " advancement, etc. to impact every facet of our lives from home and health care to transportation and business. The future is closer than we think and it's going to be incredible. Alright, that's it for today. If you found this video insightful, give it a thumbs up and don't forget to hit that subscribe button for more tech deep dives. Thanks for watching and I'll see you next time."
  },
  {
    "clip_path": "ui/clips/8MyFDttUqbM_160_163.mp4",
    "text": " catch you in the next one."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_0_20.mp4",
    "text": " Hi there. Let me show you how data breaks changes data engineering by giving you everything you need in one AI-infused intelligent platform. And the one governance layer, the Data Intelligence Platform handles it all. Streaming in Chess-Tune."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_20_40.mp4",
    "text": " transformation and orchestration, add a fraction of the typical development time and on top of serverless compute. As you might know, any new project starts with data in Chesschen, in Chesting Data from Sources, such as Cloud Storage, Message Buses, Databases, or ERMs only takes me..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_40_60.mp4",
    "text": " minutes. Lakeflow Connect seamlessly and continuously imports your data from popular business applications such as Salesforce, Workday and Google Analytics. With Lakeflow Connect, it just takes a few clicks and your data appears as fully synchronized tables in your catalog."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_60_80.mp4",
    "text": " ready to be used across the organization. And of course, there are other options too, like getting datasets or even ML models from Databricks Marketplace and partners. After interesting it, our data is now available and governed by Unity Catalog. So we can start transforming the raw data and extract..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_80_100.mp4",
    "text": " meaningful information. In the past, this required writing complex Python code and applying heart to understand optimization techniques. But now we have the Data Intelligence platform, which is infused with AI that understands your interactions with the data, it's metadata."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_100_120.mp4",
    "text": " and your code. Databricks Assistant saves your team tons of development time and keeps everyone following best practices. Here I created a single materialized view that automatically maintains fresh pre-computed data, making it ideal for dashboards. With Delta Life Tanks."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_120_140.mp4",
    "text": " You scale these transformations into robust production grade pipelines, written in simple SQL or Python. Let's look at a data life tables pipeline in action. Here, in the pipeline view, you can see multiple tables and transformation steps. It provides a clear visualization of your data flow."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_140_160.mp4",
    "text": " Also, data reliability is guaranteed by continuous data quality monitoring and status tracking in real time. Best of all, your pipeline adapts to your needs. You can easily switch between batch and streaming to match the pace of your business. Well, you have now seen..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_160_180.mp4",
    "text": " how Databricks Streamlines individual tasks. But these steps are for only building blocks of a larger workflow. This is where Databricks workflows comes in, a fully serverless orchestration engine that orchestrates everything in the lake house and seamlessly coordinates all the required steps."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_180_200.mp4",
    "text": " such as triggering D9 gesture or transformation pipelines, orchestrating ML model updates, or automating dashboard refreshes for a business team. To summarize, think of the D9 Intelligence platform as you want to stop platform for D9 engineering."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_200_220.mp4",
    "text": " You can enchest data with Lakeflow Connect, transform it using simple SQL pipelines with Delta Life tables, and orchestrate every step in your data journey with Databricks workflows, all on one secure and serverless platform. Are you ready to transform your data engineering?"
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_220_230.mp4",
    "text": " Start today at dtabrics.com slash trial. Thanks for joining me in this data engineering overview."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_0_20.mp4",
    "text": " Data lineage is the history of how data is ingested, transformed and stored within a data pipeline. This is powerful information that you can leverage to make informed decisions about your architecture."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_20_40.mp4",
    "text": " One simple way to gather this information would be to interview the data engineers across your pipeline and ask which data sources they read from, what they do with the data, and where the data goes next. Then you could map out a data flow diagram like this toy example. Data is pulled in from sources here into some stores."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_40_60.mp4",
    "text": " like this data lake and this bucket, transformed by an ETL and loaded into a data warehouse. This is then used to populate a set of dashboards. One potential use case is risk modeling."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_60_80.mp4",
    "text": " For example, we can ask the question, what's affected if this store goes down? Following through the diagram, we see a knock on effect through the data warehouse and onto this dashboard. Now, the obvious downside to collecting your lineage data by interviews is that it's expensive time-consuming error pro."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_80_100.mp4",
    "text": " an out of date almost as soon as you're done collecting it. So maintaining it at a useful level of granularity would take an unreasonable amount of time. Clearly, we need to automate our data lineage. Digital tools, such as DBT, offer static analysis of data pipelines."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_100_120.mp4",
    "text": " These pass queries to map the flow of data through a system. This requires that all data movement and transformation takes place within the tool. So, if you were copying this CSV to this book outside the platform, it would be invisible to the analysis. Some platforms search a data..."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_120_140.mp4",
    "text": " bricks offer runtime automated lineage. These maintain a metadata store which records links between services when processes move data. This works well for common use cases but may not support niche operations. And again any operations carried out on"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_140_160.mp4",
    "text": " platform will be lost. Another option is data tagging. If all data transformers in your architecture conform to some schema then they can read in and extend data lineage metadata alongside transforming the data. So this ingest"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_160_180.mp4",
    "text": " could tag everything it ingests with the source system. Then this ETL could add further tags to show that it read data from this store, transformed it, and stored it in this data warehouse. This is handled automatically in services like Microsoft Pervue, but could also be done manually. Now it's..."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_180_200.mp4",
    "text": " that each transformer in the pipeline would need to update the metadata which can be expensive to implement. However, this granular detail means that by the time we reach figures in this dashboard, we know exactly where each one came from and how it was processed. Now we have"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_200_220.mp4",
    "text": " a granular up-to-date data lineage. Let's look at some other use cases. First, migration planning. Can I rename a table in the data warehouse? I met a data tells us which teams would need to be informed and whether some data is completely unused. Next,"
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_220_240.mp4",
    "text": " compliance and provenance. How can we verify the sorting correctness of this data? Being able to easily obtain a full list of data transformations applied is an order to the dream. And this is especially important to our clients in highly regulated industries like health and finance. Finally, root cause."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_240_260.mp4",
    "text": " analysis. Let's say this graph is wrong. Why? For a given data point we can see its complete journey through our pipeline, so for example we could detect if our ETL was not deduplicating correctly. This allows critical issues to be investigated and resolved as quickly as possible."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_260_280.mp4",
    "text": " Ultimately, maintaining data lineage is essential to understanding the flow of data through a business. The best approach depends on the tooling you employ. If supported, static analysis will yield good results for minimal effort."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_280_300.mp4",
    "text": " look into whether your platform offers runtime automated lineage. If neither of these is possible, then data tagging may be the best way forwards. As ever, you'll need to consider the realities of your existing architecture with what you want data lineage to achieve for you."
  },
  {
    "clip_path": "ui/clips/p11p1IzfGFE_300_306.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_0_20.mp4",
    "text": " Data is everywhere, but you already know that. But do you know who's responsible for all of this data? Well, it's the data engineer. There are lots of roles like the data analysts, data scientists and machine learning engineer that use data. But for them to be able to use this data and actually make it useful, somebody needs to build a system to collect"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_20_40.mp4",
    "text": " and store it. And that is where a data engineer comes in. The thing is that it kind of works like a pyramid. The data engineer leads the foundation and if the foundation isn't good, the entire thing is going to collapse. Now hopefully the data engineer of your company is as good as the ones that build the pyramids because they've been around quite a while. So what are the key responsibilities of"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_40_60.mp4",
    "text": " of data engineers. Well, the first thing is data collection. And this basically means that data engineers will create pipelines to store data from various sources. It could be a company database, the data might be available somewhere else, but the purpose here is to get it all into one place where we can really understand it and store it more properly. And I actually,"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_60_80.mp4",
    "text": " it useful. And that brings us on to the second part, which is data storage. It is really important that data is stored securely and in a matter that makes it accessible for other people in the organization that want to use this data, for example, to make important decisions and conclusions. Now we can't always store and use the data right away, so we may have to transform this data."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_80_100.mp4",
    "text": " data. And this is also a key responsibility of data engineers. They make clean and transform the raw format into a format that can actually be made useful for the company. It is also all about data availability. And you want data to be available to data scientists and to analysts and machine learning engineers for whatever you need it for. Now let's talk about the skills and tools that"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_100_120.mp4",
    "text": " you'll actually need to become a data engineer. Some of the key tools that you should know are SQL or SQL, used for managing databases, more specifically relational databases, and in general, a strong understanding of database systems and different forms of architecture. It is also helpful to be strong in a program in language, like Python, which is widely used as well."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_120_140.mp4",
    "text": " Now one thing that's becoming more and more important in data engineering is the use of cloud as it's becoming more popular in companies in general. Large data sets are the new deal and therefore we need to know how to deal with them. For that reason it's great to know a cloud solution, for example AWS or Google Cloud Platform or Microsoft Azure. The most popular ones are AWS"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_140_160.mp4",
    "text": " and Microsoft Azure, but there are also a lot of other good alternatives and some of the other tools commonly used are Apache Hadoop for processing large data sets and also Spark for data processing. I recommend this website where you can actually see the skills that our employers are looking for directly on the site and the data is scraped directly from real job listings so that you"
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_160_180.mp4",
    "text": " actually know what they're looking for on the real job market. Not just what some random guy on YouTube is telling you, although I do try to base my opinions on real data. Now, how do you break into data engineering? Well, there are of course many, many ways just like with any job and it depends on your prior experience or education or lack of education. Many data engineers will have a bad."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_180_200.mp4",
    "text": " degree at least in, for example, computer science or a related field, but also other degrees like masters can be pretty common as well. Some people go directly into data engineering, whereas other people go from another data role or some other technical role and then transition into data engineering. This might also be better option if you don't have a strong education."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_200_220.mp4",
    "text": " an old background or a solid degree, as it can be quite difficult to get your first job without a degree in data engineering if you're starting out. All in all, I think data engineering is a very exciting field and it's only going to grow in the future and I wish you all the best starting your career in data engineering or perhaps you're more interested in data science or data analytics. Thanks."
  },
  {
    "clip_path": "ui/clips/6jXCgNG2JXw_220_231.mp4",
    "text": " watching this video please drop a like and subscribe if you did enjoy it and also somewhere on the screen here there should be another video that I think you might be interested in or at least a YouTube algorithm think so so check it out."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_0_20.mp4",
    "text": " Right from the start, Software Development comprised two different departments. The development team that develops the plan, designs, and builds the system from scratch, and the Operation Team for testing and implementation of whatever is developed. The Operations Team gave the development team feedback on any bugs that needed fixing, and any rework."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_20_40.mp4",
    "text": " required. Inveribly, the development team would be idle awaiting feedback from the operations team. This undoubtedly extended timelines and delayed the entire software development cycle. There would be instances where the development team moves on to the next project, while the operations team continues to provide feedback for the previous code."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_40_60.mp4",
    "text": " weeks or even months for the project to be closed and final code to be developed. Now, what if the two departments came together and worked in collaboration with each other? What if the wall of confusion was broken? And this is called the DevOps approach. The DevOps symbol resembles an infinity sign suggests"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_60_80.mp4",
    "text": " that it is a continuous process of improving efficiency and constant activity. The DevOps approach makes companies adapt faster to updates and development changes. The teams can now deliver quickly, and the deployments are more consistent and smooth. Though there may be communication challenges, DevOps manages a streamlined flow between the teams."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_80_100.mp4",
    "text": " and makes the software development process successful. The DevOps culture is implemented in several phases with the help of several tools. Let's have a look at these phases. The first phase is the planning phase, where the development team puts down a plan, keeping in mind the application objectives that are to be delivered to the customer. Once the"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_100_120.mp4",
    "text": " plan is made, the coding begins. The development team works on the same code and different versions of the code are stored into a repository with the help of tools like Git and Merge when required. This process is called version control. The code is then made executable with tools like Maven and Gradle in the build stage. After the code is successful,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_120_140.mp4",
    "text": " built. It is then tested for any bugs or errors. The most popular tool for automation testing is Selenium. Once the code has passed several manual and automated tests, we can say that it is ready for deployment and is sent to the operations team. The operations team now deploys the code to the working environment. The most"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_140_160.mp4",
    "text": " Those prominent tools used to automate these phases are Ansible, Docker, and Kubernetes. After the deployment, the product is continuously monitored, and Nagios is one of the top tools used to automate this phase. The feedback received after this phase is sent back to the planning phase, and this is what forms the core of the DevOps lifecycle."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_160_180.mp4",
    "text": " that is the integration phase. Jenkins is the tool that sends the code for building and testing. If the code passes the test, it is sent for deployment, and this is referred to as continuous integration. There are many tech giants and organizations that have opted for the DevOps approach. For example, Amazon, Netflix, Walmart,"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_180_200.mp4",
    "text": " Facebook, and Adobe. Netflix introduced its online streaming service in 2007. In 2014, it was estimated that a downtime for about an hour would cost Netflix $200,000. However, now Netflix can cope with such issues. They opted for DevOps in the most fantastic way."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_200_220.mp4",
    "text": " Netflix developed a tool called the Simeon Army that continuously created bugs in the environment without affecting the users. This chaos motivated the developers to build a system that does not fall apart when any such thing happens. So, on this note, here is a quiz for you. Match the DevOps tool with the phase it is used in."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_220_240.mp4",
    "text": " A. B. C. D. None of the above. Today, more and more companies lean towards automation. With the aim of..."
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_240_260.mp4",
    "text": " reducing its delivery time and the gap between its development and operations teams. To attain all of these, there's just one gateway. DevOps! If you enjoyed this video, if you did, a thumbs up would be really appreciated. Here's your reminder to subscribe to our channel, and to click on the bell icon for more on the latest technologies and"
  },
  {
    "clip_path": "ui/clips/Xrgk023l4lI_260_279.mp4",
    "text": " trends. Thank you for watching and stay tuned for more from SimpliLearn."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_0_20.mp4",
    "text": " Ever wonder how Netflix updates its apps without crashing? Or how Instagram adds new features almost daily? Behind every successful app update is a powerful approach called Dev Ups. Today, we'll break down exactly how modern companies deliver software updates so smoothly."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_20_40.mp4",
    "text": " and reliably. Before DevOps, software development was divided into two separate teams. Developers wrote the code, and operations teams deployed and maintained it. This separation created major problems. Long delays between updates, frequent conflicts between"
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_40_60.mp4",
    "text": " between teams, lots of deployment errors, slow recovery from problems, and frustrated users waiting for fixes. Companies needed a better way to build and deliver software. This is where DevOps comes in. Instead of keeping teams separate, DevOps."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_60_80.mp4",
    "text": " combines development and operations into a single, smooth process. Here's how it works. First, we start with planning and coding. Developers write new features or fix bugs in small, manageable pieces. Next comes the most important part, automation. Before..."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_80_100.mp4",
    "text": " For any code goes live, it passes through a series of automated checks. Code quality tests, security scans, performance tests, integration checks. If everything passes, the code automatically moves to deployment. But here's the clever part."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_100_120.mp4",
    "text": " happens gradually. New code rolls out to a small group of users first. Then, slowly expands if everything works well. The whole time, we're monitoring everything. Application performance, error rates, user experience, and system health. This creates a continuous cycl..."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_120_140.mp4",
    "text": " of improvement, where feedback directly influences the next updates. To make DevOps work, you need the right tools at each stage. Let's break down the most important ones. First, is Git. It's a version control system that tracks every change made to the code."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_140_160.mp4",
    "text": " Think of it as a highly organized record keeper. It stores every version of your code, tracks who made each change, and lets teams works together without overriding each other's work. When something breaks, you can easily see what changed and who changed it. Next is Docker. It packages..."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_160_180.mp4",
    "text": " applications and everything it needs to run into a single unit called a container. This solves a huge problem in software development, making sure your application runs the same way everywhere. No matter where you run the container on your laptop, a test server, or in production, it behaves exactly the same way."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_180_200.mp4",
    "text": " Then we have two types of automation tools, Jenkins or GitHub Actions. These are automation servers that handle repetitive tasks. Every time someone updates the code, these tools automatically build the application, run all the tests, check for security issues,"
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_200_220.mp4",
    "text": " to test environments, and if everything passes, deploy to production. Finally, Kubernetes. This is what we call a container orchestration platform. It manages your application and production by automatically scaling when more users are active, healing itself when something fails."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_220_240.mp4",
    "text": " Managing updates without downtime, balancing the workload across servers, and monitoring application health. What does all this mean in practice? With DevOps, companies can now release updates multiple times per day. Catch problems before they affect you."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_240_260.mp4",
    "text": " users, fix issues in minutes instead of days, scale applications instantly when needed, and keep system secure and reliable. For users, this means more frequent app updates, fewer crashes and bugs, new features arrive"
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_260_280.mp4",
    "text": " better app performance and quicker problem resolution. Let's see how companies use DevOps in the real world. Take Netflix. They can update their app while millions are streaming. When they added, are you still watching feature? They first tested it with just 1% of users."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_280_300.mp4",
    "text": " Monitor the response and gradually roll it out to everyone. Or your banking app. It updates security features weekly without ever losing your data. It's like changing a car's tires while driving. Even gaming companies like Fortnite can add new features while millions are playing and roll back instantly."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_300_320.mp4",
    "text": " something goes wrong. That's the power of DevOps. Constant improvement without disruption. Now you know the secret behind those smooth app updates. Modern software delivery isn't about working harder. It's about working smarter with the right processes and tools."
  },
  {
    "clip_path": "ui/clips/NDecuKLxfpo_320_338.mp4",
    "text": " Next time, your favorite app updates, you'll know there's an entire DevOps system working behind the scenes to ensure everything runs perfectly. Thanks for watching! If you want to learn more about modern technology, hit subscribe for new videos every week."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_0_20.mp4",
    "text": " Alright, let's talk about something that has completely changed the ways software is built and delivered. Devops. Now I know what are you thinking? Oh no, another complicated tech birth world. But trust me by the end of this video, you will not only understand what DevOps is, but you will also see why every modern company needs it."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_20_40.mp4",
    "text": " And don't worry, I am going to explain it with a real world story, so it will actually make sense for you. So imagine you and your friends are organizing a college event. Big annual fest."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_40_60.mp4",
    "text": " happens in almost all the colleges. So one team is a planning team and we can call them developers. Another team is setting up the venue, doing all the lights and all sound stuff. We can call them operations. Now let's say the event team plans an awesome concert and sends the plan to the venue team at the last minute. But guess what?"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_60_80.mp4",
    "text": " The venue team was ready. The stage is in setup, the lights are broken, there is no sound system. So what will happen? Chaos, right? The event will get delayed, everyone blames each other, this will be the situation. And the audience will..."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_80_100.mp4",
    "text": " get frustrated. And this is how software development used to work before. Developers would write code, build new feature and then throw it to IT team saying, hey, now it's your job. But when the app is crashed or when it didn't work, the IT team would be like, oh well, this is not my problem. You have it in the back."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_100_120.mp4",
    "text": " bad code. It's your problem. So it will be like a blame game situation and it will lead to slow releases, frustrated user, clients and that was the old way. Now the solution for this. Imagine instead of even team and when you team working without each other they will work together from the start."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_120_140.mp4",
    "text": " They will communicate constantly, the venue team starts setting up as soon as the event team finalizes the plan, they test the sound, light, stage, before hand, before the event day and make sure that everything is working perfectly. Now everything runs smoothly and the audience is happy and the event is a success."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_140_160.mp4",
    "text": " So, this is exactly what DevOps does in the tech world. DevOps means developers and operations. So, developers and operations work together instead of separately. It is about collaboration. No more blame games. Dev and IT team should work as a team. It is about automation."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_160_180.mp4",
    "text": " Task like testing development happens automatically. It is about faster and more reliable releases. Apps are updated quickly without breaking. And trust me, this makes the software development faster. It makes the software development smoother and much less stressful. But how does this dev-off-"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_180_200.mp4",
    "text": " actually work, how does this happen in real life if I say that there are three main things which DevOps team will do. Number first continuous integration we call it CI. So think of this like a constantly telling small part of event setup. So like problems are caught early. So in tech means that developer keep."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_200_220.mp4",
    "text": " emerging updates and you know the system checks for the issue automatically and it's like constantly making sure that the food and like the food is fresh the lights are working the sound is clear so everything we check every r or something okay process will be very smooth and comes c i cd pipeline this is the heart of DevOps it's"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_220_240.mp4",
    "text": " It's like a smooth assembly line where all these checks and updates happens automatically without human intervention. So here's how it works. I'll tell you. Number first, developers write new code. Number second, the CI part automatically tests the code for the bugs. If there is any bug or anything. So if the code passes that..."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_240_260.mp4",
    "text": " test, the CD pipeline deploys it to the production. Production means you'll see in the live server where the clients are using it. For the users to see is basically. And then the system monitors the app real time to catch any errors or any bug earlier. So basically CI CD pipeline is like a robot chef who makes"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_260_280.mp4",
    "text": " sure that everything runs smoothly without any human mistake and it gets the app to user quickly. And this is why big companies like Netflix, Amazon, Google, every company like these runs on DevOps. And it keeps everything running faster smoothly and the users are"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_280_300.mp4",
    "text": " happy, always happy. But then why should you care about DevOps? That's the question I'm a developer why should I care about DevOps? If you're just starting in tech you might be wondering like that but I'll tell you why companies love DevOps engineer. It's one of the highest paying tech jobs."
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_300_320.mp4",
    "text": " It makes your work easier. No more it will be like it works on my machine excuses. You don't have to be hard code for this. DevOps is more about smart automation than writing a complex algorithm. So whether you are a developer, whether you are a system admin,"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_320_340.mp4",
    "text": " whether you are someone who wants to work in tech. Learning DevOps will open a lot of opportunities for you. So next time if someone says DevOps, just remember it's like running a big event smoothly with a good teamwork and automation. So no last minute panics, no blame games, just a fast and a smooth"
  },
  {
    "clip_path": "ui/clips/J0eBHrTN9tA_340_356.mp4",
    "text": " for everyone. And that my friend is DevOps. Thank you so much for watching this video till the end. I hope you like this video. If you really like it share it with your friend who are looking for the possibilities and make sure to drop a comment about how the video was. I'll see you in the next one."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_0_20.mp4",
    "text": " We are predicting the company name by providing the input text. Hello all, welcome back to the Cloud and Data Science. As we all know, large language models is the hard topic in today's industry. In this video, we are going to see how to integrate LAN chain with large language models."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_20_40.mp4",
    "text": " using Azure OpenAI or OpenAI. Langchain provides a framework for developing the applications using the Logge Language models. It provides easy to use abstraction for working with Logge Language models. Logge Language model provides various components, various modules."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_40_60.mp4",
    "text": " to consume the language models using Azure OpenAI or OpenAI. The following are the modules as we see in the screen, model interface. So it provides an IOW interface to integrate and interact with the language models. Azure through Azure OpenAI"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_60_80.mp4",
    "text": " or WAPIN AI. The other important feature is data connection. Currently, large language models are trained on the generic data. So it provides interface to interact integrate with application specific data, domain specific data so that we can get more domain space."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_80_100.mp4",
    "text": " weak accurate predictions and results. And lying chain also provides construct to for the sequence of the calls and it provides memory to store the state between runs of a chain. Also it provides"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_100_120.mp4",
    "text": " to log these steps in between the chain. So the two important features I like one is domain specific data to integrate with domain specific data and the other one is state maintenance between the chains of the calls and sequence of the calls."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_120_140.mp4",
    "text": " As I mentioned, these are the important components in the Lang chain. It provides prompts and once we got the response output, it provides output process to parse the their X data and it also provides various example selectors and it provides documents."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_140_160.mp4",
    "text": " load us to load the documents and text splitters to split the text and it provides memory to store the state between the chain of calls and it provides various models access to various models and it also provides vector stores to store the data in the form of vectors and"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_160_180.mp4",
    "text": " and it provides retrievers to retrieve the application specific data or domain specific data. These are the some of the use cases specific to the chains. It helps to build the agents, chatbots and question answering systems. And it provides ablur data analysis."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_180_200.mp4",
    "text": " And it also provides natural language APIs to integrate and interact with various natural language models. And it also provides a provision to evaluate the results and it also provides way to extract the data and to summarize the data. These are some of the use cases specific to the language."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_200_220.mp4",
    "text": " Now let us see how to use this Lang chain to build our first LLM application. As we seen the screen there are four steps. First we need to install the Lang chain using the pip install command and second step is"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_220_240.mp4",
    "text": " we need to install the open API again using pip install open API, open a and then once we have the open AI key, we need to get our own open AI key after registering with open AI, then we will initialize the open AI LLM class to start our predictions. We will provide the"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_240_260.mp4",
    "text": " input parameter to the LM class and get the output as a result. So let us see those four steps in action. Go to that your VS code or any editor. So first step as I mentioned we have to install the"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_260_280.mp4",
    "text": " Langchain once we have the Langchain installed then we have to install the OpenA Then once we have the OpenA then Third step is we need to get the key. How do we get the OpenA key? Simply you have to go and register for the OpenA and there you can create new secret key"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_280_300.mp4",
    "text": " copy that new secret key and then paste that new secret key here. Once you have the key then all you need to do is you need to import that open-A wrapper from the Langchen. Langchen provides the wrapper to the open-A. You can also use Azure Open-A once you have"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_300_320.mp4",
    "text": " Now that Openier app, then you can initialize the large language model class, LLM class, and then you can call predict method, LLM dot predict. In this example, what we are predicting is what would be a good company name for a company that makes colorful socks, that is the input parameter to the predict method."
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_320_340.mp4",
    "text": " dhn the output is feed full of fun this is the output this is the company name. So, this is the simple application what we are doing is we are predicting the company name by providing the input text using the large language model and by using the lang chain"
  },
  {
    "clip_path": "ui/clips/piVqFcuBV2c_340_355.mp4",
    "text": " interface to integrate with the WAPINYA. Thanks for watching. In the next video, we are going to see more in detailed steps of use cases. Thanks for watching. Please don't forget."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_0_20.mp4",
    "text": " Welcome back to BraveHub. In today's video, I'm going to teach you the basics of Leng Chain in under 5 minutes. If you're looking to quickly get started with this powerful tool for building language model applications, you're in the right place. Let's jump in. Leng Chain is a framework designed to simplify the development of applications."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_20_40.mp4",
    "text": " powered by large language models, LLMs, it's perfect for creating chatbots, knowledge-based systems, and other AI-driven applications. What makes Lengchains stand out is its ability to chain together multiple components, like prompts, memory and tools, making the development process much more efficient. First,"
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_40_60.mp4",
    "text": " First, let's install Leng Chain. Open your terminal and run the following command. Pipp install Leng Chain. This command will install Leng Chain and its dependencies. Once that's done, you're ready to start building. Let's create a simple Leng Chain that combines a prompt with an LLM to generate text. Here,"
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_60_80.mp4",
    "text": " Use how it's done. In this example, we're creating a Leng Chain that takes a name as input, and generates a short poem about that person, using OpenAI's GPT3. The LM Chain object combines the prompt with the LLM, making it easy to generate customized content. One of the powerful features of Leng Chain."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_80_100.mp4",
    "text": " is its ability to add memory to your chains, allowing your application to remember context across interactions. Let's see how that works. Here, we're adding simple memory to our length chain, enabling the application to retain information between runs. This is useful for building more interactive and context-aware applications."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_100_120.mp4",
    "text": " LangChain also supports integrating tools like web search, calculators and databases, making your applications even more powerful. Let's add a simple tool to our chain. In this example, we're using a simple calculator tool within our LangChain. The chain now not only generates text, but can also perform calculation."
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_120_140.mp4",
    "text": " making it a more versatile solution. And that's a quick overview of Leng Chain in under 5 minutes. We covered how to install Leng Chain, create a simple chain, add memory, and integrate tools. Leng Chain is incredibly powerful, and I encourage you to explore its full capabilities. If you found this video helpful,"
  },
  {
    "clip_path": "ui/clips/bV5wI0DB7YM_140_157.mp4",
    "text": " Please give it a thumbs up, subscribe for more quick tutorials, and hit the notification bell so you never miss an update. Got questions or ideas for future videos? Drop them in the comments below. Thanks for watching, and see you in the next video."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_0_20.mp4",
    "text": " Hi everyone and welcome back to TechWorld with Preet. In today's video we are diving into a fascinating question. What is Langchin and why should you use it? Langchin is an exciting tool that's gaining momentum in the AI space and by the end of"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_20_40.mp4",
    "text": " this video, you'll not only understand what it is, but also have a strong sense of how it fits into modern AI workflows. Let's get started. Coming back to question what is Langchene and why do we use it in the first place? What exactly is it doing for us?"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_40_60.mp4",
    "text": " Well, answering that question is a big trickier than it seems. So to really understand it, we are going to walk through a short journey across this video and the next few. To make things more concrete, let's look at a real world use case. Have you ever wondered how some apps let you interact with large top?"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_60_80.mp4",
    "text": " documents, like asking questions about the contents of a PDF. We'll start by looking at a public web app called pdf.ai. It's not something I built, someone else did, but it's a great example. Take a look at web app. Next we will understand how this app works."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_80_100.mp4",
    "text": " and we can use that knowledge to better understand Langchain. Let me show you a quick demo. First, sign into PDF.ai with your Google account. Once you logged in, you will be taken to the upload page where you can upload your PDF file."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_100_120.mp4",
    "text": " Icept uploaded a 277 page document about the cracking the technical interview to pdf.ai. On the left you can see the document and on the right I can type questions about it."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_120_140.mp4",
    "text": " design an algorithm to find all pairs of integers within an array which some to a specified value. After a brief pause the app gives me an answer. If you check closely it's getting the information directly from the PDF. What's even cooler is that it shows the exact section where the"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_140_160.mp4",
    "text": " info came from. For example, it says the answer is from page 91. If we search for page 91 in the PDF, we can see the same details as shown in the chat response. So, how does this magic happen? Let's take"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_160_180.mp4",
    "text": " Make a look behind the scenes to understand how an app like this is built. Here's what's happening behind the scenes. Text Extraction. The app reads the PDF and extracts the text, breaking it into smaller sections, embedding scenes generation. Each section is converted."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_180_200.mp4",
    "text": " into vector embeddings, a mathematical representation of text meaning. Question handling. When you ask a question, it's converted into an embedding and the system compares it to the embeddings from the document to find relevant sections. AI response. Finally, check GPT-O."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_200_220.mp4",
    "text": " a similar model processes the relevant sections and generates a response. Next question comes in mind what Lanchine can do. Lanchine is a powerful tool that helps developers build smart AI apps and this image shows all the cool things."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_220_240.mp4",
    "text": " it can do. Let's break it down. Checkports. You can build checkports that talk like humans using real-time data and memory. Agents. These are AI tools that can decide what to do next, like searching the web or calling APIs. Interacting with APIs. Langching can cause..."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_240_260.mp4",
    "text": " connect with other services and fetch real-time info using APIs. Code understanding, it can help explain or work with code, like debugging or code review. Quarring tabular data, Lanching can understand and answer questions from data tables like Excel Sheets."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_260_280.mp4",
    "text": " You can test how well your AI app is performing, extraction. It can pull out specific information from documents or files. Question answering using docs, ask questions. And it finds answers from your files or documents, summarization. It reads long."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_280_300.mp4",
    "text": " content and gives you a short meaningful summary. Why is Lanchine so popular? Think of Lanchine like a kitchen helper for developers. Imagine you're a chef and someone has already chopped the veggies, measured the spices and got everything ready for you."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_300_320.mp4",
    "text": " to do is cook the meal. Lanchine does the same thing for developers. It takes care of all the hard setup. So you can just focus on building your AI app. So that's Lanchine in a nutshell. A framework that turns the complex into the simple making it easier to"
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_320_340.mp4",
    "text": " build powerful lm applications. In upcoming videos we'll go deeper into Lengen's features, build real-world projects and even talk about bringing these apps to production. If you found this helpful don't forget to like share and subscribe to TechWorld with breed. Let's get started."
  },
  {
    "clip_path": "ui/clips/vhcoXDeUUIE_340_344.mp4",
    "text": " Keep learning and building amazing things together. See you next."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_0_20.mp4",
    "text": " Let's talk about Langchain. What is it? How does it work? Why should you care? And how you can get started today? All of that and more in two minutes. Start the timer. Langchain is an open-source framework designed to make it easier for developers to build powerful applications that interact with language models."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_20_40.mp4",
    "text": " like GPT-4, and Thropix-Clawed, Cohear Models, and Beyond. Before Lengchain building applications with language models often required a lot of custom code to handle tasks like memory management, prompt optimization, and interacting with external data sources. Lengchain simplifies this by offering modular tools that handle these challenges"
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_40_60.mp4",
    "text": " more efficiently. In practice, Lengchain lets you define different components, like memory management and API connections that interact seamlessly with language models. Think of it like Lego for developers. You can snap together the pieces you need to create custom workflows that perfectly fit your application. Now that's great in all, but why should you care?"
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_60_80.mp4",
    "text": " Well first, it's incredibly flexible. Unlike other frameworks, Lengchains modular design lets developers easily integrate various components, whether it's language models, APIs, or custom logic into one cohesive application. This flexibility means you can quickly assemble anything from a chatbot to a document summarizer without reinventing the wheel."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_80_100.mp4",
    "text": " Second, Lengchain makes handling contacts much easier. One of the biggest challenges with language models is their inability to remember long conversations for large data chunks. Lengchain provides built-in memory management tools that allow your applications to maintain conversations stay over time or dynamically retrieve relevant documents. Third, Lengchain"
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_100_120.mp4",
    "text": " It's a pain-shine when it comes to real-time data integration. It works smoothly with external services like databases, APIs, and cloud platforms, enabling your applications to pull in live information and provide contextually accurate responses. This makes it ideal for building systems that need up to the minute data or complex external interactions."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_120_140.mp4",
    "text": " If you're still not convinced, here are a few additional reasons why you may want to dive into Lengcheng. Reason number one, accessibility. Whether you're a beginner looking to explore language models or a seasoned developer building advanced AI applications, Lengcheng's modular structure adapts to your needs. You can start simple and grow your applications complexity as you learn more."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_140_160.mp4",
    "text": " Reason number two, support. Length chain boasts an active and growing community, offering plenty of documentation, tutorials and support. Whether you're debugging, learning, or sharing your own use cases, you're not alone. There's a whole community of developers ready to help. And finally, Reason number three, job prospects. AI and machine learning engineers are enough."
  },
  {
    "clip_path": "ui/clips/lO9PA3JkDdk_160_176.mp4",
    "text": " high-demand. With salaries averaging over $128,000 in the US, a cordon is hit for cruder. Ready to dive in? Check out Zero to Mastery's Lane Chain Bite Size course. Will you learn to build state-of-the-art large language model-powered applications?"
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_0_20.mp4",
    "text": " What exactly is Generative AI? When new content is created by artificial intelligence, it's called Generative AI. This could involve generating texts and images, as well as videos, music or voices. To do this, you describe in a chat dialogue box what you want the AI to be."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_20_40.mp4",
    "text": " create. This description is called a prompt. The Generative AI tools provide answers to all sorts of questions, summarise complex information and generate diverse ideas quickly. Depending on how they are used, they can create short stories, paintings, pieces of code, or even"
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_40_60.mp4",
    "text": " musical compositions. The foundation for this creation lies in large amounts of data that the AI system accesses to identify patterns and similarities. The content produced by the AI is new. It's often impressive and challenging to distinguish from things humans have made."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_60_80.mp4",
    "text": " A generative AI can also be misused. In so-called deep fakes, AI is utilized to produce images or videos that seem real. AI-generated texts are also tough to recognize as machine-made. Moreover, the AI can provide answers that sound..."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_80_100.mp4",
    "text": " correct but are actually incorrect. This is called hallucinating. The quality of what's created depends on both the quality of the data used and the quality of the prompts given. To effectively utilise generative AI, we need to learn how to guide the tools with meaningful prompts."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_100_120.mp4",
    "text": " and use them thoughtfully. Generative AI holds immense potential and can help us in many ways, such as serving as a writing or learning partner. However, the AI should do the hard work and humans should be responsible for the facts."
  },
  {
    "clip_path": "ui/clips/rwF-X5STYks_120_122.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_0_20.mp4",
    "text": " Agendic AI represents a significant evolution in artificial intelligence that focuses on autonomous decision making and action-taking capabilities. Let's break down this concept in simple terms. What is Agendic AI? Agendic AI refers to artificial intelligence systems capable of..."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_20_40.mp4",
    "text": " of autonomous action and decision making. Unlike traditional AI that follows strict rules or generative AI that creates content, agentic AI can independently pursue goals with minimal human supervision. Think of agentic AI as a factory manager rather than a factory worker."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_40_60.mp4",
    "text": " On AI agent, might perform one specific task, like a factory worker on an assembly line, Agentic AI orchestrates multiple processes and makes decisions about how to achieve broader goals, like the factory manager overseeing operations. Key characteristics of Agentic AI. Agentic AI."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_60_80.mp4",
    "text": " systems are distinguished by four fundamental characteristics. They can make decisions and take actions without constant human oversight. They focus on achieving long-term objectives rather than just completing specific tasks. They learn from experiences and adjust their strategies based on changing conditions."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_80_100.mp4",
    "text": " They anticipate needs and take initiative rather than just reacting to commands. Here are the key differences between Agendic AI and other forms of AI. Agendic AI versus traditional AI. Traditional AI systems operate based on predefined rules and algorithms requiring"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_100_120.mp4",
    "text": " explicit instructions for every task. They're reactive and limited to specific domains. If traditional AI is like a calculator that can only perform these SAT calculations you input, Agendic AI is like a financial advisor who can analyze your finances, set goals, and make investment to see..."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_120_140.mp4",
    "text": " on your behalf. Agentech AI vs Generative AI. Generative AI, like chat GPD, creates content such as text, images, and videos based on prompts. It's primarily focused on creation and requires human guidance. It Generative AI."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_140_160.mp4",
    "text": " is like a highly skilled artist who can paint whatever you describe. A GEDDIC AI is like a home renovation contractor who not only designs or kitchen but also orders materials, coordinates workers, and completes the project with minimal direction from you. Generative AI focuses on creating while a GEDDIC AI focuses"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_160_180.mp4",
    "text": " on doing real world examples of Agenic AI. They act as your personal assistant. In other words, an AI that can book your flight, schedule transportation to a hotel, and make dinner reservations with a single command. AI agents and call centers can analyze customers"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_180_200.mp4",
    "text": " sentiment, review order history, access company policies, and respond to customer needs simultaneously. When it comes to supply chain management, they act as systems that can forecast a man fluctuations and proactively adjust inventory levels to prevent shortages. In multimedia creation, the agent"
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_200_220.mp4",
    "text": " Heligates research, text generation, image selection, and design to other AI systems to create a complete multimedia report. In scientific discovery, they act as an AI agent that can identify new materials or drug compounds, select optimal suppliers, and even order necessary materials."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_220_240.mp4",
    "text": " Agenic AI represents the next frontier in AI development, with many experts viewing it as a crucial step toward artificial general intelligence AGI. As these systems continue to evolve, they promise to transform how we work, solve problems, and interact with technology."
  },
  {
    "clip_path": "ui/clips/TS3DO0ZM65Q_240_252.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_0_20.mp4",
    "text": " Hello everyone, welcome back to the IMITZTookChannel and today we are going to dive into the fascinating"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_20_40.mp4",
    "text": " This video we are going to cover how chain AI model 4 and what are their capability. You might be heard about what is chain AI and you might be know what is the Google part and what is the chain AI. These model have a lot of curiosity among the people but they can also be confusing when we talk about the chain AI. So when we talk about the chain AI, we are going to be able to see the chain AI model"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_40_60.mp4",
    "text": " the genitive AI, many people think it is a magical model that write a human language such as chaggitp 3.5 or 4 model and if you know about the Gemini which is for the Google. These models were popular and they were different approach and they are different. In this video I will"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_60_80.mp4",
    "text": " we provide a clear and simple explanation what the real life example to help you to understand what is Genetics, Genetics AI is all about. So let's get started. So Genetics AI fall under a broader category of the artificial intelligence known as a machine learning. You might be already familiar with the"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_80_100.mp4",
    "text": " machine learning, where computer learn performed task to without being a specific program. They are two-minute type of machine learning with a super-wide machine learning and so on, super-wide machine learning. Super-wide machine learning is involved training models on the liberal data to make a prediction or a classification. On the other hand, we have"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_100_120.mp4",
    "text": " have unscrupized machine learning deal with the finding the pattern on the structures in unlabeled data. Now, let's talk about the generative AI. In generative AI, they are two main type of model. One is the discriminative model and another is the generative models. So, discriminative models are 4."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_120_140.mp4",
    "text": " focus on the distance between the categories or classes. By generating model, our focus on generating on the new data. Now let's example, let's say we have a generative model train on the cat images or let's say dog images. When given a new output, so..."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_140_160.mp4",
    "text": " on the description of a CAD, what the CAD slide, what see a bit full of the CAD, the disability to create new content when it is said to be a Genitive AI model apart. Now it is discussed some real example of the Genitive AI. So this model can be used for the images."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_160_180.mp4",
    "text": " generations, text analysis and we ever viewed generation and sector. Companies are leveraging the genitives knowledge for the various tasks such as the content creations, data argumentations, data sentiment analysis, even the genitings of personalized recommendations. Now if you look at the core, it's the"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_180_200.mp4",
    "text": " are have heavily using to converting to the shifting to the gen A because they are having different problem now the gen A I can solve their problem by using this large LAM models. So genitive A I have been involved over time some architecture more complex like G24 or"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_200_220.mp4",
    "text": " vast amount of data to generate divers on high quality content. So in this conclusion the AI goals are really good emotions in the innovations and creativity by understanding these models. So let us explore this video sphere. So in the upcoming big guy one."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_220_240.mp4",
    "text": " to launch a bootcamp based on the chain and where we have a top faculty discussing what is the genitive and how you can implement the genia into this one. In upcoming 2 or 3 years this genia will be changed differently in the quadrits in the real life example. So you must know what is the genitive and how you can implement the genia into this one."
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_240_260.mp4",
    "text": " a janeai and how to implement janeai into this work. So if you want to know about the janeai and the machine learning you can check out this follow link in the bio to check the description and the website page where we have a different course on the machine learning and janeai and the data engineering courses. So I hope you found this video very useful and"
  },
  {
    "clip_path": "ui/clips/sv69UAvVOLo_260_276.mp4",
    "text": " And if you need more content along the data science, and don't forget to comment down below, and don't forget the heat on the icon button, and stay updated on the latest video. Thanks everyone for the watch."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_0_20.mp4",
    "text": " Hi there. Let me show you how data breaks changes data engineering by giving you everything you need in one AI-infused intelligent platform. And the one governance layer, the Data Intelligence Platform handles it all. Streaming in Chess-Tune."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_20_40.mp4",
    "text": " transformation and orchestration, add a fraction of the typical development time and on top of serverless compute. As you might know, any new project starts with data in Chesschen, in Chesting Data from Sources, such as Cloud Storage, Message Buses, Databases, or ERMs only takes me..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_40_60.mp4",
    "text": " minutes. Lakeflow Connect seamlessly and continuously imports your data from popular business applications such as Salesforce, Workday and Google Analytics. With Lakeflow Connect, it just takes a few clicks and your data appears as fully synchronized tables in your catalog."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_60_80.mp4",
    "text": " ready to be used across the organization. And of course, there are other options too, like getting datasets or even ML models from Databricks Marketplace and partners. After interesting it, our data is now available and governed by Unity Catalog. So we can start transforming the raw data and extract..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_80_100.mp4",
    "text": " meaningful information. In the past, this required writing complex Python code and applying heart to understand optimization techniques. But now we have the Data Intelligence platform, which is infused with AI that understands your interactions with the data, it's metadata."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_100_120.mp4",
    "text": " and your code. Databricks Assistant saves your team tons of development time and keeps everyone following best practices. Here I created a single materialized view that automatically maintains fresh pre-computed data, making it ideal for dashboards. With Delta Life Tanks."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_120_140.mp4",
    "text": " You scale these transformations into robust production grade pipelines, written in simple SQL or Python. Let's look at a data life tables pipeline in action. Here, in the pipeline view, you can see multiple tables and transformation steps. It provides a clear visualization of your data flow."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_140_160.mp4",
    "text": " Also, data reliability is guaranteed by continuous data quality monitoring and status tracking in real time. Best of all, your pipeline adapts to your needs. You can easily switch between batch and streaming to match the pace of your business. Well, you have now seen..."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_160_180.mp4",
    "text": " how Databricks Streamlines individual tasks. But these steps are for only building blocks of a larger workflow. This is where Databricks workflows comes in, a fully serverless orchestration engine that orchestrates everything in the lake house and seamlessly coordinates all the required steps."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_180_200.mp4",
    "text": " such as triggering D9 gesture or transformation pipelines, orchestrating ML model updates, or automating dashboard refreshes for a business team. To summarize, think of the D9 Intelligence platform as you want to stop platform for D9 engineering."
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_200_220.mp4",
    "text": " You can enchest data with Lakeflow Connect, transform it using simple SQL pipelines with Delta Life tables, and orchestrate every step in your data journey with Databricks workflows, all on one secure and serverless platform. Are you ready to transform your data engineering?"
  },
  {
    "clip_path": "ui/clips/qpZlAADOJCA_220_230.mp4",
    "text": " Start today at dtabrics.com slash trial. Thanks for joining me in this data engineering overview."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_0_20.mp4",
    "text": " This is the most comprehensive cursor tutorial on this planet. So make sure you watch till the end to build your own dream start up using cursor. You will ask who is this tutorial for? This tutorial is specifically designed for someone who is a complete non-coder or a coder. Or anyone who wants to start using cursor to build their own apps, web apps, mobile apps or literally anything."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_20_40.mp4",
    "text": " they've dreamed of. If code has always felt scary, this is exactly what you need to finally break that fear and start building. Why do you need this tutorial? Because this will help you build your dream startup without needing to write code manually. Gursar plus the right prompts is a game changer and you will learn this shortly. What skills will you gain?"
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_40_60.mp4",
    "text": " Let me be honest the one skill I can guarantee after completing this tutorial is you'll be able to build anything Using cursor and I mean it anything just mark my words and give this tutorial your full attention one simple request from me to you Only the top 1% of people will complete this full tutorial So make sure you're in that 1% if you're still watching this video right now you've all"
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_60_80.mp4",
    "text": " already pass most of the people who quit too early, stay till the end and you will walk away knowing how to build your own apps without writing a single line of code. Alright, enough of the talking. Let's jump in and start building something awesome together. Okay. So the first thing you need to do is download cursor, go to Google and search download cursor. This is the official website you want to visit. You can download cursor."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_80_100.mp4",
    "text": " for Mac by clicking the download button. If you are using Windows or Linux, you can check the other platform options. Make sure you are downloading the latest version, which is 0.48 because that's what I'll be using in this tutorial. Once downloaded, just open cursor. When you open cursor, you'll see the main interface, the cursor dashboard, from here, we'll create our first project."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_100_120.mp4",
    "text": " Click on Open Project and then create a new folder. Let's name it something like cursor tutorial 101. Open this folder inside cursor and there you go. Your first project is now set up. On the left hand panel you'll see the file explorer. Here you can add files, folders and refresh the workspace. There's also a search part to find files or get integration."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_120_140.mp4",
    "text": " tab which we won't be covering today and the extension section. Cursor is based on VS code which was originally developed by Microsoft as a coding ID. Now on the right hand side you'll see the chat window. This is where the magic happens. There are three interaction modes. One, agentic mode for guided workflows and full builds. Two, ask mode to ask questions about..."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_140_160.mp4",
    "text": " your codebase, three manual edit where you highlight code and ask for changes. You'll also find passchats and notepads where you can plan features, leave notes or outline new ideas. To get started, we'll build a basic to do app using HTML, CSS and tailwind. For non-corders, HTML is a markup language that structures your web page."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_160_180.mp4",
    "text": " and tailwind CSS is for styling. Now I'll ask cursor created to do app using HTML and tailwind CSS. Let's build it step by step. Make sure you select Argentic mode. I'm using Gemini 2.5 Pro with thinking mode enabled. But you can also use claw 3.7 on it. Even the free plan on cursor. Let's see."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_180_200.mp4",
    "text": " you build projects, but it comes with a two week trial and a 2000 completion limit. The pro plan is $20 per month and gives you unlimited completions plus faster responses. Now cursor is generating the app. It's planning its next move and starts by building index.html. Also, cursor also creates app.js, but don't stress about the code."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_200_220.mp4",
    "text": " Just follow along and we'll edit it together. Except all the changes cursor suggests. Now right click the file and select open with live server. You should see your basic to do app. Try it out. Add a new task. Delete a task. All of this was created from a single prompt. Awesome, right? Let's say you want to change the style of a button. Highlight the button."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_220_240.mp4",
    "text": " code and send it to the chat. For example, make the button yellow with a stylish hover effect. Cursor understands the request, shows you the current code, then updates it with the new styles, accept the changes, save the file and refresh the page to see the updated button design. Boom! Yellow button with hover styling. Now let's take it up a notch."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_240_260.mp4",
    "text": " will copy a UI design from Figma and build the exact same layout, go to Figma and search to do app UI design web template, open any nice looking template, pick the perfect component you like, let's say a task card, right click and copy as PNG, then go back to cursor and paste it directly into the chat window."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_260_280.mp4",
    "text": " cursor now has the image context. Now we will use the prompt. Use this design to create my to do app layout step-by-step. Cursor breaks it down, plans the design structure, creates a dark theme, adds components styling, and modifies both index.html and app.as to reflect the new design, except all changes, save, refresh the browser, your app now mirrors the face."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_280_300.mp4",
    "text": " my design with a fresh stylish UI. If you don't want to manually accept changes each time, you can enable auto run mode from cursor settings. But I recommend turning it on only when you're confident about the prompts. For beginners, keep it manual so you have control. That was a complete walkthrough from downloading cursor to building a full app to editing code and copying UI from a design tool."
  },
  {
    "clip_path": "ui/clips/ylZSxDGJR9c_300_316.mp4",
    "text": " If you want to learn more advanced stuff like building full stack apps, integrating AI or working with MCP tools, make sure to subscribe to the channel, join the school community, and drop a comment if you want more tutorials like this one. With this knowledge, you can start building real apps using cursor, even if you have never written a line."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_0_20.mp4",
    "text": " Hello world, it's Siraj and let's learn about a popular new deep learning framework called PyTorch. The name is inspired by the popular Torch deep learning framework which was written in the Lua programming language. Learning Lua is a big barrier to entry if you're just starting to learn deep learning and it doesn't offer the modularity necessary to"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_20_40.mp4",
    "text": " interface with other libraries like a more accessible language would. So a couple of AI researchers who were inspired by Torch's programming style decided to implement it in Python, calling it PyTorch. They also added a few other really cool features to the mix, and we'll talk about the two main ones. The first key feature of PyTorch is in PyTorch."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_40_60.mp4",
    "text": " imperative programming. An imperative program performs computation as you typed it. Most Python code is imperative. In this numpy example, we write four lines of code to ultimately compute the value for d. When the program execute c equals b times a, it runs the actual computation then and there, just like you told it to. In contrast,"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_60_80.mp4",
    "text": " in a symbolic program, there is a clear separation between defining the computation graph and compiling it. If we were to rewrite the same code symbolically, then when c equals b times a is executed, no computation occurs at that line. Instead, these operations generate a computation or symbolic graph. And then we can convert the graph into a function."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_80_100.mp4",
    "text": " function that can be called via the compile step. So computation happens as the last step in the code. Both styles have their trade-offs. Symbolic programs are more efficient since you can safely reuse the memory of your values for in-place computation. TensorFlow is made to use symbolic programming. Imperative programs are more flexible since..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_100_120.mp4",
    "text": " Python is most suited for them. So you can use native Python features like printing out values in the middle of computation and injecting loops into the computation flow itself. The second key feature of PyTorch is dynamic computation graphing as opposed to static computation graphing. In other words, PyTorch is defined by run."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_120_140.mp4",
    "text": " So at runtime, the system generates the graph structure. TensorFlow is defined and run, where we define conditions and iterations in the graph structure. It's like writing the whole program before running it, so the degree of freedom is limited. So in TensorFlow, we define the computation graph once, then we can execute that same graph many times. The great thing about this..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_140_160.mp4",
    "text": " is that we can optimize the graph at the start. Let's say in our model we want to use some kind of strategy for distributing the graph across multiple machines. This kind of computationally expensive optimization can be reduced by reusing the same graph. Static graphs work well for neural networks that are fixed size like feed-forward networks"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_160_180.mp4",
    "text": " or convolutional networks, but for a lot of use cases, it would be useful if the graph structure could change, depending on the input data, like when using recurrent neural networks. In this snippet, we're using TensorFlow to unroll a recurrent network unit over word vectors. To do this, we'll need to use a special TensorFlow function called while loop. We have to use"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_180_200.mp4",
    "text": " special nodes to represent primitives like loops and conditionals because any control flow statements will run only once when the graph is built. But a cleaner way to do this is to use dynamic graphs instead, where the computation graph is built and rebuilt as necessary at runtime. The code is more straightforward since we can use standard 4 and"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_200_220.mp4",
    "text": " if statements. Any time the amount of work that needs to be done is variable, dynamic graphs are useful. Using dynamic graphs makes debugging really easy, since a specific line in our written code is what fails, as opposed to something deep under session.run. Let's build a simple two-layer neural network in PyTorch to get a feel for the syntax. We start by importing our framework."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_220_240.mp4",
    "text": " as well as the autograd package, which will let our network automatically implement back propagation, then we'll define our batch size, input dimension, hidden dimension, and output dimension. We'll then use those values to help define tensors to hold inputs and outputs, wrapping them in variables. We'll set requires gradients to false since we don't need to..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_240_260.mp4",
    "text": " to compute gradients with respect to these variables during backpropagation. The next set of variables will define our our weights. We'll initialize them as variables as well, storing random tensors with the float data type. Since we do want to compute gradients with respect to these variables, we'll set the flag to true. We'll define a learning rate, then we can begin our"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_260_280.mp4",
    "text": " training loop for 500 iterations. During the forward pass, we can compute the predicted label using operations on our variables. MM stands for Matrix Multiply and Clamp clamps all the elements in the input range into a range between min and max. Once we've matrix multiplied for both sets of weights to compute our prediction, we can count."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_280_300.mp4",
    "text": " calculate the difference between them and square it, the sum of all the squared errors, a popular loss function. Before we perform back propagation, we need to manually zero the gradients for both sets of weights, since the gradient buffers have to be manually reset before fresh gradients are calculated. Then we can run back propagation by simply calling the backward function on our..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_300_320.mp4",
    "text": " It will compute the gradient of our loss with respect to all variables we set requires gradient to true for. Then we can update our weights using gradient descent. And our outputs look great. Pretty dope. To sum up, PyTorch offers two really useful features, dynamic computation graphs and imperative programming. Dynamic computation graphs are built and re-"
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_320_340.mp4",
    "text": " built as necessary at runtime and imperative programs perform computation as you run them. There is no distinction between defining the computation graph and compiling. Right now TensorFlow has the best documentation on the web for a machine learning library, so it's still the best way for beginners to start learning. It's best suited for..."
  },
  {
    "clip_path": "ui/clips/nbJ-2G2GXL0_340_358.mp4",
    "text": " reduction use since it was built with distributed computing in mind. But for researchers, it seems like Pi Torch has a clear advantage here. A lot of cool new ideas will benefit and rely on the use of dynamic graphs. Please subscribe for more programming videos and for now, I've got to torch my hair."
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_0_20.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_20_40.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_40_60.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_60_80.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_80_100.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_100_120.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_120_140.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_140_160.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_160_180.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_180_200.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/uHMG2XngNYQ_200_220.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_0_20.mp4",
    "text": " Streamlit is a modern web framework for data science applications and the company found it around the framework psycho system. The development flow in Streamlit entails that every time you save some changes, Streamlit will be able to rerun your updated application which enables a quick feedback loop during development. The data flow in Streamlit apps ensures that the entire python..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_20_40.mp4",
    "text": " codebase will be run when changes are made to either the code or the state of the widgets in the UI. Srimlet also offers ways to modify these behaviors using callbacks, such as on-change and on-click. Behind the scenes, Srimlet makes use of caching to speed up the entire process. The Srimlet right method or SD right for short is one-"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_40_60.mp4",
    "text": " One of the most powerful streamlit features is for displaying and styling data. S.T.R.E.R.E.T can take different types of data as input and figure out the right way to display them. The supported data types include text strings, data frames, figures, charts, objects, models, and many more. Magic and magic commands enable developers to display almost anything."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_60_80.mp4",
    "text": " without having to even use an explicit command like SDRIGHT. Just type the variable name on a separate line and let's streamlets do the rest. Widgets help you input data and interact with your application. They include buttons, sliders, select boxes, text inputs, check boxes, and all the other widgets you'd expect to use in a web or mobile application. Creating widgets is as easy as..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_80_100.mp4",
    "text": " as just instantiating a python variable. This streamlit variable will hold your inputs, which can be used in other operations stored or displayed. The layout in streamlit applications can be easily customized using layout methods such as STSidebar to create a left panel sidebar for your controls, ST columns to place widget side by side."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_100_120.mp4",
    "text": " As the expanders to hide large content and SD progress that can be displayed during wrong running paths. Teams help you change the color scheme and the fonts within your applications. Streamlit comes with a default light theme and a dark theme, as well as the functionality to create and save your own custom teams. Caching allows your app to run faster when performing long running time..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_120_140.mp4",
    "text": " Caching is performed by wrapping functions with the add SC cache decorator which checks all the code and the input variables passed to that function in order to decide to run that function again or to skip execution. Pages for multi-page apps in streamlit are created by adding individual python scripts within a page directory. Next to the main page."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_140_160.mp4",
    "text": " of your application called mainpage.py. The Model in Streamlit relies on several pillars. Streamlit apps that are essentially Python scripts that run from top to bottom, scripts that are re-executed on each page load, the fact that Streamlit will draw its output live as the script is run, caching which is used for efficient and faster applications, scripts that are re-exec-"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_160_180.mp4",
    "text": " on each widget interaction and multi-page applications that are built from individual Python scripts stored in a pages folder. Components are third-party modules that extend what's possible with Streamlit. You can use the component gallery to find components, install them as regular Python packages or you can create and publish your own packages. To install Streamlit it's recommended you"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_180_200.mp4",
    "text": " set up a local Python environment on your computer using konda, pip, pip, and vm or virtual m. Or you might as well consider an a konda. First you have to make sure that you installed a Python version, either 3.7 or above, then you can install streamlit using pip install streamlit. And you can test your installation by simply running streamlit hello. After which a hello app"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_200_220.mp4",
    "text": " should appear in a new tab in your browser. To run Streamlit for a file, name say MyFile.py. Simply use the command Streamlit, run MyFile.py. To create a Streamlit app, we start a new Python file. We import Streamlit and the necessary packages at the top. We can then set the app title using SD title. When we need some data to work with, we'll use"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_220_240.mp4",
    "text": " the open source uber raw dataset. We create our load underscore data method and then we'll use the load data function to load 10,000 rows of data in a pandas data frame. We can also add caching to our load data function using the decorator at st.cache underscore data to improve the performance. We can also use the"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_240_260.mp4",
    "text": " to draw a histogram using our data. We can use STMAP to plot our data on a map. To filter the data using a slider, we can simply use ST slider. And finally, you can also use a button to toggle our data. We showed previously how we can create a single page app using Streamlit. But oftentimes, a multi-page app is needed. To build a multi-page app, we start with our entry point..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_260_280.mp4",
    "text": " file page, say, home.py. We then create a folder named pages where we add the rest of our pages. The command streamlet run home.py. We run our entire multi-page app and this is how we split our existing Uber dataset single page app into a multi-page app with a home page, a plotting demo page, a mapping page and a data frame page. So..."
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_280_300.mp4",
    "text": " We show the case to what streamlet is, what it can be used for and how it can be used. If you like this video about streamlet and you would want me to create a series of more advanced videos on streamlet, please like this video and subscribe to the channel. You should also click the notification bell if you want to be kept up to date with similar content. If there are any particular topics you'd want me to discuss in the future related to"
  },
  {
    "clip_path": "ui/clips/o6ez9gvxLOk_300_303.mp4",
    "text": " data science or to stream the team particular, leave me a comment down below."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_0_20.mp4",
    "text": " You've heard the term project management probably many times. What is project management? It's applying science and art to projects. But what is a project? A project is a temporary endeavor. It has a start and an end. And the main goal of this endeavor is to deliver a..."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_20_40.mp4",
    "text": " product, a service, or some other result. For example, a lab report, an experiment, a building, software, a service, capability to perform a service, a project could be all of these things, getting people together to deliver this end results."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_40_60.mp4",
    "text": " And goal could be a very daunting task. The PMI talk about the talent triangle. You should be aware of technical project management. The not-some-bolts of project management, you should also be aware of strategic objectives of the firm, you should be business savvy, and you should also have great leadership skills. Did you know?"
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_60_80.mp4",
    "text": " that the top 2% of project managers as voted by their colleagues and peers are those that have superior relationship skills and great communication skills, emotional intelligence. So being a project manager involves many different things. But let's very rapidly comb through the landscape of project management. Project management."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_80_100.mp4",
    "text": " is broken out into five groups of processes. The first stage here, which is a group of processes, is known as initiating. This is where the project manager works with someone called a project sponsor or initiator to develop a charter. This charter is a"
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_100_120.mp4",
    "text": " very important document. It authorizes the project, puts the project on the map, and gives the project manager the authority to apply resources to the project. After this, the next stage that follows not in absolute sequential order, but thinking about how things flow after the project is authorizes the project."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_120_140.mp4",
    "text": " you want to begin planning in detail. This is known as the project management plan. There are many subsidiary plans that lead up to the final project management plan, but that plan needs to be authorized by the project sponsor, by senior management, by the customer."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_140_160.mp4",
    "text": " in some cases and we refer to these as stakeholders. A stakeholder is anyone who can be affected by the project and anyone who can affect the project. The next stage is to begin executing that plan. We call this the Executing Process Group and this is where deliverables are created."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_160_180.mp4",
    "text": " The next stage is monitoring and controlling the project. This is where the project manager could play detective, finding out what went wrong and if a change has occurred without proper authority and what to do to bring the project back in alignment with the plan. The final stage here."
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_180_200.mp4",
    "text": " is known as the Closing Process Group and this is where final reports and closure is followed through on. This is where the deliverable is transition to the customer. In summary, in initiating the major goal is to follow through with whatever has been done prior to"
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_200_220.mp4",
    "text": " authorization such as the business case being developed, we follow through and authorize the project based on the business case based on a benefits management plan. In planning, we follow through on planning the project after authorization, we create a project"
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_220_240.mp4",
    "text": " management plan. In executing, we get the work done, but we don't forget about the people. The people performing the work are so important. In monitoring and controlling, we check and act as needed to get every project aspect back on track if it is out of alignment. And in closing,"
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_240_260.mp4",
    "text": " close out the project or a phase in the project and ultimately hand over that final product service or result to the customer so they can begin realizing benefits. And that's the summary of project management. Be aware that everything I have presented to you is from a much larger book. This is one of"
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_260_280.mp4",
    "text": " our books, project management essentials. If you are looking at learning more about project management, go to our website, sign up for training. You could become a certified associate in project management, known as a CAPM. All you need is a high school diploma and 23 hours of formal project management education. So what are you waiting for? Let's"
  },
  {
    "clip_path": "ui/clips/cbzQyp-4_DU_280_299.mp4",
    "text": " Let's get started. See you in the course."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_0_20.mp4",
    "text": " In this video, I'm going to talk about the system design interview given at Fang companies and how to approach it to land a $300,000 a year job. The system design interview opens with the interviewer asking you very simply to design a very complex application. For example, they might ask designing URL shortener service like Bitly, designed an online bookstore like Amazon, designed a chat application like WhatsApp or Slack,"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_20_40.mp4",
    "text": " or design a ride-sharing service like Uber or Lyft. The first step in your approach is to define the requirements. A successful candidate will spend 10 to 12 minutes on this section. For example, functional requirements specify what a system should be able to do. They outline the essential capabilities and operations of the system from the user's perspective. If the question is something like design Uber, that's when the interviewer expects you to..."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_40_60.mp4",
    "text": " to ask probing questions and narrow down the scope. Start by asking questions like what parts of Uber should we focus on. Will authentication be in or out of scope? Asking these types of questions will allow us to write down clear requirements for the problem. We might land on requirements like users should be able to book a ride anywhere in the US. Drivers should be able to accept or decline rides based on their own preferences. Drivers should be..."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_60_80.mp4",
    "text": " be matched with riders based on several factors such as rating and proximity, riders should be able to view cancel and request support for their paths in future rides. Nailing these down early in the interview will allow you to focus on the parts of the design that really matter and avoid going down the rabbit hole on the things that don't. Next we have the non-functional requirements. Non-functional requirements outline how a system performs the required functions."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_80_100.mp4",
    "text": " They include constraints and conditions such as availability, consistency, latency, throughput, and others. We might propose something along these lines and see if the interviewer is in alignment. The system should be highly available, the system should prioritize consistency to ensure that only one driver is assigned to the ride, and other state-full operations work is expected. The system should have an ability to process"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_100_120.mp4",
    "text": " high throughput of data quickly to maintain real-time tracking of the drivers. After we've defined our functional and non-functional requirements, it's time for back of the envelope calculations. Let's think about the requests per second our app should be able to support. For example, for a URL shortener design, we can use powers of 10 to keep mass simple. For example, if you have 50,000 daily active users, we can look at 5 times 10..."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_120_140.mp4",
    "text": " 10 to the fourth and each makes around 10 requests a day, just multiply to get five times 10 to the fifth request per day. Then, since there are roughly 10 to the fifth seconds in a day, five times 10 to the fifth request divided by 10 to the fifth seconds equals around five requests per second. There's no need for more here. After the back of the envelope calculations, it's time for the API design. A successful..."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_140_160.mp4",
    "text": " candidate will spend five to ten minutes on this section. Here's the deal on API design. Most people use rest for their APIs and focus on basic crud endpoints to show how their app works. However, in Assistant Design interview, the API isn't always the main focus. And some senior engineers might even skip this part altogether. Sometimes choosing a different protocol other than rest can be very beneficial. To really stand out."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_160_180.mp4",
    "text": " consider using options like GRPC, GraphQL, or a BFF back in for front end layer that connects the clients to multiple microservices. After we design or don't design our API, we get into the most important section which is entity and high level design. A successful candidate will spend 25 minutes on this section. We start by defining our key entities. Identify"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_180_200.mp4",
    "text": " core entities that represent the essential data structures within your system. For Uber, key choices would be user, rider, driver, trips, payment history, and others. Describe the attributes and relationships of these entities, ensuring that they align with the system's functionality. Don't just drop some table names and call it a day. Describe how the entities relate to one another, using diagrams to show connections. This could be..."
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_200_220.mp4",
    "text": " include relationships like one to many or many to many. If your data is not relational and you are using Cassandra or DynamoDB, you must model your data according to your most frequent access patterns. Depending on the entities that you have chosen, you can make an informed decision about the database. Instead of choosing between families of databases like SQL or no SQL, the modern approaches to just review"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_220_240.mp4",
    "text": " three actual databases and learn the trade-offs between them. For example, we can look at MongoDB, Cassandra, and CockroachDB. MongoDB is a powerful document oriented database. CassandraDB is all about massive scalability and fault tolerance. And CockroachDB is designed for global distribution and strong consistency. After we choose our database, we can talk about how"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_240_260.mp4",
    "text": " level design. Begin by sketching the overall architecture of the system, include all major components such as databases, servers, external systems, and any third-party integrations. Discuss the flow of data and interactions between these components to ensure they align with the system's functional and non-functional requirements. Then we can talk about the component breakdown. Identify and describe the roles of each major"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_260_280.mp4",
    "text": " component, for example, front end services, back end services, load balancers, and data storage systems. You also want to explain how components communicate, emphasizing the technologies and protocols that facilitate the communication. Finally, we can talk about scalability and redundancy. Highlight your design choices that address scalability, such as microservices, distributed systems, or caching layers. Also"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_280_300.mp4",
    "text": " consider redundancy strategies for ensuring system reliability and availability, such as data replication or failover strategies. Finally, we can really flex in the deep dive section. A successful candidate will spend around 10 minutes here. In the deep dive phase of a system design interview, you'll focus on a single component of your system and break it down in detail. This is your chance to really"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_300_320.mp4",
    "text": " show off your knowledge. One common pitfall here is fitting your diagram so tightly that it's tough to add new components. So make sure you have left some room for growth. It's a great opportunity to prove you understand both the overall design and the finer technical details. The best components to deep dive often include caching, workflow engines or message cues and for some problems may even include geospatial"
  },
  {
    "clip_path": "ui/clips/XUIjv8lprsk_320_331.mp4",
    "text": " data or analytics. If you can do all of that in 45 minutes, you'll get the job. God bless. Subscribe for more scheduled a session at EasyClimbe.tech with a mentor who can help you master system design. Thank you for..."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_0_20.mp4",
    "text": " Hi everyone, welcome to my channel. My name is Salina. I am a software engineer and I make programming related videos and in this video I want to talk about data structures. This is going to be an introduction video so it's not really important if you are not already familiar with data structures if you never even heard of data structures because I am"
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_20_40.mp4",
    "text": " going to explain everything step by step. And then in my following videos of this playlist, we will see and focus on each individual data structure on its own. So the first question that I want to answer is what are data structures? Well, a data structure is just a collection of data. So it is a collection of integer"
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_40_60.mp4",
    "text": " or characters or floats, doubles, or it can be some user defined type, so classes. And if you're not familiar with classes, make sure to watch my object-oriented programming course, which I will link here, and then in the description down below as well. And it is a collection, but it is an organized..."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_60_80.mp4",
    "text": " collection and it is organized in such way that it makes the process of working with that data accessing that data modifying it changing it it makes that process very fast and easy. Why do we need data structures? We need them because without data structures are cooled would be much slower and then the process"
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_80_100.mp4",
    "text": " of working with our data would be much, much harder. So we organize that data into data structures. And for this, I want to give you an example that I am pretty sure every single one of you is going to understand. So imagine the following situation. You wake up in the morning and you are running late for work or for school. So you..."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_100_120.mp4",
    "text": " need to get ready and you need to get dressed but you need to do that very very quickly. So you look outside to check the weather or you check it on your smartphone and you realize that it is sunny outside. So you decide that you will wear a t-shirt, pants and then socks and shoes and you will probably wear some..."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_120_140.mp4",
    "text": " accessories, things like a wallet, a watch, sunglasses, things like that. But then you realize that your clothes is not organized. So here is your clothes. Now my question for you is, how are you going to find those things that you need in this pile of clothes? And how long will it take?"
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_140_160.mp4",
    "text": " So this is a rhetorical question and rather than answering this question I want to give you an alternative instead. So imagine if your clothes were well structured and organized, wouldn't it be much easier to access the clothes that you need in this situation? So again, as we said, you need great t-shirt"
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_160_180.mp4",
    "text": " Blue jeans, great shoes, socks, and sunglasses, watch, and wallet. And a jacket in case the weather gets worse. And you are ready literally in five minutes. And yes, you first need to invest some time to think about the best way to organize your clothes. And maybe you need to learn a few tips and tricks on how to"
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_180_200.mp4",
    "text": " do that, but later in the process of you getting ready and accessing that close is much faster. And this example is not just applicable to organizing your clothes because you probably have some other way that you use to organize your books for a school and then there are ways to organize and structure streets in your town."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_200_220.mp4",
    "text": " There is some other way to organize your post, your messages, and photos on Instagram and Facebook and Twitter and things like that. And by the way, you can follow me on Instagram and Twitter at True Code Beauty. Salina, please put it here when editing. So as I said, you have different ways to organize different things."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_220_240.mp4",
    "text": " in your real life. And the same way in programming, you would use a different data structure to organize data in different situations. And some of the examples of those data structures that we will see in my following videos are an array, a list, a stack, a queue, a graph, a hash table, and so on."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_240_260.mp4",
    "text": " And in my following videos, I will dedicate that specific video to each individual data structure so that you can learn advantages and disadvantages of that data structure and then you will learn some rules and tips and when to use a specific data structure. So if you enjoyed this video, please give it a thumbs up for the YouTube algorithm."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_260_280.mp4",
    "text": " By the way, we will be talking about algorithms on this channel as well because those are closely related data structures. So if you don't want to miss any video that I publish, you can subscribe to my channel and press the bell icon because then you will be notified when I publish my next video. If you don't, no problem."
  },
  {
    "clip_path": "ui/clips/_T42E9RkWVQ_280_284.mp4",
    "text": " Thank you very much for watching and I am going to see you in my next video. Bye."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_0_20.mp4",
    "text": " Hey my name is Satjit Patnayak and today we shall be talking about various machine learning algorithms under 5 to 6 minutes. So let's get started. The machine models are broadly classified into supervised and..."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_20_40.mp4",
    "text": " supervised and reinforcement learning and we shall be discussing most of the algorithms in this video. Supervised learning is basically when you deal with labeled data or a case where you know what needs to be done. For example, predicting the rainfall in Mumbai for the next six days."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_40_60.mp4",
    "text": " There are two types of supervised learning, regression and classification. Talking about regression model. Let's say you want to predict the price of a house given the size or else predicting the rainfall of a city. In a problem where we are predicting a country."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_60_80.mp4",
    "text": " in a numerical parameter that's basically a regression model. Some of the most common regression algorithms are linear regression, which is simply finding a line that fits the data. Its extensions includes multiple linear regression, that is finding a plane of best fit."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_80_100.mp4",
    "text": " and polynomial regression that is finding a curve for best fit. Next is decision tree regression. It looks something like this where each square is called a node and the more nodes you have, the more accurate your decision is."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_100_120.mp4",
    "text": " decision tree will be in general. The next algorithm is random forest regression. Random forest involves multiple decision trees using bootstrap data sets of original data and randomly selecting a subset of variables at each step."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_120_140.mp4",
    "text": " of the decision tree. The model then selects the mode of all the predictions of each decision tree and by relying on the majority wins model it reduces the risk of error from individual tree. The next thing we shall be discussing is..."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_140_160.mp4",
    "text": " neural networks. Next we talk about neural networks. It's a combination of multiple layers, including input layer, hidden layer and output layer. But the behavior is pretty similar to a human brain. Input goes through the..."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_160_180.mp4",
    "text": " the denseliers and then the error is calculated. And then the weights are updated in the process of back propagation. And during this process of forward path propagation and back propagation weights are updated multiple times. And we get and optimized out."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_180_200.mp4",
    "text": " After regression, next we will be talking about classification. What is a classification model? Here the output is discrete. Let's say you want to predict whether a transaction is fraudulent or not."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_200_220.mp4",
    "text": " In a problem, where we are predicting whether a new entity belongs to a certain class or a certain type, that's basically a classification model. Which means we are going to classify the data into predefined classes. Some of the most common..."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_220_240.mp4",
    "text": " classification algorithms are logistic regression, which is similar to linear regression but is used to find the probability of a finite number of outcomes. Typically, 2. Next, we have KnifeBase. This is the probabilist."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_240_260.mp4",
    "text": " classifier based on applying base theorem with strong independence assumptions between the features. The next one is support vector machines. It is a supervised classification technique that carries an objective to find"
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_260_280.mp4",
    "text": " a hyperplane in an dimensional space that can distinctly classify the data points. Next, we will be having this entry random forest and neural networks where the explanation is pretty similar to what we discussed in the regression part."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_280_300.mp4",
    "text": " The only difference here is the outputs are classes instead of a continuous number. Now, next, let's jump over to unsupervised learning. Unlike supervised learning, unsupervised learning tries to find patterns in the data with..."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_300_320.mp4",
    "text": " without having the labeled outcomes. A simple example is customer segmentation, where we don't have labels. And we don't know how many categories of customers do we have. And we still have to find customers who are similar in age."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_320_340.mp4",
    "text": " based on some clustering technique. Now let's talk about clustering. There are various clustering techniques such as chemins, BB scan, hierarchical clustering etc. After clustering, the next topic of"
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_340_360.mp4",
    "text": " under unsupervised learning is dimensional reduction, which is the process of reducing the features or dimensions of your feature set. A very common technique is PCA which is principal component analysis."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_360_380.mp4",
    "text": " PCA is used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller one that still contains most of the information in the large set. Similar to PCA, we also have Colonel PCA L."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_380_400.mp4",
    "text": " D.A. and Q.D. You can find more details about these dimensional reduction techniques on my channel where I have explained each of the algorithms in details. That's it for this video. I hope you enjoyed it. In case you did, please don't forget to like, share and subscribe."
  },
  {
    "clip_path": "ui/clips/sOncDHXhIec_400_420.mp4",
    "text": " the channel. Thank you."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_0_20.mp4",
    "text": " Hi, welcome to take a short. In this video we are going to see what is design patterns and what are the advantages of design patterns and there are different types of patterns available. So we can see them in short and quickly. First we can see the definition of the design patterns. So design patterns are a solution to the software design"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_20_40.mp4",
    "text": " problems you find again and again in real world application development. So what does this mean? We can take one scenario. So let's consider I am going to build a ticket booking application. So in that I am having options like bus ticket booking."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_40_60.mp4",
    "text": " try and ticket booking and flight booking and also hotel bookings. So if we need to create a method, let's say whatever the language may be, it's Java, C, Sharpa, whatever. So if we're going to create a method to have a book. So let's say book."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_60_80.mp4",
    "text": " ticket or book appointment we can see. So, how we will use this to book a ticket? So, some will write we can have a parameter like type and base on the type."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_80_100.mp4",
    "text": " we can have if condition if type equal to bus do certain operation. So, this applies to all other types as well. So, this can be flight or"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_100_120.mp4",
    "text": " train, this is one kind of development and what others will do is they will have some logic like, instead of if block they will have this switch condition. So, they will keep"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_120_140.mp4",
    "text": " write in a cases if it is a bus do some logics or if we train do some logics. So, it will go on like that. Now, solution itself looks good, but why we need design patterns. So, let us consider a scenario, I am having only one method. So, this method can be"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_140_160.mp4",
    "text": " get large in case of the logic behind every type is very big. So, it will be like 1000 lines of code inside a single method itself. So, what we can do? So, in those scenario there is one of the we can use a method to reduce this. So, we can call bus booking maybe this"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_160_180.mp4",
    "text": " one of the methods. So we can call this directly to reduce the size of this method and also reduce the logic behind this. So we can have a method like this. This is one of the approach we can reduce our code and also this one of the good coding practice as well."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_180_200.mp4",
    "text": " We have segregated it as each function but still we are having some issues here. Like in case if I need to modify any one of those bookings, let's say I will not change only bus booking other modules of priest. So in this scenario also I need."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_200_220.mp4",
    "text": " to modify this controller which will make the changes in the other methods as well. Like for example, if I changing this, I need to unit test this and test every method inside this to make sure everything is working fine. So this is overhead for developer and also for a tester. So the better approach is to be using an interface kind of segree."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_220_240.mp4",
    "text": " So, what does that mean? So, let us say we can have an interface called ticket booking. This is one of the interface we are having. So, we can have four separate interface. So, which can"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_240_260.mp4",
    "text": " extend from ticket booking and then we can have train booking which can in it from ticket booking. So we can write all the logics inside this bus booking for regarding bus and also we can write as a logic everything for train inside that train booking."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_260_280.mp4",
    "text": " interface. So, in case if we need to modify the bus booking, it will be only affecting this interface. It won't impact any other interfaces. So, by this way we can achieve a loose coupling between the components. The segregation of our code by achieving a good coding practice is termed as design pattern actually."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_280_300.mp4",
    "text": " So if you look at the definition now, so we came across several problems during a development right. So for that there are some solutions already been implemented like the interface that is one of an example. So like that there are lot of solutions as been implemented. We need to follow them to make sure we are following everything properly."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_300_320.mp4",
    "text": " So, we can say that design patterns are reusable solutions to the problems that develop a encounter in a day-to-day programming. I think you would have got cleared about the design patterns and what advantages of it? It reduces the time and our code will be reusable and it will be easy to extend. Next, we are going to see."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_320_340.mp4",
    "text": " the different types of design patterns and what are the different approaches used in each sections. So, there are three types actually one is creational, structural and behavioral. So, in creational pattern there are five patterns are available abstract, builder, factory, prototype and single end. So, we can go through this pattern."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_340_360.mp4",
    "text": " 1 by 1 in our upcoming videos. So, I will just explain in short this creation patterns are used for creating an object. So, to create an object we will usually do like OBJA equal to new OBJA, but it will consume a memory every time we call this"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_360_380.mp4",
    "text": " approach. So how we can reduce this, this is what we are going to discuss in these in these type of creation patterns. Next is the design patterns like a structural design, sorry, next we are going to see the structural patterns. So structural patterns in the sense, we have discussed already right. So this is one of the structural patterns only. So initially, we created a method, everything inside a scene."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_380_400.mp4",
    "text": " single method itself. We have structured it using the interface separation and method separation. So, like that the modification of the class and the objects is known as the structural design patterns. So, in these different patterns we are going to see them and next is the behavioral pattern. So, we have created an object."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_400_420.mp4",
    "text": " Let's say we are having 2 to 3 objects or 2 to 3 methods. So how the different methods communicate with each other. So these sandal will be discussed in the behavioral patterns. We can compare this design pattern with any real time example as well. Let's say we are going to a hospital. So that is one of the design patterns that the doctor follows. If you go to a fever, they will first give the..."
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_420_440.mp4",
    "text": " medications then they will recommend the blood test based on that only they will diagnose they won't come into a conclusion directly so they are already like predefined solutions for every problems that is what the design pattern states hope you all understood a basic about the design patterns thanks for watching"
  },
  {
    "clip_path": "ui/clips/4x6c3Hw9cys_440_443.mp4",
    "text": " To get more useful information like this, subscribe to take care sir."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_0_20.mp4",
    "text": " Hello everybody, in this video I will give you basic understanding of what is a Gile and why a Gile in software development. One wouldn't be able to appreciate a Gile methodologies unless we go back a decade or so when most of the software world used to follow the waterfall model of the"
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_20_40.mp4",
    "text": " development. In Waterfall model projects our split into distinct faces with no overlap whatsoever. The faces were requirements, design, coding, unit testing, integration testing and system testing. So one face had to get over."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_40_60.mp4",
    "text": " that is frozen to start another and typically the project duration used to take many months to years. The biggest problem in this approach was by the time the project was delivered to the customer, either the requirements would have changed or the requirements may have been wrongly interpreted."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_60_80.mp4",
    "text": " the development team. Because of the rigid boundaries in waterfall, it had the following shortcomings. Any change was expensive. As it meant, it had to start from requirements and be corrected along the other phases. Communication silos, as each person restricted themselves"
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_80_100.mp4",
    "text": " to that face. The responsibility was fragmented. No one took into-end responsibility, everyone focused only on their face. Low levels of motivation as exposure and learning was limited. The data from the stand-in group shows that only 4-"
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_100_120.mp4",
    "text": " 14% of the projects using what the fall was considered successful, that is within the budget and on time delivery. Compared to this agile methodology had a whopping 49% success rate. Over the course of the last two decades, the software industry has moved from..."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_120_140.mp4",
    "text": " various variations of waterfall, base developments and nowadays most organizations have adopted to one or the other forms of agile development. Agile is a time boxed iterative approach that builds and delivers software incrementally instead of trying to deliver"
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_140_160.mp4",
    "text": " it all at once at the end of the project. It works by breaking projects down into little bits of user functionality, prioritizing them and continuously delivering them in shorter cycles. In February 2001, about 17 software developers met to discuss"
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_160_180.mp4",
    "text": " users on arriving at a lightweight software development methodology. Although they couldn't agree on many things, they all agreed on the following 12 principles. The highest priority is to satisfy customer through early and continuous delivery. Welcome changing requirements."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_180_200.mp4",
    "text": " even if it is late in the development. Daily working software frequently, from a few weeks to months with preference to shorter time scale. Business people and developers should work together. Build projects among motivated individuals."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_200_220.mp4",
    "text": " individuals, give them the support and environment and trust them to do their job. The most efficient way to convey information to and within development team is face-to-face communication. Working software is the primary measure of progress."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_220_240.mp4",
    "text": " remote, sustainable development. Team should be able to maintain a constant pace than burnout after a release. Continuous attention to technical excellence and good design enhances agility. In simplicity, the art of maximizing the amount of work."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_240_260.mp4",
    "text": " not done is essential. The best products emerge from self-organizing teams. At regular intervals, teams should reflect on how to become more effective. These principles are core to any of the agile methodologies based on these 12 principles."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_260_280.mp4",
    "text": " The agile manifesto was made according to which one has come to value, individuals and interactions, over processes and tools working software, over comprehensive documentation, customer collaboration, over contract negotiation, responding to the"
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_280_300.mp4",
    "text": " change over following a plan. While there is value in the items on the right, the items on the left are valued more. There are various agile methodologies starting from Scrum, Extreme Programming, Feature Driven Development, Dynamic Systems Development Method."
  },
  {
    "clip_path": "ui/clips/N2hDKpgzdIE_300_306.mp4",
    "text": " etc. We will dwell into some of these in future videos."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_0_20.mp4",
    "text": " Hey everyone, it's more than 20 years of hands-on coding experience. I follow some patterns and practices to design a system when I face a problem. It could be a product design and enterprise solutions for a day-to-day usage. It could even be a startup solution for the legacy system renovation. Those are not divine street rules that I explained now, but I..."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_20_40.mp4",
    "text": " share your only mind experience and how do I do it. Each slide, one recommendation and let's begin with the system design quick decision cheat sheet. We need heavy systems to use a cache. Then most of your system operations are read, catching, reduces load on database and improves response time dramatically."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_40_60.mp4",
    "text": " combine cache and CDN low latency needs. Or ultra fast delivery, pay local catching with content delivery network. This ensures user get data from the nearest server and reduce into wait. Implement a message queue via service systems. To handle lots of rights, messages queues like very time."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_60_80.mp4",
    "text": " Thank you all Kafka. Let's view process work asynchronously and avoid slowdowns. Choose IDB maps, AC compliance. If you need strong consistency and transactions, traditionally SQL database are your best that for reliability, unstructured data, go for noisecloud, data."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_80_100.mp4",
    "text": " doesn't fit a gridget shema like logs, documents or sensor data, is best stored in a flexible noise-quiled database. Use Blob object storage. Are you handling large files like videos and images, use objects storage such as AWS, S3 or Azure Blob for scalable reliable storage of big files."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_100_120.mp4",
    "text": " combine message queue and cache, then complex pre-computation like newsfeed, then you need to pre-comput and quickly solve data, queue jobs for processing and store results in cache for instant access, implement elastic search, twice or search indexing."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_120_140.mp4",
    "text": " I volume search, search needed for searching huge datasets has you specialized engines like elastic search or data structures like tries considering the sharding. I mean having difficulty with scaling the scale databases split your database into shots to distribute data"
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_140_160.mp4",
    "text": " Vanga Rath"
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_160_180.mp4",
    "text": " quickly from any location. Choose a graph database, graph-based relationships for deeply connected data like social networks, graph databases, hello, quick traversal and querying. Implement horizontal scaling and more servers or not to your infrastructure to handle growth instead of"
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_180_200.mp4",
    "text": " just upgrading one machine. So scale horizontally, optimize with database indexes. High performance queries think about it. Indexes let your database find data much faster, especially for large datasets and frequent queries. Use batch processing and message more processing for operations on which they"
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_200_220.mp4",
    "text": " sets, process in batches using background jobs for better efficient. Rate limiters control how many requests user or clients can make helping protect against abuse. API gateways manage, load and secure traffic between your services and clients in a microservice setup. And replicas for key systems."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_220_240.mp4",
    "text": " So, failures don't take down your application. Replicate data across nodes or regions to avoid loss and enable quick recovery after failures. Websites enable 2A real-time data flow, perfect for chats, live feeds or notifications. A happy system regularly checks if nodes are alive, quickly spotting."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_240_260.mp4",
    "text": " and responding to failures. Check some spell data that data hasn't changed or been corrupted during storage or transfer. Gross protocols allow information to spread efficiently across nodes in a distributed system. Consistence has shing at balance load and keep data evenly distributed as your..."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_260_280.mp4",
    "text": " or remote service. Spatial, data structures like GeoHash or Quattres, LO efficient queries for location-based service. In distributed systems, sometimes we choose eventual consistency to achieve higher availability. Break large responses into pages, experiments, or well-meant clients and service by the way."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_280_300.mp4",
    "text": " I introduced in a video also, pagination with my backend service as well as with React. List recently used. L.I.U eviction keeps your Yorkesh refresh by renewing the list recently used items first. Auto scaling automatically adds warehouse resources during spikes."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_300_320.mp4",
    "text": " and scales down when demand the gods. Data lakes store massive amounts of raw data for analytics and compliance. Connection pooling manages database connections efficiently. I plot above compressive data for faster transfer. By the way, use token bucket and lead to bucket algorithms to prevent things from happening."
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_320_340.mp4",
    "text": " bio'ensa her\ufffd\ufffd su \u05de\u05d0hyunin i\u00e7in???? jumping bore sal milliseconds FXM bilgin \u03c0\u03c1\u03bfic ausge\u653e\u00e7\u0131 \u00f6\u011frendiysi Clinic %33"
  },
  {
    "clip_path": "ui/clips/J2KvCBawzF4_340_348.mp4",
    "text": " Those are my suggestions for the XINIC system. If you liked it, please subscribe to the channel and give comments. Thanks!"
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_0_20.mp4",
    "text": " projects are all around us. Everything that we wear, that we eat, that we touch, or use every day. Was an outcome of a successful project to create a unique product that you're enjoying now. Projects literally run the modern economy. And they are managed through project management processes. Hi, my name's Vijay and I'm the..."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_20_40.mp4",
    "text": " founder of ProjectsMind.com. And in this short video, I would explain the project management concept in simple terms and make them crystal clear to you. So, before moving to project management, let's first understand what the project is. A project is a temporary endeavor, undertaken to achieve a particular aim in a certain duration and budget."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_40_60.mp4",
    "text": " Every project has a start and finish date with something to deliver under a certain cost. The project always produces a unique output and it doesn't continue forever. It ends when the objectives are met. In short, projects are temporary, time bound and unique. Projects require continuous movement of people, processes, and tools to achieve..."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_60_80.mp4",
    "text": " its goals. Without these rhythmic efforts, project cannot reach its desired destination in a given budget and time. In the 1950s, organizations started to systematically apply project management tools, and techniques to complex engineering projects. While the project management term itself wasn't coined until the 1950s."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_80_100.mp4",
    "text": " Use a project management dates back to the ancient genesis of flood times. It's how Noah managed the construction work of the Ark to save humanity on the Earth. You would not be surprised to know that. The unclear understanding of the project management concept is the topmost root cause of conflict and confusion among the project team. Which often results in project delay."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_100_120.mp4",
    "text": " even failure of the project. So what is project management? Basically, it's an art and science of getting things done in a given timeline and budget. An art because there is no one definitive answer for how to best manage a project, and a science because there are well-developed techniques and process that can help significantly to complete a project successfully."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_120_140.mp4",
    "text": " Project management is the application of knowledge, skills, tools, and techniques to project activities to meet the project requirements. It's how the first high speed rail system was constructed in Japan in 1964 and widely known as the bullet train. It's how the world's largest offshore wind farm was constructed in the factory in London and installed in the middle of the ocean."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_140_160.mp4",
    "text": " It is how the International Space Station was created by a consortium of 15 nations and five space agencies. Its how your smartphone was designed in California and was assembled with more than 100 parts from over a dozen different countries, and it's not just about creating stuff around. Service industries like information and technology, software,"
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_160_180.mp4",
    "text": " and healthcare have some of the most sophisticated project management practices and processes. Projects are the backbone of the global economy and project management runs this economic cycle in an efficient manner. Project management requires both theory and practice. It needs mastery of theoretical concepts but also the skills to temper them with practical consideration."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_180_200.mp4",
    "text": " project management is a hybrid area that combines different disciplines, methods, and ideas. For efficient integration of these knowledge areas, project team need to monitor and control the various flows. You can think of project management as managing three flows. The flow of the people, the flow of the process and the flow of the tool."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_200_220.mp4",
    "text": " It also deals with a reverse flow of return of experience at the end of project life for improvement in future projects. Any organization that wants to manage its project successfully needs to have the right people, the right process, and the right tools. All three together at same time. So, what we do in project management on daily basis? Project management."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_220_240.mp4",
    "text": " management is about knowing exactly what we are doing, why we are doing and how you're going to achieve it. It's about ensuring that everyone involved in the project has a common understanding of the project school, before starting the actual work. It is a continuous process of identifying, preventing and solving problems till project completion. However, successful projects don't just have..."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_240_260.mp4",
    "text": " happen by themselves. They're not just a long list of tasks to be done. They are an integration of various knowledge areas like scope, cost, time, quality, risk and communication. Project management ensures a carefully planned sequence of activities, where progress is continuous and smooth, one task leads seamlessly to the next, and every possible risk"
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_260_280.mp4",
    "text": " been considered and back up with a mitigation plan. By now, you must have already realized why project management is critical for the success of any business or company. But remember, it's not that difficult to forget how the world might look without project management. Because, project management is virtually invisible but connects all critical chains of"
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_280_300.mp4",
    "text": " project. Whether you're shifting to a new office building or launching the world's largest space shuttle, every project needs a manager to ensure its success, someone with their eye on every detail, add every phase, from initiation to closure. Depending on the size, and complexity, projects are often divided into logical phases. Taken together, these"
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_300_320.mp4",
    "text": " Faces represent the path a project takes from the beginning to its end and are generally referred to as the project life cycle. If you are still wondering what is project management process then please note that the implementation of each phase is done through the project management process. Usually, the project management process has five components including initiation, planning,"
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_320_340.mp4",
    "text": " execution, monitoring, and closure. This ensures the due course of action is performed to complete the project task and achieve the goals. However, behind every successful or unsuccessful project, there's are the unsung heroes, the project team who kept everything and everyone on track to ensure the achievement of the desired result. Always remember."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_340_360.mp4",
    "text": " However, the real hero of project management isn't the project manager, it's the project team. It's collaborative's effort where every input of each team member counts. At ProjectsMind.com, we support project people in all the amazing work they do every day. Our objective is to provide access to world-class project management education for anyone, and"
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_360_380.mp4",
    "text": " anywhere in the world for free. The concepts you will learn are applicable to all projects from Fortune 500 firms to all the way to your own home. If you'd like to know more about project management, visit us at projectsmind.com. And don't forget to hit subscribe button below to see our latest videos when they come out. See you later and bye for now."
  },
  {
    "clip_path": "ui/clips/U7Cs_gDIfdA_380_382.mp4",
    "text": ""
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_0_20.mp4",
    "text": " So in this video, we're going to give you all the tools you need to get started with Docker. So Docker at its most basic level is a framework that allows us to take our applications and bundle them into reusable containers that can run on any system and that are isolated from other processes running on that system. So what does that mean? So we have an operating system like Linux that's running on our device. And on top of that operating system, we have a number of running"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_20_40.mp4",
    "text": " processes and we have a number of file stored on that devices file system. When we're using our computer, we're creating a lot of processes and each one of those processes is going to use the some shared storage in our machine. A container is an isolated box that contains whatever processes we want it to contain, along with its own segment of storage that's isolated from everything else. This means that things that are inside"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_40_60.mp4",
    "text": " Docker containers can't access things outside Docker containers and things outside of Docker containers can't access things inside of Docker containers So when we're using Docker will generally start with a Docker file So this is essentially a script that contains all the dependencies that we need the installation for our project and any Configurations that we might have for our project or its dependencies when we run Docker build Docker will x"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_60_80.mp4",
    "text": " execute everything in that script and bundle it along with our code and generate an image that contains everything we need to run our code. Once we have that image, we can deploy that to any device, including our development machine that we're working on, our production server that's hosting our project, or any cloud environment that manages these Docker containers for us. And that brings us to the first really great thing about using Docker."
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_80_100.mp4",
    "text": " And that's that these images that we create are super portable. Any person on an development team with their own development machine, with their own dependencies and operating system can load up our Docker image, run Docker run, and then have the exact same container that everyone else in the team has. We no longer have to worry about hard to solve bugs because of somebody saying, well, it works on my machine. Similarly, when we deploy this to production,"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_100_120.mp4",
    "text": " we know that we have the exact same container running in production as we have on our development machine. So this can stop a lot of the bugs that we have when deploying. Another great thing about Docker is that we conversion our Docker file along with our code base. So our Docker file contains all the dependencies and configurations that we need and assuming that we version these, for example, and get together, we can make sure that any version of our code base has"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_120_140.mp4",
    "text": " the correct versions of these dependencies and configurations as well. Docker containers are often compared to virtual machines, which run an entire operating system for every app that we deploy. Virtual machines have some similar advantages to Docker containers, but every time we deploy an app, we have to include that entire OS, and that introduces a lot of overhead in terms of system resources. Docker containers on the other..."
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_140_160.mp4",
    "text": " hand share the OS of the host machine with every running container. And this means that there's very little overhead to spinning up a new container, even though these containers are still isolated from one another. This isolation is another really important benefit of Docker, and it's especially useful in production when we want to make sure that different apps that we deploy aren't interfering with each other. So here's the general workflow that we have when we're using"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_160_180.mp4",
    "text": " first we create a Docker file inside our project. That's essentially the script that Docker will use for creating our image. Then we run a command to generate the image, that image contains the entire file system for our container and all the configuration that we need to run our project. Once we have that image, we can push that image to a container registry and a container registry is essentially a server that holds these"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_180_200.mp4",
    "text": " container images and they can be pulled later on other machines. Then we can pull our image from our container registry, run that image, and we end up with a running container that's an isolated instance of your running code for your project. So let's take a look at an example. So on my screen here, we can see that I have a very simple flask app that simply returns hello world anytime that we make an HTTP request. Now I don't want all the developers to be able to see the result."
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_200_220.mp4",
    "text": " is working on this project to have to install Python and Flask on their local computer. So I've created a Docker file that isolates all of these dependencies and the project itself into a container. So the first line of this Docker file is specifying that I want to use the Python 3.10 image from Docker Hub. Docker Hub is Docker's own container registry, and Python is the name of an image on Docker Hub that contains Python..."
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_220_240.mp4",
    "text": " on already installed. Next we can see that I'm copying all the data in my current directory, including this apt-up pi and my requirements file into the image, and then I'm running pip install to install flask in the container. Once everything's set up, I'm specifying that I want the container to run the flask run command to start the server when I start that container. Now that I have this Docker file, I can run the Docker"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_240_260.mp4",
    "text": " command and this will run everything that I've specified in my Docker file, including installing the dependencies and copying my code into the container image. Once this is done, I can run the Docker images command and this will show me the image that I've created for this project. Once that image exists, I can use the Docker run command to create a container from that image that'll run my code. We can see that my Flask app is now six-"
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_260_280.mp4",
    "text": " successfully running inside the container, and if I make an HTTP request, I can see that everything's working as expected. If I run the Docker PS command, I can see that I have this container running, and I can see that it's based on that image that I created earlier. It's even possible to use Docker to run a shell inside our container, and I'm able to use all the standard Linux commands to see everything that's going on inside of my container. Now if I wanted to deploy this..."
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_280_300.mp4",
    "text": " application to production because everything's in a Docker image, this would be super easy. All I'd have to do is push my container to a container registry and then pull that container onto my infrastructure to run it. Even in this simple example, I don't have to worry about what version of Python is installed on the machine or whether or not Flask is installed with PIP. Everything is isolated into this reproducible container that I can deploy wherever I want."
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_300_320.mp4",
    "text": " If you enjoyed that video, you can get a lot more content just like this on interviewpan.com. We published two to four videos a week, really, it's just an arbitrary number. It's whenever I can sit down and do a video because these videos take a whole day to do, and we're always online to answer any questions you may have. Join our Discord, join our newsletter, The Blueprint, where you can get more weekly data structure and algorithm..."
  },
  {
    "clip_path": "ui/clips/NPguawVjbN0_320_334.mp4",
    "text": " system design kind of topics and subscribe and like this video if you actually like this video and it helps you and also tell a friend that we exist. That's all."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_0_20.mp4",
    "text": " Hey everyone, welcome back for the second system design problem breakdown. The first one was ticket master. The reception to that was really really positive. That was encouraging. To be honest, I didn't know if I wanted to make these YouTube videos. You all really enjoyed that first one. There were some great comments. So I'm going to try to keep these rolling. I can't make any commitments, but maybe once every two weeks or so, at least in the near."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_20_40.mp4",
    "text": " term you should see a new one. So the one that we're going to do today is we're going to design Uber. So this is the hardest or the most interesting in my opinion of the proximity search problems. If you can do this one then you can probably do find my friends Yelp, etc. It has a lot of the same common pieces. And this is asked a ton at most of"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_40_60.mp4",
    "text": " the top companies, frankly, notably at Asylotic Google and Meta. So I think it's a great one for us to practice. Now a bit about me as a refresher. I spent five years at Meta. I was an interviewer and a staff engineer there. And I'm now the co-founder of Hello Interview. And so Hello Interviews is a site that helps candidates prepare for upcoming interviews, largely by a mox with senior."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_60_80.mp4",
    "text": " Faying Engineers and Managers like myself. And so I've asked this question well over 50 times. And I know exactly where candidates of all level, from mid-level up to senior director, staff and principal, do well, and where they tend to get tripped up. So we're gonna walk through this as if it's a real interview. But I may periodically interject or I will periodically."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_80_100.mp4",
    "text": " with some tips or frameworks and really sharing that perspective from a senior interviewer. Lastly, if videos aren't your thing, I've written this detailed breakdown to this problem here on HelloInterview.com so you can come over, I'll link it in the comments and read this if you prefer. We also have a handful of other breakdowns."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_100_120.mp4",
    "text": " the common problems in this list is continuing to grow. So come over here and check it out. All right, without further ado, let's get into it. Okay, first things first, let's touch on the roadmap that we're going to follow throughout this video, as well as the roadmap that I recommend that you follow for any of your system design."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_120_140.mp4",
    "text": " reviews, particularly those for which you're designing a user-facing product, like design over. So the first thing we're going to do is we're going to go over the requirements of the system. This is both the functional requirements, the features of the system, as well as the non-functional requirements, so the qualities of the system. We're then going to touch on the core entities in the API."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_140_160.mp4",
    "text": " to what you may be thinking in terms of data model. But it's really what are the objects that are exchanged and persisted in my system. And then the API will use those objects in order to expose the user-facing APIs that allow you to satisfy your functional requirements with the features of the system. Next, we'll sketch out a high-level display."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_160_180.mp4",
    "text": " design, the goal of this high level design is simply to satisfy the core functional requirements of the system. It should be simple just to satisfy those core features. And then lastly we're going to detail in some deep dives. And the goal of the deep dives is to make sure that our system satisfies all of our non-functional requirements. So this is the roadmap that we're going to follow."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_180_200.mp4",
    "text": " So the very first thing that you're going to do is you're going to write down the functional requirements of the system. The functional requirements, as I mentioned a moment ago, are the core features of the system. So these are often like users should be able to statements. Ideally, the interviewer has chosen a system."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_200_220.mp4",
    "text": " system that you've used before and you know pretty well. That would be amazing. That would make this a lot easier. In the case that they didn't, you'll need to ask a handful of questions to try to understand that system. Now, I'm going to assume that most of us know Uber pretty well and it's certainly the case that I do. So the core requirements of the system are that user"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_220_240.mp4",
    "text": " should be able to input a start location and a destination and get an estimated fair. So this is what happens at the very beginning when you open up your Uber app you end up saying where you want to go it knows where you are and it gives you a bunch of price estimates."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_240_260.mp4",
    "text": " And it says that for Uber X, this will cost you $32. For Uber XL, this will cost you XYZ. Now, I'm actually gonna mark out of scope multiple car types, oh bad typing, multiple car types. And we're just gonna do Uber X, for example. So just one estimate."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_260_280.mp4",
    "text": " And then the next thing once you get that estimate is that users should be able to request a ride to be matched with a nearby available driver in real time. So users should be able to request a ride based on an estimate. Cool. And then the last thing."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_280_300.mp4",
    "text": " is that drivers need to be able to accept that ride request and then ultimately navigate to the user's location and finally to the user's destination. So drivers should be able to accept, deny a request and navigate to pick up slash drop off."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_300_320.mp4",
    "text": " Now, there's different versions of an Uber system design question. In some versions, maybe you don't have the estimated fair part. Maybe you focus more on the actual navigation. This is typically where I lead candidates. It's what I think is the most interesting, but know that it's not all inclusive."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_320_340.mp4",
    "text": " So maybe you want to take some moments to yourself to think about some additional functional requirements maybe and how you additionally include those in the high-level design that we'll get to later. So you saw that I added one thing out of scope here. It's good to clarify some additional features that you might think of but you're clearly not going to do. It's really important that you stay focused."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_340_360.mp4",
    "text": " in a system design interview. They move really quickly. So I recommend that you focus in on at most, those three core features of a system, and everything else you put out of scope. So for example, some other things that I could think of is ratings for drivers and writers. I'm not gonna handle that. That's gonna be out of scope."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_360_380.mp4",
    "text": " Some other things is like schedule a ride in advance, showing my product thinking as an interviewer or as a candidate, but showing clearly that I can prioritize and focus on what matters. So we'll keep it there. The next thing that we do is our non-functional requirements. So the non-functional requirements are the qualities of the system."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_380_400.mp4",
    "text": " These are typically like the illities. So it's scalability, availability, integrity, these sorts of things. Now most candidates make the mistake of a rushing through their non-functional requirements and b just writing down some of those buzzwords. And I can tell you right now that's both not useful and it doesn't help you with the design later on."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_400_420.mp4",
    "text": " So, for all candidates, but particularly senior and staff candidates, the non-functional requirements are incredibly important. This is what's going to dictate your deep dives later on. So you want to make sure that you A, identify the qualities that are unique and relevant to this system, what makes this system challenging, what makes it difficult."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_420_440.mp4",
    "text": " And then B, that you're able to put them in the context of this system and see ideally quantify them if possible. So for example, what do we care about in this system? Well, we care about low latency as we do in most systems, but in the context of Uber, it's low latency."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_440_460.mp4",
    "text": " matching and we can quantify that so we can say maybe less than one minute to match or failure until that failure would just be you know we give the user a message thing that there's no available drivers in the area. Now what else is important? We can consider cap theorem here so do we prefer"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_460_480.mp4",
    "text": " or availability given that partition tolerance is a must. Now in this case we actually really care about consistency but specifically consistency of matching. So we want to make sure that any given ride is only ever matched to one driver. So ride to driver is one."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_480_500.mp4",
    "text": " to one. We want to make sure that there are multiple drivers who get the same ride. And for that reason, consistency is going to be incredibly important. Now, on the other hand, we do still care about availability. We care about availability, basically everything outside of matching. So, highly available outside the match."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_500_520.mp4",
    "text": " And this means that we're going to minimize downtime. The system is going to be consistently available reliable. We can process requests 24-7s. This is going to be an important component of our system. And then lastly, we should be able to handle high throughput. But again, putting that in the context of our system, this is in surges for people to see."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_520_540.mp4",
    "text": " hours or special events like if you know a football game or the Tivis with concert just got out you're gonna have massive searches there and so we can try to quantify what our searches might be and given that we're talking about stadiums or New Year's Eve, searches are probably on the order of hundreds of thousands."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_540_560.mp4",
    "text": " of requests within a given region. Perfect. And then additionally, we can consider some out of scope here as well. Some additional things that are always worth considering are"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_560_580.mp4",
    "text": " GDPR and user privacy, but we're not going to go into it in this design, certainly not. Resilience and handling system failures. Frankly, we may touch on that a bit, but it's not going to be the primary importance of the design. Monitoring, always a good thing to have in any design, but we're not going to have it in"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_580_600.mp4",
    "text": " no monitoring, logging, alerting, etc. and then deployment, CI-CD pipelines. Those are other things that would be great in any design, but aren't going to be the focus of our system. So, now we have a really clear understanding of the system that we're going to build. Both the core features, being able to get fair estimate."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_600_620.mp4",
    "text": " requests to write, match them, and allow drivers to accept or deny those requests, as well as the non-functional requirements, the importance of low latency matching, consistency of matching, high availability outside of the matching context, and the ability to handle high throughput. So from here, we can move on to our core entities and our API."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_620_640.mp4",
    "text": " Alright, before we move out of the core entities, there's one thing I want to call out. And that's that some viewers might have realized that we didn't do back at the envelope estimations. And you're right. So my opinion here is that in conducting so many of these, the back of the envelope estimations particularly early on in the design, like right now, are almost always..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_640_660.mp4",
    "text": " is a waste of time. So candidates estimate daily active users, bandwidth, storage, and at the end of those estimations, they look at their numbers and they go, okay, so it's a lot. And then they move on. And at most, they came to the conclusion that it's a scaled system that's going to need to be horizontally scaled, going to need to use things like maybe S3, whatever it may be."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_660_680.mp4",
    "text": " But they knew that. They knew that before they started the calculations. So I learned nothing about the candidate during that. They burned valuable time, and it's usually just a 35 minute interview, when you exclude pleasantries and time for questions at the end. So my opinion here is that candidates should forego the back of the envelope estimations immediately."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_680_700.mp4",
    "text": " after the non-functional requirements and only do them if the results of the calculations is going to directly influence the design. So here's how I suggest you approach that. When you finish your non-functional requirements say this to the interviewer. Say I know a lot of candidates do back to the envelope estimations at this point. It's my preference that I forego them"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_700_720.mp4",
    "text": " for now. And that instead, I do estimations during my high level design, if there are some calculations that I need to do for which the result will have a direct influence on my design. Is it okay with you? And 99.9% of interviewers will say, yes, that's great. You might have 1% of weirdos who say no, in which case, no worries. You'll do your estimation."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_720_740.mp4",
    "text": " then to satisfy them. But that's my strong recommendation when it comes to back to the envelope estimations. Now, monologue aside, the next thing that we do here is we jump into the core entities. And so you may be wondering to yourself, why does he keep saying core entities as opposed to data schema or data model as you're probably more accustomed to? And the reason is I found that most candidates and myself, when I'm an interviewer,"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_740_760.mp4",
    "text": " You don't know the full data schema at this point. I don't know all the fields and all of the columns. It's far too early in the design. But I can sketch out what my core entities are. What are the entities that are being persisted? This usually maps one to one with the tables or the collections that you're going to have. So oftentimes I'll document the entire schema in the full columns as I'm going in my highlight."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_760_780.mp4",
    "text": " design as you'll see me do here today. And I'll start here instead by just boliting those core entities and these core entities are going to be useful to inform my API. We'll see that here in a sec. So what are the core entities for something like Uber? Well, I'm going to have a ride object. Of course, I'm going to have a driver, I'm going to have a rider, and then this one's maybe less obvious."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_780_800.mp4",
    "text": " I'm going to have a location entity. In this location entity is going to be responsible for having the most up-to-date location of all of my drivers so that I can quickly match them based on proximity. And we'll see, of course, how that works in the high-level design. And so now we move on next to the API. And the API is goal."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_800_820.mp4",
    "text": " is really simple. People kind of overthink this or make this more complicated than it needs to be. We're going to use our core entities in order to go one by one satisfying our functional requirements. So we might have one, we might have many APIs per requirement, but we're going to look at those one by one and make sure that we have the full set of APIs."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_820_840.mp4",
    "text": " in order to meet them. So for example, the very first one that we have here is to get a fair estimate. And so what this is going to do is it's going to request the ETA and the price of a ride, and it's going to persist those in our database. And I'm going to choose to, instead of use a separate entity, just create a ride entity with an ETA."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_840_860.mp4",
    "text": " a price and that right entity is something that we can then continue to update if ultimately they decide to request the ride and match etc. If they don't we have some excess data laying around but it'll be great for analytics to know what people are requesting and ultimately not choosing. So this API is going to be opposed because we're going to be creating new data. I'm going to have the"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_860_880.mp4",
    "text": " fair estimate and it's going to return a partial ride. And I use partial here this is just TypeScript notation you can use whatever you want this is not a requirement for the interview it basically just says we're going to return part of the ride entity. Specifically the parts that I'm interested in are the ID the right ID the"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_880_900.mp4",
    "text": " in the price. And I might mention that verbally in the interview. You might even write it down. And the things that's going to be passed into the body here are just going to be the source and the destination. Basically, where am I coming from? And where do I want to go? So we're exchanging our core entities. In order to satisfy our first functional requirement. Easy."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_900_920.mp4",
    "text": " Now to move on to the next functional requirement, which was, user should be able to request a ride based on an estimate. So now given that estimate that was just returned to us, we should be able to request a ride. I'm going to use patch, since we're going to be updating that same row, this could also be put, it doesn't matter too much, I wouldn't get caught up here. But I'm going to request a ride."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_920_940.mp4",
    "text": " and that's going to take in the ride ID that was returned here. So that ride that we persisted, that we made an estimate for, that has a source in a destination already, I'm now going to request that I get matched with a driver and I actually want to go on that ride and it's going to take in a ride ID."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_940_960.mp4",
    "text": " This operation is going to be asynchronous. We talked about it taking up to a minute to match with a driver. This is just going to return a 200 or status code maybe a 400 in case of failure. And it's going to happen asynchronously. Great. Now, the other thing to consider is that the actual matching process."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_960_980.mp4",
    "text": " is going to require us knowing the location of drivers. We mentioned that. So how do we know the location of drivers? Well, we're gonna have to have some additional endpoint here. Maybe it's a location endpoint update that drivers call every end seconds with their Lat launch to let us know where they are."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_980_1000.mp4",
    "text": " So that's going to be important. And note, a lot of candidates miss this up front in the API section. That's totally fine. There's no expectation that you get every single API right out of the gate. As you're designing, you might find more and you'll come back and you'll add to this list. That's totally great. Of course, I've done this so many times. I have the four sites that you might not have in an interview."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1000_1020.mp4",
    "text": " So I'm going to add it here immediately, but just know. This is kind of a moving model that can be updated as you go. And so now we've satisfied the second function of requirement. The third one is the driver should be able to accept or deny a request, and then also navigate to pick up and drop off. So to be able to accept or deny a request, we're going to go"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1020_1040.mp4",
    "text": " go back to a ride API, this is going to be driver accepts, and they'll pass in that ride ID as well as maybe true false, or yeah, you know, true false if they accept or deny it respectively. So given this ride ID, do I accept?"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1040_1060.mp4",
    "text": " or deny matching. And again, I chose patch here because we're going to be updating that same row. Realistically, this could also be a post because it probably will need to create a new row on the status or maybe updated different status. We could come back and change that, but I think patch is going to be fine for now. And then the last thing is that drivers are going to be navigating first to the pick up location."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1060_1080.mp4",
    "text": " And then next to the destination, they need to be able to update us, you know, tell us that I've gotten to the Dest, I've gotten to the initial pickup location. It's time for me to go to the destination. So I'm going to create an endpoint for that. It's going to be called ride driver. It's going to be update. And what it's going to take is, of course, that right idea again. And then"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1080_1100.mp4",
    "text": " then you know some status and maybe this status is going to be picked up or you know dropped off. It doesn't matter, something like that. And what it's going to maybe return is the lat launch of the next place that they need to go or know if not relevant. In the case that they picked them up from a destination."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1100_1120.mp4",
    "text": " Then we'll update that in the database and we'll send them cool or from a pick application, excuse me, here's the lot launch of the destination you need to navigate to. On the other hand, if it tells that they dropped off a candidate, then we'll just give them a 200 return null here. There's nowhere further for them to go. Great, so this is a complete set of our APIs that use our core entities."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1120_1140.mp4",
    "text": " in order to satisfy our functional requirements. A couple things that I'll note quickly, beyond the fact that you can come back and amend these as I already mentioned, people may be noticing that I don't put any data types here. Now why is that? Now if you're a mid-level candidate or a junior candidate, go ahead, you can put data types. That's fine. Senior staff candidates."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1140_1160.mp4",
    "text": " please don't. I find that this is a massive waste of time. I see candidates sitting here doing things like this, typing out number and number and it's borderline insulting to the interviewer. Maybe I'm being too flippant in saying that but like it's inferred. You're a senior candidate, you're a senior plus candidate. We know that you know that a lot of two"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1160_1180.mp4",
    "text": " it is a number. We know that you know that a right idea is either a string or a number respectively. We know that accepting is a boolean. We know these basic types. So unless you're gonna introduce something unique, like I did here in the case of an enum, then maybe it's worth spelling it out. But otherwise, you're gonna be smart. They're following. They've done this plenty of times. Save yourself the time."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1180_1200.mp4",
    "text": " Again, 35 minutes. It's going to go quick. The other thing to mention is that you'll notice maybe that I don't put like user ID in any of these requests. So for example, a driver accepts, well, how do we know which driver sent this? And so we're going to get this off of the session or the JWT that's in the request header. And this is important for us to be able to see."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1200_1220.mp4",
    "text": " security reasons. I suggest you don't put something like user ID here unless you're explicit with your interviewer that you don't actually mean that you're putting this in the body, but it's in the header instead. Because if we did put it in the body, then any user could, say, request the ride on another user's behalf by just making this request."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1220_1240.mp4",
    "text": " with the other user IDs, user, or with the other users user ID in the request body. So good API security, we're going to use other JWT tokens or session tokens and they're going to be in the request header. This is something that I would speak to during the interview. Maybe I would write it as a bullet, but not necessary."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1240_1260.mp4",
    "text": " Alright, so we got our core entities in our API down. Let's keep moving, now for the fun stuff. Alright, we've later released strong foundation. We've gone over our requirements, the core entities, and the API. At this point, ideally, you're about 15 minutes into the interview. If you're a mid-level candidate, me-"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1260_1280.mp4",
    "text": " Maybe it's taking you a little longer than that, 15 to 20. If you're a senior staff candidate, ideally 15, maybe even a little bit less. And this gives you plenty of time now to go into the high level design, which we'll do now, while leaving room at the end for deep dives. So we're going to continue on this theme of each section relying on the one before it."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1280_1300.mp4",
    "text": " high-level design, the goal, as we mentioned, is to be able to satisfy our core functional requirements of the features of the system. And the way that we're going to do that is we're just going to go one by one through our APIs and use them as the input flow of data in order to design a system that satisfies each of them. And if we satisfy our APIs, then we've satisfied our functional..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1300_1320.mp4",
    "text": " requirements because of course we built our APIs based on our functional requirements. So hopefully that makes sense. Let's start then right away with our first API endpoint. The first API endpoint again is post-ride for our estimate, which takes an source and a destination and returns to the user an estimate as well as an ETA for"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1320_1340.mp4",
    "text": " a particular ride request. So what do we need in order to make that a reality? Well, the first thing that we're going to need is a client. And we're going to have a client for a driver. And so this can be an iOS app. This could be an Android app. We won't support Web. And so this driver, oh excuse me, I have to say."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1340_1360.mp4",
    "text": " driver here. I mean writer. So this writer is going to make a request and that request is first going to hit our AWS managed API gateway. And so this guy is going to be responsible for a couple of things. It's going to be responsible for"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1360_1380.mp4",
    "text": " balancing, it's going to be responsible for routing. This is the thing that it's most important. It's going to take each incoming API request and it's going to route it to the correct microservice. And they can also do some additional auxiliary things like authentication, SSL termination, what else can it do? Rate limiting, etc."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1380_1400.mp4",
    "text": " So it's nice to draw some of these things down. Don't get distracted by them. This shows well for the technical excellence axes that meta-evaluates under and most of the major companies have something to that effect. But don't let it distract you. It's not the most important thing. So important, I got a AWS API."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1400_1420.mp4",
    "text": " managed to get away here and it's going to simply be responsible for routing to each of my respective microservices. Now in the case of this request, this is going to be our get fair estimate request. We're going to hit this ride service that we'll create here and the ride service is going."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1420_1440.mp4",
    "text": " to handle fair estimates. And so the real Uber is going to have some complex ML model that's going to be responsible for getting these estimations. And in the interview, you have a couple of options. You could just black box it and say there's going to be some complex model here. This is what we're going to use. Or you can just overly simplify. I'm going to take the route of just overly simplify."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1440_1460.mp4",
    "text": " and I'm going to really communicate that to my interviewer. And I'm going to use a third-party mapping. This could be like Google Maps. And so this is just going to be an API call that we're going to make here in order to find out given current traffic, what is the current ETA to get from our source to our destination. And then we're just going to use that ETA."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1460_1480.mp4",
    "text": " in order to estimate a price, maybe a simple multiplication from there. So our writer is going to make a request to get a fair estimate, which can hit our ride service, which can use a third party mapping service in order to actually get that estimate. And then we need to persist that estimate in our primary database here. And so I'm just going to label this primary database for"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1480_1500.mp4",
    "text": " now for primary TV. And this is where I had mentioned earlier that we're going to for go on the actual data schema and the data models until we know more during the high level design. And this is the appropriate time to do that. So now we're persisting one of those right objects. And given that I'm so deep."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1500_1520.mp4",
    "text": " into the flow of data. I know exactly what needs to be persisted and what should exist on this right object. So for example, I know that it's going to have an ID. That's pretty basic. I know that it's also going to have a writer ID. It's going to have a fair that we just estimated. It's going to have an ETA. It's going to have a source. It's going to have a destination."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1520_1540.mp4",
    "text": " It's going to have a status and that status is going to start right now as fair estimated. And we'll have some more things that we add in a moment. And so this isn't complete. There's more things here. And we're going to come back and we're going to add to it each time one of our arrows lands on our database. We're going to see is there another field that should be written here."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1540_1560.mp4",
    "text": " And so then the other thing that we'll add since we added this writer ID, we know that we have a foreign key to some writer object. This is less interesting. A writer is going to have an ID and they're going to have some metadata, probably where their location is, some of their payment information, etc. It's not the most interesting thing for this design, so I'm just going to dot dot dot it, which is kind of a great..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1560_1580.mp4",
    "text": " tactic to use to ensure that you don't get distracted. So we've satisfied our first API endpoint. Just to be super explicit about the flow of data, the writer makes a request to get that fair estimate. We calculate the estimate. We persist in our DB with these fields. Super easy. And then we return back to the user, the ID, the fair, and the ETA."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1580_1600.mp4",
    "text": " Fantastic. Now, the next API endpoint that we need to satisfy is when the user got that fair estimate and on their client, they clicked on it to say, I like the estimate I want to request to ride. This is what starts the actual matching process. This is what ultimately matches them to a nearby driver based on proximity."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1600_1620.mp4",
    "text": " So to handle that, I'm going to introduce an additional microservice. And this is going to be the ride matching service. And the reason I'm using a separate service here, right, matching service. The reason that I'm using a separate service here is for handful of reasons. And the things that..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1620_1640.mp4",
    "text": " these two do are incredibly different. This one's going to be more computationally expensive, and it's also going to be an asynchronous process. And so by separating them, I allow them to scale independently. I allow them also to be maintained by separate teams, and that's going to be an effective for this current design."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1640_1660.mp4",
    "text": " So now the writer is going to make a request. So request ride. It's going to hit our matching service, which is going to be responsible of course for matching drivers and writers. And so the first thing that this thing needs to do is it needs to know the location of the drivers."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1660_1680.mp4",
    "text": " So we're going to have some location DB remember we had a location entity. So we're going to have some location DB and this is going to store the location of drivers. It's going to be just the current location of the drivers. And so we can make a request here to get..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1680_1700.mp4",
    "text": " the driver locations and this is probably going to be within some radius. So maybe we say get all the drivers within one mile, two miles, three miles of the riders, get all drivers with an end number of miles of the current riders location. The rider driver thing really trips me up. Anyway, hopefully."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1700_1720.mp4",
    "text": " you guys are still following. And so then this begs the question naturally of, well, what about how do we get locations into that database in the first place? That's where this end point came in, right? Location update. This is the API endpoint that drivers are going to call in order to make sure that this location DB is up to date."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1720_1740.mp4",
    "text": " So we'll introduce now a driver client. That driver client is gonna make periodic calls. Let's say every five seconds now, just to be easy. Every five seconds to a location service that is responsible for simply updating the location of drivers."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1740_1760.mp4",
    "text": " So this is going to be update location. So it's going to take a driver location, sort in the location DB, so that the ride matching service can query it. So the ride matching service then has the eligible drivers. It's going to need to make sure that none of those drivers are currently in a ride or maybe offline. So we can say get status."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1760_1780.mp4",
    "text": " based on driver ID here. So this guy can make a call to this service. In order to query our database here for a driver and a driver is going to have an ID. It's also going to have some metadata like its car, its license plate, their image, all that sort of stuff. But importantly it's going to have status and that status might be..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1780_1800.mp4",
    "text": " in Ride, offline, or available. Maybe there are some additional ones too, but we'll stick with those for now. So Ride Matching Service got all of the drivers within close proximity and then filtered out those that are in a status other than available. And then it starts sending out requests to them, saying, do you want to accept this?"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1800_1820.mp4",
    "text": " And the way that it's going to do that is going to be a bit abstracted in our design, but I'm going to introduce this Notification service a notification service itself is an interview question So I'm going to black box it for now and this is going to use like Apple push notifications I believe Google Android uses fire"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1820_1840.mp4",
    "text": " So am I making that up? Don't quote me on that. But regardless, we're going to use the native push notifications for each of them respectively. Our arrow now is going to be like this. We're going to send a push notification to the driver's device saying, do you want to accept this ride? And if they choose to accept, they're going to make an API."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1840_1860.mp4",
    "text": " point when they are an API call when they click on it, they're going to be calling that driver accept endpoint. So that's going to be this accept and it's going to take in a Boolean. It's going to hit our writing service and then we're going to update the status of the ride accordingly. And so one additional thing that could"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1860_1880.mp4",
    "text": " be here is of course the driver ID. We'll call it optional because it doesn't exist from the start. It'll only exist once there's a match. And then we would update our status to maybe matched or in a ride or whatever maybe but something along those lines. So that's what happens when the driver"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1880_1900.mp4",
    "text": " than accepts the ride. So then the very last thing that happens is that the rider needs to know where to go, or excuse me, the driver needs to know where to go. And so we created an endpoint called update for them, where they tell us that the status has changed, and we give them the next place to be. So the driver calls."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1900_1920.mp4",
    "text": " update, it updates their status to maybe they got to the source, they got to the destination, any of those additional statuses, and then we optionally return the lot launch of where they need to go next. So if they just got to the source, then maybe their status updates to picked up driver, and we send back the destination."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1920_1940.mp4",
    "text": " Okay, so at this point we have a fairly simple design. It should satisfy all of our functional requirements and we should be able to go back, look at it and confirm that. Yes we can request fair estimates. Yes we can request a ride based on an estimate and yes drivers can accept it and then I request and then navigate to the drop off and pick up location."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1940_1960.mp4",
    "text": " Now, this design is simple. And it doesn't go into detail in a lot of places where we will need to go into detail, such as how does this location DB work? How does the ride matching service ensure consistency such that there's no double matching of drivers to rides? These are all things that we're going to go into."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1960_1980.mp4",
    "text": " our deep dive. But for now, we should be able to communicate with our interviewer. I have a high level design that satisfies my functional requirements. Next up, I'm going to go deep in a couple of places to satisfy my non-functional requirements. All right, so our high level design is in place and we're going to jump into those deep dives now."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_1980_2000.mp4",
    "text": " Now if you're senior or staff candidate in particular these deep dives are really where you're gonna earn your keep This is where you're gonna show off that you have level appropriate knowledge So if you're a mid-level candidate then what we have on the board right now is pretty damn close So what would be a passing interview? That's probably not quite enough your interviewer is gonna probe you in in some places"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2000_2020.mp4",
    "text": " maybe as it pertains to the matching algorithm and consistency or this location DB being sped up, maybe sharding, and your primary DB scaling, etc. You should be able to answer some of these questions to the best of your ability fairly competently. And that's probably a round good enough for a mid-level hire. Now for senior, that's certainly not the case."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2020_2040.mp4",
    "text": " You would now need to go deep to satisfy your non-functional requirements. You should be able to go deep in a couple of places, you know, in two places or so. And then for staff, the expectation is that you can go even deeper. So maybe in more places, say three places, but the degree to which you go deep, the depth is increased. And it's often even..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2040_2060.mp4",
    "text": " the case with staff candidates that you teach your interview or something. So this isn't strictly a requirement, but it's usually a really good sign. If your interviewer walks away and it turns out that this staff candidate had a lot more knowledge about, say, DynamoDB and they were able to go deep in some properties of DynamoDB and how it pertains to this particular system, leveraging their own personal experiences. And that's a really fantastic sign. So we're going to"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2060_2080.mp4",
    "text": " a couple of places where you could go deep and we could show off that depth. We'll do more than is needed in frankly even a staff interview probably. But I kind of want to show the range of places where candidates could go deep here. So as I mentioned in order to inform where we go deep we come back to our non functional requirements and we can really just go one by one through these. So the first thing is we need to make sure that our system has"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2080_2100.mp4",
    "text": " as low latency matching. That's great. So that's going to matter in a couple places. But the place that I want to focus in on first is our location service, because this is kind of a really pivotal and interesting part of our system. So as it pertains to efficiency and cost for what it's worth, we need this query on the locations DB to get near proximity."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2100_2120.mp4",
    "text": " drivers to be really quick. So note that. And then the additional thing along this path is that this is going to be a lot of updates. So we need this path to be able to handle the number of transactions per second that we're getting from driver updates. And so that's actually where I'm going to start. Is I'm going to estimate that. And we talked about in the beginning how don't do math just for the sake."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2120_2140.mp4",
    "text": " of doing math, and you can do it later on in your design when it makes sense. And that's exactly what I'm going to do here. So I know that Uber has 6 million drivers. I'm going to estimate that maybe 3 million of those are active at any given time. I'm sure it's worth it. It's probably an overestimate, but we'll go with it anyway. And then we had mentioned earlier that we were going to send updates every 5 seconds."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2140_2160.mp4",
    "text": " So a driver will send their location update every five seconds. So that means that we're going to have 600,000 updates coming into the system every second. Great. So whatever database we have here as well as the service is going to need to be able to handle 600,000 transactions per second. Now we'll talk about later in the interview. Realistically, there's other ways."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2160_2180.mp4",
    "text": " to trim down on this. But let's keep that higher estimate for now. Awesome. So let's talk about this location DB. How can we store the location of all of our drivers and make it incredibly quick and efficient in order to look up drivers that are within some radius of them?"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2180_2200.mp4",
    "text": " Well, the most obvious thing that we could do is use some SQL database, say Postgres. We could have a column for Lat and Lawns respectively. And then we would just query, range query in order to determine any drivers that are within a Lat and Lawns upper and lower bound."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2200_2220.mp4",
    "text": " And I guess we would have an upper bound, a lower bound, a left and a right bound respectively. So four total bounds. Now this sucks for a handful of reasons. The first is that the indexes that we would build here on Latin Long respectively are B-trees. And B-trees are optimized for one-dimensional data. So a lot of launch is inherently two-dimensional as we just mentioned."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2220_2240.mp4",
    "text": " And so that's not going to be well optimized. He's going to be slow queries, non-performant queries. And also it's going to require a scanning a large number of rows for wider queries. If we have a region of say 20 miles or something, then now we're going to have to scan a ton of rows in order to get this data. And so that's also bad. Lastly, and why Postgres is just a terrible answer here."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2240_2260.mp4",
    "text": " is that, you know, postgres can handle maybe two to four k transactions per second. That certainly can change depending on, you know, right replicas, optimize hardware, but let's just estimate it's around there. And so if we do that, if we have two to four transactions per second, then we're way off the 600k that we need to be at."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2260_2280.mp4",
    "text": " So we can kind of consider this out of the question. It's a bad option. If a candidate proposed this, I would probe deeper, even if they were mid-level. It's not a great answer. So if I probe deeper and I pointed out the limitations of that, then naturally a candidate might realize two things. One, if I'm going to use Postgres."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2280_2300.mp4",
    "text": " then I need to be able to look up these proximity searches much more efficiently. And then two, I need to somehow reduce my 600K TPS to something closer to 4 TPS. And so to handle the first one, you would introduce a geospatial index. And so a geospatial index is"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2300_2320.mp4",
    "text": " something that makes querying spatial data more efficient. And in this case, we would use a quadri. And for Postgres, specifically, has an extension called PostGIS. And so quadraries, the way that they work, maybe zoom over here, and I have a little diagram that I can show."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2320_2340.mp4",
    "text": " So the way that quad trees work is that they take a map, you can imagine that this is a map of the world. And they recursively split it into four regions. So imagine this first split here. And then we make this a tree. So here's our root node. This region is our red guy here. This region green."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2340_2360.mp4",
    "text": " blue and orange as you can see. And there's some k-value, in our case, I've randomly chosen five. And this k-value determines whether or not we need to recursively split within any given cell. So in this case, we only have three drivers in this region of the world. And so we..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2360_2380.mp4",
    "text": " We don't need to split them further. This is less than our k-value of 5. Same 2 here and same 2 here. However, in this region you can see there's a lot of blue dots. There's a lot of drivers. So we can recursively split again into fourths as we do here and then bang add to our tree for additional branches."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2380_2400.mp4",
    "text": " And then we take a look at each of these respectively. So this one has four, we don't need to go further, three, three. This one itself had four or five, six, seven, which is greater than five. So we recursively split it again. Right? And so when you want to query, you simply walk down this tree. And when you get to any given leaf node, it should be the list of all of the drivers."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2400_2420.mp4",
    "text": " within that cell. So that's how a quad tree works. And that's what is a really popular geospatial index. And one that we can have supported here by Postgres, by PostGIS. So that's one option. There's some pros and cons there. This is going to be linked in the bottom of the YouTube video. So you guys can read this on your own time."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2420_2440.mp4",
    "text": " So that's great. We've now made the queries a bit more efficient. It's wonderful. But how do we handle these increased transactions per second? Well, sort of our only option there in this current setup is to add a queue here. And this queue would be responsible for handling all of these transactions coming in. And then batching them to"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2440_2460.mp4",
    "text": " right in batches such that we can get the 600k down to 4k and as you can see this would be some pretty large batches. So obvious downsides to this are one, we've introduced a significant amount of latency such that our location DB is now going to be mildly inaccurate because we needed to wait until all of these locations came in into a large enough batch."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2460_2480.mp4",
    "text": " for us to write them to our database. So that's not good. And then additionally, when we wrote to our database because we're using quadraries, we need to re-index this entire thing. So we have all this new data that comes in. This tree needs to shift around. This is expensive as well. It also takes up a lot of space in memory. So there's some additional downsides as to why this is an OK."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2480_2500.mp4",
    "text": " but not a great option. If a mid-level candidate or even a senior candidate landed on this, it would be okay. Mid-level candidate certainly passing. Senior, I'd probably point out some of these limitations and then hope that they could go further. For a staff candidate, I have no expectation that you really have a deep understanding of these geospatial indexes."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2500_2520.mp4",
    "text": " But again, I should be able to point out the limitations. You realize them and adjust accordingly. So this is not optimized, but this is an OK answer. Now, there's a better answer. And that better answer is that in order to handle these high TPS, we can make this an in-memory data store, specifically something like."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2520_2540.mp4",
    "text": " redis, which can handle anywhere from 100k to, if it's well optimized, a million transactions per second. Now, redis cluster. So, that's great. That handles that issue. And then from a geospatial index perspective, redis supports something called geohaching. And so geohaching is a-"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2540_2560.mp4",
    "text": " different algorithm that's used to do geospatial queries. And it's very similar to quadraries, but it doesn't require the maintenance of this additional data structure, this additional index. So with it, geohaching works, let me paste in a quick example here next to this one. It's like this. So again, we split the..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2560_2580.mp4",
    "text": " region into force. You can imagine this is a map of the world, we split the region into fours. And then we continue to just recursively split the region into fours again. Now we don't make any determination based on K, so it's not density dependent. We just continue to split until we get to a precision value that we're happy with. So in this"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2580_2600.mp4",
    "text": " case we have zero one this cell is two. If we split this cell two again then we end up with two zero two one two two and two three respectively this is layer two. Now we could then go again to a layer three and maybe take this two zero and this two zero will become two zero one or two zero zero two zero one two zero two two zero three."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2600_2620.mp4",
    "text": " get the picture and this can continue to happen recursively. And so what you end up with is a base 32 encoding of this string that looks something like this. And the longer the string is, the more precise, the more that final layer is zoomed into a smaller region."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2620_2640.mp4",
    "text": " this string is, the short of this string is the less precise, of course. And you can do things like look at just the first one character, two characters, three characters, and understand like the larger region that this exists in before you continue to zoom in. So this is how Geo hashing works, Reddist supports Geo hashing out of the box. And so the"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2640_2660.mp4",
    "text": " The benefits of GeoHashing, which are great for this situation, is that it's easy to calculate, it's relatively cheap to calculate a GeoHash, and it's easy to store, it's just a string, and it doesn't require any additional data structure. So when you're in an interview and you're having to make a decision between quad trees and GeoHashing, which might sound like some pretty obscure..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2660_2680.mp4",
    "text": " knowledge. But for what it's worth for these proximity questions, yelp, find my friends, Uber, this is kind of a place where you spend a lot of time and a lot of focus. So the question is often between quadri or geohash. And the takeaway that I want you to have is that quadri is great for when you have an uneven distribution or an uneven density."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2680_2700.mp4",
    "text": " of locations. So something like Yelp, for example, where you have a really dense New York city, but there's nobody in the Atlantic Ocean or nobody in the middle of Kansas, whatever. Then it's fantastic. But it's also required that you don't have a high frequency of updates, because we don't want to re-index that tree on every single update. It's expensive. And Geo hashing on the other hand."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2700_2720.mp4",
    "text": " is indiscriminate as it pertains to density. The world is split into evenly precise boxes. So less good when you have an uneven distribution, or an uneven distribution of densities of locations, but really good when you have a high frequency of right."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2720_2740.mp4",
    "text": " So in this case, despite the fact that our Uber drivers locations would have an uneven density, because we have such a high frequency of rights, it makes geohashing the optimal answer. And so that's what we're going to end up going with here, is redis and geohashing. And it's worth noting there are a couple other geospatial algorithms."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2740_2760.mp4",
    "text": " indexing strategies that exist. Uber actually uses something close to geohashing, but instead of it being squares, they're hexagons. I believe hexagons, maybe octagons. And this solves the problem that the distance from the middle of a square to the outside of the square is not the same as to one of its edges. So I've sort of made it more circular in that way. I think this was built in-house."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2760_2780.mp4",
    "text": " at Uber. So a couple options that exist, but these are the two main ones. The two main ones, if you know these two in an interview, you're totally set. If you can weigh the trade-offs between them just at the depth that I just described, you're in a really good spot. So now we've solved our problem. If we don't need a queue anymore, a queue here would be over-engineering. We can easily handle the number of transactions per se."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2780_2800.mp4",
    "text": " into a redis cluster using geo-hashing to then do these proximity searches. Awesome, hopefully that makes sense. Okay, and then the other thing that I want to mention before we move away from location is that your interviewer might ask 600k transactions per se"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2800_2820.mp4",
    "text": " second is really high. How can we make that smaller? Do we need to overload our system like that in the first place? And this is a great question. We don't have to. There's no reason for us to do it every five seconds. We could make some product trade offs on accuracy and say it's every 10, 15, 20 seconds, even up to a minute. But a more"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2820_2840.mp4",
    "text": " sophisticated answer would be that we could do this dynamically on the client. So the client could take in some information and be adaptive or send adaptive location updates based on things like the status is the driver currently accepting rides are they not if they're not then of course we don't need to know their location. The speed are they parked have they've been parked for"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2840_2860.mp4",
    "text": " 20 minutes if so their location isn't changing there's no reason for us to send updates the proximity to ride requests or hot areas if they're out in the boonies then maybe we don't need to send their location as frequently because we don't need this high precision so we can put this logic we can put this logic here on the driver and call this dynamic"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2860_2880.mp4",
    "text": " location updates and this is going to make sure that this TPS goes way down. I'd still stick with the setup that we have here but you know maybe we now got this 600k down to 100k or maybe even something smaller. Alright so that should cover that deep dive specifically low latency matching under one"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2880_2900.mp4",
    "text": " and it's an optimizing for our location service and our location DB. The next one we can look into then is consistency of matching. So this is where we wanna make sure that no driver is assigned more than one ride at a time and no ride is matched to more than one driver. So there's two things that we need to consider here. Let me kind of paste those in to save the typing."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2900_2920.mp4",
    "text": " So for consistency of matching, we can't send more than one request at a time for a given ride. We don't send any driver more than one request at a time. So we're going to handle these two separately. So when our ride matching service, we're going to get that list of drivers. And so we have a list of drivers, list of eligible drivers, maybe it's the top 10 drivers or so."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2920_2940.mp4",
    "text": " In the way that we can handle this first one, making sure that that ride doesn't send out to more than one driver at a time, it's really easy. This can just be logic within our ride match service. So we can simply have a while loop here that says while we still don't have an accepting driver, go to the next one, send him a request, wait our 10 seconds."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2940_2960.mp4",
    "text": " and then if we still haven't matched, go to the next one, send the request, etc. And this is because a single ride instance, ride matched instance, is only being handled, or a single ride that needs matching, excuse me, is only being handled by a single one of these ride matching service instances."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2960_2980.mp4",
    "text": " a single server here. So we can have that logic within that server. And it satisfies one pretty simply. I'll kind of paste in some pseudo code in a moment that hopefully will make that even even more clear, except for my description there wasn't as clear as it could have been. But one is the easy one. Two is the hard one. And two is the hard one because you need to remember"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_2980_3000.mp4",
    "text": " that we have multiple of these. So in each of these cases, our servers are horizontally scaled. And so we have multiple instances of this ride matching service. And what you could imagine happening is that people just got out of a concert. And the closest driver is the same"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3000_3020.mp4",
    "text": " for a lot of people. And those ride requests got matched on different servers here. And so each of these different servers said, give me who is the closest. And in each case, it returned a list that maybe looks like driver one and then driver two. And so each of these instances, each of these ride requests sent."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3020_3040.mp4",
    "text": " notification to driver 1. And in the case of a Taylor Swift concert there's 100,000 people or whatever, then driver 1 is sitting there and their phone just went they got 100 requests all at once. That's obviously bad. We don't want that to happen. So given that we have these multiple instances there's nothing that we can do on a single box because we need a coordination between them."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3040_3060.mp4",
    "text": " the n number of instances here. They need to all see a similar state that a given driver is currently occupied. Okay, so what we could do is we could update a status here. We already had a status field in our driver. And so we could have an additional status maybe that says,"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3060_3080.mp4",
    "text": " you know, request sent something like this. And so when we get to driver one in our list, we would update this status and then send them the notification such that any other instance here that wants to send to driver one, they first need to look in our database and see was that request sent. This of course would need to be."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3080_3100.mp4",
    "text": " atomic, but we would check that and then if the request was already sent we would move on maybe to driver 2 because we know the driver 1 was already occupied. So this is very similar to ticket master actually and how we handle no double booking for anybody who watched that youtube video if you haven't go check it out. But very similar"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3100_3120.mp4",
    "text": " idea here. And if you watch that YouTube video, then you see the issue with this approach as well. And that's that we only want to send these requests for a certain period of time. We only want to request to be outstanding for say 10 seconds, maybe 5 seconds. A driver has 5 seconds to accept it. And if you've ever been in a newbie, you've probably seen this on their app. They have a pop up that comes on"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3120_3140.mp4",
    "text": " their app, it has some loading bar that's loading quickly about five seconds and they have to click that button before that five seconds or else we're going to go send that red request to somebody else. So in this case, if we set this to request sent and the driver doesn't respond, they don't accept it or not, they're just currently driving, they're not touching their phone, whatever it may be. Then this status..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3140_3160.mp4",
    "text": " is stuck, it's stuck is request sent. And for an indefinite period of time now, nobody else can send a request to driver one because they're stuck in this status. So we need some way to update this status. And so one thing that many candidates think of is that they want to do something like this. They want to introduce some cronjo."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3160_3180.mp4",
    "text": " which runs every, you know, n seconds or minutes. And it looks for drivers that have a request sent, and then we would need to have a status update time. And so we basically do a query on this driver table to find anybody who's in a request sent status, and to see if it's"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3180_3200.mp4",
    "text": " been 10 seconds since that status was updated. And if it was, update this back to available. And so this works. Like this would absolutely work. But the issue with this, just like with ticket master, and maybe even worse in this case, is that you're introducing some delta N from the time that a driver should have become available to the time that the cron job run. So if the..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3200_3220.mp4",
    "text": " The Cronjob runs every minute and our Delta here is five seconds. We only want a driver to be locked for five seconds. Then it could take up to 55 seconds before we actually unlock you. When you should have been available to accept rides. This is an optimal and in a case where things are really busy, we want to make sure that we can match you to the optimal driver to ensure that you..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3220_3240.mp4",
    "text": " of the best experience possible. So this is an option. It's an option that's great for a mid-level candidate. It may even be passing for senior depending on how well you do in other areas of the design, but I would ask follow up questions. It's probably not good enough for staff. I would hope that a staff candidate identifies the limitations here, but it is one option."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3240_3260.mp4",
    "text": " A better option, just like with Ticketmaster, is to actually not do that, and instead use a distributed lock. So this could be Redis. We could use the same instance here. I'll actually even talk about in a moment how it could be in the primary database, but just I want to make this clear by having it separate for now."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3260_3280.mp4",
    "text": " And so we can have a redis instance here, which when we want a Lalka driver, all that we do is we set driver AD with some value, maybe true, and then it's going to have a TTL of five seconds. Excuse me, true."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3280_3300.mp4",
    "text": " This is just a key value pair. So we put in redis, which is going to be a distributed lock in this way. We're going to use this distributed lock to say this driver is currently locked. So we'll add him to redis, add driver ID to redis. The value doesn't matter. I put it as true. Who cares? We're going to set this as driver ID. And we're going to."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3300_3320.mp4",
    "text": " and set a TTL of five seconds. So in five seconds, that entry is going to be automatically removed from the cache. And so now what's going to happen is that anybody trying to send a request to a driver first needs to look to see if it's locked. So we'll check here. This is going to be really quick. It's basically just a hash map. They're going to look up that driver ID. They're going to see does the driver ID exist in my lock? If it does, I got to..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3320_3340.mp4",
    "text": " go to the next one. He's currently locked. But if the driver doesn't respond now, this row is automatically removed and the next person who reads this won't see driver ID here and they can make a request to the driver ID. So this is fantastic. There's a distributed lock, very similar to what we did in ticket master. It ends up working such that all of our ride matching services."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3340_3360.mp4",
    "text": " have a consistent global view of which drivers are available to send requests to. And it's wicked fast, snappy, great, etc. Now you might point out I have two reticences now, why don't I just put this stuff here as well? You totally could do that. Another thing that you might want to do, which might even..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3360_3380.mp4",
    "text": " smarter is that this could be something like DynamoDB, which I think would be a good choice here. There's not a ton of relations. High availability is important. We can scale this largely and definitely. And then we can introduce another table, which would be a driver lock. And this would just have the driver ID. And you can set in..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3380_3400.mp4",
    "text": " DynamoDB, TTLs, and rows on tables. So, could end up doing something like that. Now, you don't need a separate, why is my text so much bigger? You don't need a separate lock here, or separate a redis instance to manage. You can just use the same database, good consolidation, nobody did introduce an additional."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3400_3420.mp4",
    "text": " technology, but we'll use this as the lock. So that's how we're going to handle the consistency of matching. And then just to make sure that this is crystal clear, I want to paste in here kind of what this would look like. So within this ride matching service, you would say, well, we don't have a match. Then the driver is going to be the next driver from a list that we got from our location, DB."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3420_3440.mp4",
    "text": " We're going to lock that driver by updating the driver lock in our database. Then we're going to send a notification to that driver to say, would you like to accept that, right? Then we're going to wait our 10 seconds. That driver might accept, they might deny. It's going to come through this path update, it's going to update our primary database, and their status is going to update to in-ride."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3440_3460.mp4",
    "text": " And so at this point then, no match should be false because there is a match. They already in a ride. This is ended. Everything is good here. And then of course we'll have some way to notify the user. Otherwise, I'm not going to go into a tremendous amount of detail there. Great. So those are certain."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3460_3480.mp4",
    "text": " the two most important deep dives for this interview, but let's let's continue to make sure that we round this out and recognize that the video is getting a little long as well. The next one I want to talk about is handling high throughput surges for popular events. So this one's relatively straightforward. What we want to do is make sure that we can scale, we can scale dynamically in order to meet that demand. So if we had them,"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3480_3500.mp4",
    "text": " in instances here and then all of a sudden we had all of these people who want to ride, then maybe it's too much to be handled by these instances that we currently have. And we can't horizontally scale them dynamically quickly enough. And so naturally the solution to something like that, let's see if I can fit this elegantly into our diagram, is to introduce..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3500_3520.mp4",
    "text": " a queue. And so we can have a queue here. This is going to be our right request queue. Right request queue. Apologies that this is a bit tighter than I would like it to be. Okay. So we have our right request queue here. And when rides come in, they're going to sit on this queue."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3520_3540.mp4",
    "text": " And then our ride matching service is going to pull off of this queue in order to match drivers to writers for a given ride. So if we have a huge surge, no problem, we'll be able to handle that. I'll wait in our queue here and we'll just pull off when I'm ready. Our queue is likely going to be partitioned by regions. I'm going to show that you run into it."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3540_3560.mp4",
    "text": " this queue is that if it's simply a FIFO queue, then you might have somebody who is, the request came in first, but it's a much harder request to match. Maybe it's out in the boonies, there were no drivers around, whatever it may be. And then you could have some people who are stuck behind the queue here, not getting matched, despite the fact that they're in the middle of New York City and there's drivers everywhere."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3560_3580.mp4",
    "text": " something like this. So that's not optimal. A couple things that we could do for that is that we would partition this queue based on location or based on regions. So within New York City, you would have kind of more fine-to-grained regions, even more fine-grained probably than the burrows, but you would partition your queue on this in this way."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3580_3600.mp4",
    "text": " you don't have that problem where somebody ends up sitting behind a request that's taking a longer time to match. And then additionally, the added benefit here is that if any of these ride matching services go down and remember, these are pretty computationally expensive. Like, they're waiting these 10 seconds. They're taking a long period of time. And so any of them could go down in that minute that we're trying to find a match. And if that happens..."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3600_3620.mp4",
    "text": " No problem. We'll just pull it off the right request queue again. It won't have been acknowledged as succeeded and then we'll continue to match that user. So this works pretty nicely. So the right request queue, hopefully it's relatively straightforward. I won't go into a tremendous amount of detail, but that's how we're going to handle these high surges. High availability."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3620_3640.mp4",
    "text": " outside of matching. So this is where our system, you know, it's kind of the traditional answer. We can horizontally scale each of these. These will have their own load balancers. This was DynamoDB, so largely at scales and definitely we talked about the scaling here from Redis. So that's great. The last thing is that we can split"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3640_3660.mp4",
    "text": " these by region. So realistically we're not going to have a single instance of this full setup. Like maybe this is the northeast region that you're looking at here and then we're going to have a copy and paste of this entire thing out in California for you know the southwest region same with up in Seattle for the northwest region and these will be located in different data centers."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3660_3680.mp4",
    "text": " Now there's probably things like users who can travel, of course, you could go from New York to California, which will have an every data center, maybe just a read replica or a copy in the primary database of any given user table. But that's an additional way that we can increase availability."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3680_3700.mp4",
    "text": " the string on the system make things like the cue partitioning more streamlined and obvious by splitting by region here. All right I think that just about wraps it up. Congratulations if you got all the way through it. Apologies this video ended up being a little bit longer than I intended."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3700_3720.mp4",
    "text": " Of course, as I mentioned, I went into more detail here than would be required in the interview, so I don't want anyone to feel overwhelmed. Just to be crystal clear here, mid-level candidate, if you got that high-level design and you were able to answer questions from your interviewer about some of the deep dives you're doing great. Senior candidates, I hope that you could have gone deep and at least a couple of these"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3720_3740.mp4",
    "text": " places, the location service, and ride matching, even if you didn't land on optimal with the distributed lock with TTL, but you landed on the cron job or you suggested the queue here with PostJS and quad trees, even if you didn't know the geospatial index. Probably great, very likely a higher."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3740_3760.mp4",
    "text": " staff candidates, we do have that higher bar. Expect that you were able to show more depth in places, but maybe it wasn't the places I went deep, maybe it's places that you understand even better than I do. And that would be great as well. So don't feel overwhelmed, but I wanted this to be relatively comprehensive so that you feel well equipped when you go into your interview. All right, with that said,"
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3760_3780.mp4",
    "text": " Thanks everybody for watching. If you have questions, comments, concerns, anything I did, stupid that you want to call out, please put them in the comments. I'll be checking those out and responding frequently. This was the second of these that we've done. Again, check out ticket master. If you haven't seen that when yet, I'll try to do another one here in the coming handful of weeks."
  },
  {
    "clip_path": "ui/clips/lsKU38RKQSo_3780_3784.mp4",
    "text": " Alright, thanks everybody for watching and best of luck with your upcoming interviews. Take care."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_0_20.mp4",
    "text": " A stack is a linear data structure that stores the data based on the order they were added to the stack. Stacks have two main operations, push and pop. Push inserts an element to the end of the stack while pop removes the last item. This means the first item pushed will be the last one pop from the stack. This is a..."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_20_40.mp4",
    "text": " comparable to a stack of heavy books. For example, you can't access the bottom book without removing all the books on top of it. A queue is similar to a stack, but the first item pushed to the queue is the first one to be removed. The end queue operation adds an item to the end of the queue while the DQ operation"
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_40_60.mp4",
    "text": " removes the first item in the queue. This is similar to a queue at a store where people line up in order and the first person to reach the counter gets served first. An important detail to note is that de-cuing has constant time complexity, meaning it doesn't take longer as the size of the queue increases. An array is a linear data structure."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_60_80.mp4",
    "text": " that stores data sequentially based on the assigned index in the array. Arrays have a fixed length and the items are stored together as a contiguous block in memory. The predictable representation of arrays in memory make them ideal for quickly accessing and modifying the value located at any position in the array. Arrays have two basic operations."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_80_100.mp4",
    "text": " reading the value required in index, which is a whole number used to locate data relative to the Array's location memory. Writing is done by specifying an index and replacing old data at that location with new data. Arrays provide random access as opposed to sequential access seen in stacks and queues, eliminating the need..."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_100_120.mp4",
    "text": " to access unnecessary data before reaching the desired location. List or vectors are mostly just resizable arrays. They store data sequentially using indices that can dynamically change their size.comodate more or less data. Reading and writing data at a specific index has a constant time complexity, just like"
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_120_140.mp4",
    "text": " but lists can also push and pop items like stacks making lists of versatile data structure. Items can be inserted or deleted at any index of a list, but these operations have a linear time complexity because every item to the right of the insertion or deletion has to be moved. A linked list is a data"
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_140_160.mp4",
    "text": " structure consisting of nodes and pointers. Each node contains data and a pointer to the next node in the chain. The last node has a null pointer, unless the linked list is circular. Insertion and deletion have constant time complexity unlike array based lists, but accessing a node near the end of the linked list requires traversing all the nodes."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_160_180.mp4",
    "text": " preceding it, making it difficult to officially insert and delete nodes that are further down the list. A tree is an extension to a linked list where each node can point to multiple nodes, but multiple nodes can't point to a single node. A node is a parent if it points to another node. A node is a child if it has a parent node."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_180_200.mp4",
    "text": " The root node is the entry point of the tree because there are zero parents. Every other node in the tree has exactly one parent. Trees are heavily used in algorithms due to the often quick logarithmic search, insert and delete time from the cities that arrays can't provide. They are a common choice when data needs to be sorted and many insertions or deletions need to take place."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_200_220.mp4",
    "text": " The most common implementation of a tree is a binary tree, but the binary tree itself has many variants optimized for specific situations. Binary trees have a maximum of two children per node, where the left child has a lower value than the parent and the right child as a greater value. A graph is an extension of a tree where there's..."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_220_240.mp4",
    "text": " no restriction on the number of parent nodes or child nodes. In graph theory, nodes are usually called vertices, and pointers are called edges. Directed graphs are graphs that only contain one-way edges, if A points to B then B doesn't point to A. Directed A-cycle at graphs are directed graphs that don't contain any cycles. Dogs are quite similar to trees."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_240_260.mp4",
    "text": " due to this property and a lot of tree algorithms are compatible with tags. Now the graph has many uses because it is a digital representation of relational data. Graph there is home to many of the best algorithms such as shortest path, max flow, shortest circuit and many more. There are many ways to implement graphs using an edge"
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_260_280.mp4",
    "text": " Since he lists, matrix, ichthylist, or even the traditional point to graph, but these come with their own pros and cons. A dictionary or a map is a data structure that stores data as a collection of key value pairs where each key is unique and maps to its associated value. Usually, keys are simple data types like integers or strings that come."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_280_300.mp4",
    "text": " can't be changed. Dictionary operations often have a constant time complexity, which means inserting, deleting, reading, or even checking whether a key exists is instant, regardless of how big the dictionary is. Keyes provide much quicker access to unnoted data than a linear search or a thorough array, and can even outperform a binary search."
  },
  {
    "clip_path": "ui/clips/Ac5nXnahi3E_300_320.mp4",
    "text": " Speed of dictionary operations makes dictionaries very versatile as they are not limited to a consecutive set of integers for indexing like arrays. Dictionaries can replace arrays in situations where data is stored far apart, reducing the amount of memory wasted."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_0_20.mp4",
    "text": " In this video, you'll learn about what the stack data structure is, how to implement it using Python, and some of its common use cases. Let's begin. The stack is a linear data structure that follows the last in First Out mechanism. Let's take a"
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_20_40.mp4",
    "text": " look at this statement in more detail and understand what linear data structure and last in first out means. A linear data structure is where each element is connected to at most two other elements, the one before it and the one after it. When you try to visit all the elements starting from one end, there's."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_40_60.mp4",
    "text": " There's just one path that you can take. That is, a linear path from one end to the other. Contrast this with nonlinear data structures like graphs, where an element could be connected with any number of elements. There are several different ends of this structure, which means that there are several different paths that we can take to travel through the entire..."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_60_80.mp4",
    "text": " graph. Now, let's consider last in first out. It is exactly how it sounds. When the elements are taken out from the data structure, they are in the reverse order in which they were put in. This means that if the elements were put in the order 1, 2, 3,"
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_80_100.mp4",
    "text": " before, they'll be retrieved in the order for 321. Let's see how these concepts apply to a stack. Consider a stack of pancakes, which is empty right now. At the moment, the top points towards nothing because there are no pancakes in the stack. When you want to insert a pancake,"
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_100_120.mp4",
    "text": " into the stack, you place it on the current top and then update the top, which was previously pointing to nothing, to point to this new pancake. You can only insert a pancake to the top of the stack. If you want to insert it anywhere in the middle of the stack, you'll have to remove the pancakes from the top, then insert the new pancake and place the..."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_120_140.mp4",
    "text": " pancakes back again. In program, this action of inserting elements to a stack is known as pushing. When you want to take a pancake out of a stack, you will remove the pancake on the top, then the one under it, and so on. Removing elements from a stack is known as popping. Observe that the pancakes are removed in the reversal."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_140_160.mp4",
    "text": " order in which they were placed in. And at all times, each pancake was only next to, at most, two other pancakes. The one above it and the one below it. Now, let's see how you can implement a stacking code. We'll be using Python for the same. For implementing a stack, you need three things."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_160_180.mp4",
    "text": " an MDRA in which you will store the data, a variable that points to the top element's position or index, and the size of the stack. To initialize the stack, you will declare an MDRA of the given size, then set the size of the stack, and then set the..."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_180_200.mp4",
    "text": " top to negative 1, that indicates that the stack is currently empty. It will have the following two methods. Push for inserting an element and pop for deleting an element. The push method accepts one argument, the value that needs to be inserted."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_200_220.mp4",
    "text": " first checks if the stack is full, that is, if the top index is equal to its size negative one. If it is, then it throws an error. Else, it updates the top by one and place the new element at this new top."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_220_240.mp4",
    "text": " The pop method removes the element from the top. It first checks if the stack is already empty, that is, the top is equal to negative 1. If it is empty, then it throws an error. Otherwise, it returns the element at the top position and decreases the top index by 1."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_240_260.mp4",
    "text": " You don't need to delete the value at this point because when you insert a new value, it automatically overrides the old value at that position. You can also have another method called seek. That just reads the value from the top without popping it from the stack. Now that you know how to implement a stack, let's take a look at where a stack is useful."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_260_280.mp4",
    "text": " A stack is useful in implementing features like Undo, Redo, and Forwards backwards. Think about this. When you're browsing through any website, every new URL is pushed to a stack. Whenever you want to go back, you go back to the URL that was last pushed into the stack."
  },
  {
    "clip_path": "ui/clips/U0OV_wO_O9E_280_300.mp4",
    "text": " Similarly, when you undo changes in a, let's say, a Photoshop program, you undo the most recent changes and backwards from there. Hopefully now you understand what a stack is and how to create one and when to create one. Happy coding!"
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_0_20.mp4",
    "text": " Hey ya, everybody. It's Yvrow, hope you're doing well, and in this video we're going to discuss Priority Cues in Computer Science, so sit back, relax, and enjoy the show. Hey ya, what's going on, everybody? So Priority Cues, a Priority Cue is a FIFO data structure. First day..."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_20_40.mp4",
    "text": " first out. However, a major difference with priority cues is that before we start polling and serving elements, we put them in some sort of order. Higher priority elements are served first before elements with lower priority. So here's an example. Let's say that we have some student GPAs and we'll place them all within a cue."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_40_60.mp4",
    "text": " and then a priority queue and take a look at the differences. So let's create a queue. The data type is queue, and we are going to insert some doubles, some GPAs. I'll name this queue, equals new. Now, queues are actually interfaces, and we can't implement them directly. So we need to use a class that utilizes..."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_60_80.mp4",
    "text": " the Q interface. One that I know of is a linked list. Then finish and stamp-chating it. Now to add data to a Q, we use the offer method Q dot offer. And then pass in a GPA. Let's make sure that these are not in order. How about a"
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_80_100.mp4",
    "text": " 3.0 then the next student has a let's say 2.5 then a 4.0 1.5 and 2.0 okay now let's display the elements of our queue one easy way in which we can do this."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_100_120.mp4",
    "text": " that is to use a while loop. And this is our condition. We'll continue this while not logical operator, q.isempty method. While our q is not empty, pull each element and display it. So within a print line statement, let's see."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_120_140.mp4",
    "text": " use q dot pole to display and remove each element beginning with the first one and then working our way to the end. So this will display the elements within my queue. So these are in first come first serve order. Whatever element we were offered first, we are serving that element first. If an element..."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_140_160.mp4",
    "text": " was added last, well then it served last. So that's a standard queue. Now with a priority queue, we're going to arrange them in some sort of order before we actually pull these elements. So let's change our linked list to a priority queue. And run this again, and let's take a look at the differences. Okay, these are all..."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_160_180.mp4",
    "text": " in order now. So if we're working with numbers like doubles, these are arranged in a sending order beginning with the smallest element. Let's pretend that these are grade point averages, GPAs. Why might we want to put these in order? Let's say whatever student performed the worst gets two hours of free tutoring, whichever student performed"
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_180_200.mp4",
    "text": " second worst gets one hour of free tutoring and third gets half an hour of free tutoring. Now if you need these in desunding order, well there's one change that we're going to make. Within the constructor we can pass in a comparator, but that's a little advance for us and we haven't discussed that yet. So there is a default comparator that we can use found within collection."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_200_220.mp4",
    "text": " collections dot reverse order method. So these elements will be in reverse order then. In this example, let's say that whichever student has the best GPA, we'll receive I don't know a gold medal like the Olympics and the second best student will receive a silver medal."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_220_240.mp4",
    "text": " then it bronze metal, and then every student after that, I guess we'll receive nothing. But that's okay though, they got some free tutoring in the last example. Okay, now let's change the data type. Let's say that these are now strings, and let's put these in standard order. So let's change these two strings. Maybe the student has a B."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_240_260.mp4",
    "text": " This one has a C, a, f, and what are we missing? D. If we have a priority queue of strings, well then these elements will be in alphabetical order. So if we need these in reverse alphabetical order, again we can pass..."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_260_280.mp4",
    "text": " a comparator, and then one that we can use is from collections. Collections.reverseorder. And these elements within our priority queue are now within reverse alphabetical order. So yeah, that's a priority queue. Think about it like a queue. However, we first sort these elements."
  },
  {
    "clip_path": "ui/clips/7z_HXFZqXqc_280_300.mp4",
    "text": " based on a certain priority. We will pull and serve the elements with the highest priorities first and work our way to elements with lower priority. So yeah, that's a priority queue. If you would like a copy of this code, I'll post this to the comment section down below and well, yeah, those are priority queues in computer science."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_0_20.mp4",
    "text": " So let's begin by talking about what union find does. Imagine if you had some objects and you want to group them together according to some criteria. So once such grouping might be this. Now once you have completed the grouping, you would want to know certain properties about the groups, such as how many groups there are, and if two objects be..."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_20_40.mp4",
    "text": " belong to the same group. The union-fine data structure, as his name suggests, supports two operations. The first one is called union, which takes us input two elements x and y from the set of objects, and it joins or unions the group's containing x and y. For example, we can start with 0 and 2 belonging to the..."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_40_60.mp4",
    "text": " the same group and one and three belong to the same group. And when we call this function union zero one, it joins or unions the groups containing zero and one. So the final result would be all four objects belonging to the same group. The next operations called find and takes a single."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_60_80.mp4",
    "text": " object X as the input and returns the group that X belongs to. So now let's see how we can implement union find. We will first walk through the ideas and discuss the code at the end. Union find is useful in graph type problems. So our first step will be to cast the grouping in the form of a graph. And the way."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_80_100.mp4",
    "text": " we do that is to connect all the objects that belong to the same group with an edge like so. Now, remember we need to find a way to determine which group each element is in. To do so, we designate a representative element for each group. This element will represent the entire group belongs to in the sense that the find..."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_100_120.mp4",
    "text": " function will always return the representative. For example, when we call find a for the representative of the group that for belongs to is zero. So therefore we return zero. And of course, the representative will return itself. As another example, when we call find of two, we"
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_120_140.mp4",
    "text": " return the representative of the group 2 belongs to, which is 5. Notice that two elements belong to the same group if and only if they have the same representative. So the representative provides a way of checking if two elements belong to the same group. So now we need to implement the other function, which allows the union of 2."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_140_160.mp4",
    "text": " groups. And to do that, let me rearrange this graph in a suggestive way. I've now put the representatives at the top and the rest of the elements of the group towards the bottom. Now each group is represented as a tree with the root being the representative. With this, we can introduce the notion of parents and children."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_160_180.mp4",
    "text": " For any given element, how do we find the representative? Well, we must travel upwards the tree towards the root. In other words, we need to keep traveling towards the parent until we reach the top. As an example, suppose we want find a 4. The parent of 4 is 3, so we travel upwards the 3. The parent."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_180_200.mp4",
    "text": " 3 is 0, so we travel up to 0. And we will know when to stop if we just use the simple convention that the parent of a root is itself. So if we can keep track of the parent of every object that we can very easily travel upwards the tree towards the root. So what about the union operation?"
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_200_220.mp4",
    "text": " In this new interpretation, the representatives are the roots of the trees. So if we want a union to trees, we can just set the root of one tree to be the child of another. For example, suppose we want a union 1 and 2. 5 is the root of 2, which we can get by calling find of 2. And similarly, 0 is the root of 2."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_220_240.mp4",
    "text": " one. We then set the parent of 5 to be 0. And what this does is that it says 5, which used to be a root, now as a child of 0. And all of the elements that used to be in the tree rooted at 5 now have a root of 0. And now these two trees have been unioned. Now..."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_240_260.mp4",
    "text": " Now we have the basic idea, let's discuss how we can implement this in code. We initialize this array called parent, where parent of i gives the parent of the object i. Initially, each element is their own individual group, so we initialize the array with parent of i equals i. Next, we implement this function find of x."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_260_280.mp4",
    "text": " which finds the root of the tree that contains X. So first, if the parent of X is not itself, then we know that we have not yet reached the root of the tree. So we travel upwards the tree by one step by calling the find function recursively on a parent of X. Otherwise, we know that we have reached the root of the tree."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_280_300.mp4",
    "text": " So therefore we just return x. And finally we define the union function which joins the trees containing x and y. So as we've seen before we first need to find the root of y and the root of x. And we then set the parent of 1 to be the other. So that is the algorithm and now"
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_300_320.mp4",
    "text": " let's discuss the complexity of this algorithm. If the data structure contains N elements, then on average, the height of the trees will be on the order of log of N. Since both the find and union operations involve traversing up the trees at most two times, the time complexity of both functions is O of log of N."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_320_340.mp4",
    "text": " The space complexity will be O of N, only to the parent array, which will have length N. All right, I hope this video gave you a better understanding of the union-fine data structure in algorithm and helped you prepare for your interviews. If you liked the video, please give it a like and subscribe to our channel. That's all I have for you today. We'll see you."
  },
  {
    "clip_path": "ui/clips/ayW5B2W9hfo_340_345.mp4",
    "text": " guys next."
  },
  {
    "clip_path": "ui/clips/3x-dBvZl0uA_0_20.mp4",
    "text": " Think of an algorithm as a recipe for a computer. Just like a chef falling steps to create a delicious dish, according algorithm outlines the precise instructions computer needs to solve a problem. Whether it's sorting your music library or recommending movies."
  },
  {
    "clip_path": "ui/clips/3x-dBvZl0uA_20_40.mp4",
    "text": " Every program relies on algorithms to process information and deliver the desired art room. You may have heard of some famous algorithms like Minimax algorithm, a search algorithm that explores all possible future moves and outcomes of a game and chooses the mood that leads to the best outcome for the player. PageRack"
  },
  {
    "clip_path": "ui/clips/3x-dBvZl0uA_40_60.mp4",
    "text": " Algorithm used by Google search to rank web pages in their search engine results. RSA, a public key cryptography algorithm used for secure data transfer. Our key means clustering and unsupervised machine learning algorithm used to group data into clusters based on similarity. Now these\u2026"
  },
  {
    "clip_path": "ui/clips/3x-dBvZl0uA_60_80.mp4",
    "text": " Algorithms don't have to be complex. They often follow basic steps. Like start to do this if this happens to that. And finally, and it's like a clear set of instructions telling the computer what to do in each situation. So the next time you use a computer program, remember the invisible recipe."
  },
  {
    "clip_path": "ui/clips/3x-dBvZl0uA_80_100.mp4",
    "text": " working behind the scenes. Holding algorithms are the building blocks of any computer system. If you like this video, do like, comment and subscribe and don't forget to press that bell icon. Until then, keep up scaling."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_0_20.mp4",
    "text": " Imagine a scenario where you have a framework for creating simple tags documents. Wherever you need to work with documents, you use the methods and functions associated with text documents. And your entire codebase is tightly coupled to text documents. One day, you decide that you need to add the ability to work with the PDF format. You..."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_20_40.mp4",
    "text": " found all the places where you use the text document methods and edit conditions to check which of two file formats you are currently using and which function we need to call in which case. A few days later you decided to edit the Moby and Toc formats as well and now you see the problem. You don't want to add conditions in every place again."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_40_60.mp4",
    "text": " And from this point, you want a universal strategy that works with all existing formats, and an easy way to add new formats. This is where the factory method design pattern comes into play. Defectory method is a creation design pattern used in software development to create objects using a superclass, allowing superclasses to..."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_60_80.mp4",
    "text": " altered the type of object they create. It provides an interface for creating instances of your class, but delegates the responsibility for creating instances to its subclasses. This pattern is typically used to provide loose coupling and flexibility when creating objects. For example, let's define an interface document with methods for working with"
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_80_100.mp4",
    "text": " documents. Then let's create an interface document factory with a create document factory method. Then for each supported document let's create implementation of the document factory interface and name them for example TXT document factory and PDF document factory. Each of these classes implements the create document method."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_100_120.mp4",
    "text": " to return specific document types. Now, when you want to add a new document format, such as Moby, you simply create and implement a Moby Document Factory class. That will return Moby Document objects from the create document factory method, and that's all. Usage is also simple. At the point when you need to interact with the document of specific..."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_120_140.mp4",
    "text": " type, you instantiate the corresponding document factory and later use it to create and manage documents. The code itself will not know exactly which document it's using right now, but it doesn't need to know that, right? There are four main actors in this design pattern. The first one is the factory."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_140_160.mp4",
    "text": " Factor is an interface that declares a factory method, responsible for creating objects of a particular type. The next one is concrete factory. This is a subclass of factory that implements a factory method for instantiating concrete product classes. The next actor is the product. It is also an interface that defines methods"
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_160_180.mp4",
    "text": " for objects created by a factory method, and the last sector is concrete product, a product subclass that implemented the specific functions of the created object. Speaking of benefits, the main one is, of course, the flexibility to introduce new types of products without changing existing client code. What's also"
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_180_200.mp4",
    "text": " So good, is that this design pattern reduces the coupling between client code and concrete implementations, providing loose coupling, because clients depend on the factory interface rather than concrete classes. Last, but not least, the factory method separates object creation from the rest of the code, making it easier to manage changes to the object creation."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_200_220.mp4",
    "text": " The disadvantages are the following. Creating separate concrete factory classes for each object type can result in more classes and potentially more complex relationships between them. Additionally, if the logic for creating multiple products is similar, you may end up duplicating code across different..."
  },
  {
    "clip_path": "ui/clips/-MHnvg7xZsI_220_227.mp4",
    "text": " of a particular factory. That's all about factory method design pattern. See you in the next video."
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_0_20.mp4",
    "text": " Imagine you are writing an application for a bakery that automates the process of baking different types of bread. You realize that the basic steps for baking bread are always the same, prepare dough, bake and slice. However, the details of each step can vary depending on the type of bread. In this scenario,"
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_20_40.mp4",
    "text": " the template method comes into play. The template method is a behavioral design pattern that defines the skeleton of an algorithm in a base class, while allowing subclasses to provide specific implementations for certain steps without changing the algorithm structure. It promotes code reuse and enforces a consistent structure for closely-"
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_40_60.mp4",
    "text": " related algorithms. To implement this in your application, you create a base class called Bread Recipe that outlines the steps to bake bread. This class contains a method named MakeBread, which is the template method. This method outlines the steps for baking bread in a specific order."
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_60_80.mp4",
    "text": " First, prepare dough, then bake, and finally slice. We also have some additional methods to provide baking details for make bread method. In this case, these are, get size, get grain type, get form, get topping, do slice. However, the bread recipe class"
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_80_100.mp4",
    "text": " does not provide the implementation for these details. Instead, it declares these step methods abstract, meaning the subclasses will need to provide their implementation of these methods. Then, you create subclasses for each type of bread. For now, let's take wheat bread recipe and rye bread recipe. Each of these subclasses..."
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_100_120.mp4",
    "text": " in herits from bread recipe and provides its implementation of the details. For example, the white bread recipe might include specific ingredients and baking times unique to white bread. When the user needs to bake a specific type of bread, the application creates an instance of the corresponding subclass and calls its make bread method."
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_120_140.mp4",
    "text": " This method executes the steps in the predefined order, but the actual details of each step depend on the implementation in the subclass. There are only two actors in this design pattern, abstract class and concrete class. The first one is the base class that defines the template method and outlines the algorithm"
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_140_160.mp4",
    "text": " structure, it may also provide default implementations for some steps. The concrete class is a subclass of abstract class that provides concrete implementations for the steps defined in the template method. One of the key benefits of this pattern is code reuse. It promotes efficiency by enabling various subclasses to share a"
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_160_180.mp4",
    "text": " common structure. Additionally, it offers flexibility by enabling subclasses to overwrite specific steps or provide different ways of implementing certain behaviors. This is further supported by its ability to separate the high-level algorithm from the specific implementation details, ensuring a clear organization of concern"
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_180_200.mp4",
    "text": " However, there is also a significant drawback to using the template method pattern. Its maintenance can become very challenging. Changes to the base class impact all subclasses, so managing and updating the code without affecting multiple application areas can be tricky. That's all about the template method design."
  },
  {
    "clip_path": "ui/clips/6y_j4TqxDQc_200_202.mp4",
    "text": " Potter. See you in the next video."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_0_20.mp4",
    "text": " Imagine you are building a calculator up allowing users to enter some basic math expressions. Right now, you only want to implement plus, minus, and parentheses, but you will definitely want to add other operations in the future. Your expression language may be more challenging at some point, so you want your application to be well designed."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_20_40.mp4",
    "text": " Here, the interpreter design pattern is a good choice for you. The interpreter is a behavioral design pattern that provides a way to evaluate sentences or expressions in a language. It defines a language and its grammar in terms of a set of classes, where each class represents a rule or production of the language."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_40_60.mp4",
    "text": " This is useful when creating an interpreter for a simple language or expression, such as regular expressions, mathematical expressions, or domain specific languages. Let's start with the math expression interface with a single calculate method. Then, we will create four different classes inherited from math expression."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_60_80.mp4",
    "text": " plus expression, minus expression, brackets expression, and number. The first two classes will contain two math expressions because each of those classes is a binary operation, that is, we need two operands to perform our action correctly. Their calculate function calls the evaluation on both math expressions."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_80_100.mp4",
    "text": " and then returns their addition or subtraction. The bracket expression class contains one math expression and is used to group multiple math expressions into one. Its evaluation method returns the result of the calculated method of its single math expression. The last class, number, contains a number and its calculated method"
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_100_120.mp4",
    "text": " returns exactly that number. You can easily interpret any expression containing plus, minus, and brackets using these four classes. Now, to add new operations, you can add new classes representing this operation and the overall structure will not change from this. There are three actors in this design pattern."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_120_140.mp4",
    "text": " The first abstract expression or math expression in our case is an interface that declares an interpret, a method used by concrete expressions to interpret the input. In our example, interpret is calculate. Terminal expression is a subclass of abstract expression that implement the interpret method."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_140_160.mp4",
    "text": " for terminal characters in the grammar. In our case, it's just a number. Terminal means that this class does not contain any other abstract expression. The last one, non-terminal expression, is also a subclass of abstract expression that implements the interpret method for non-terminal characters in the..."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_160_180.mp4",
    "text": " grammar, it usually combines and interprets multiple terminal and non-terminal expressions. In our example, non-terminal expressions are plus expression, minus expression, and brackets expression. One of the main benefits of this design pattern is its modularity. It breaks down complex grammars into..."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_180_200.mp4",
    "text": " modular expressions, resulting in a more organized and maintainable codebase. This modularity also contributes to the system's extensibility, making it easier to change existing grammar or add new language constructs by adding or modifying expression classes. However, despite these benefits, the interpreter has one significant limitation."
  },
  {
    "clip_path": "ui/clips/Ia83UqTNApY_200_220.mp4",
    "text": " Its applicability is restricted, as it is most suitable for scenarios where there is a need to create an interpreter for languages, expressions or grammars. There also can be more efficient choices than this pattern for more complex scenarios. That's all about the interpreter design pattern. See you in the next video!"
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_0_20.mp4",
    "text": " Imagine that you are developing a strategy video game with several units. Archers, Cavalry and Spearman. Each unit can perform actions like attack, defend, move and so on. You want to implement the same action across all units without having to change the unit classes each time. Also, all the logic must be..."
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_20_40.mp4",
    "text": " being encapsulated and addition of new units and actions must be also implemented seamlessly. The visitor pattern is the best for this problem. The visitor is a behavioral design pattern that allows you to add new operations to existing classes without modifying their structure. It separates the operations."
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_40_60.mp4",
    "text": " from the classes they operate on by encapsulating them in visitor objects. This pattern is handy when dealing with a structured hierarchy of classes and multiple operations that need to be performed on those classes. First, we define a unit interface representing all types of units. This interface includes"
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_60_80.mp4",
    "text": " includes a method accept the key to the visitor pattern. Next, we have concrete classes for each unit type, implementing the unit interface, archer, cavalry, and spearman. Now, we define the unit visitor interface. This interface declares a visit method for each type of unit. We then..."
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_80_100.mp4",
    "text": " create concrete visitors for different actions. For example, in an attack action visitor, we should define how particular units attack. Finally, you can use a following pattern in your game. Here, each unit type can accept different actions, for example, attack without altering their classes. There are four..."
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_100_120.mp4",
    "text": " actors in this design pattern. First, the visitor interface, which declares the methods for each element type in the hierarchy. The concrete visitors are classes that implement the visitor and provide implementations for its methods. The element interface declares the accept method which accepts the visitor, and the concrete"
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_120_140.mp4",
    "text": " elements are classes that implement the element interface and its method. They receive the visitor and delegate the visit to the appropriate visitor method. One of the key advantages of the pattern is the separation of concerns. It separates operations from the element classes, enhancing clarity and maintaining the"
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_140_160.mp4",
    "text": " Additionally, it's highly extensible, allowing new operations to be added without changing existing element classes. These aligns with the OpenClosed principle as adding new visitors and operations doesn't require modifying existing code. Moreover, the visitor facilitates double dispatch."
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_160_180.mp4",
    "text": " by enabling the right method to be called based on both the element and the visitor. However, there are drawbacks. It can add complexity, especially when there are many elements and visitors to manage. Another issue is the potential for code duplication. Similar operations on different element types might result"
  },
  {
    "clip_path": "ui/clips/kyquNfJ-rME_180_199.mp4",
    "text": " in repeated code when multiple visitors are implemented. Furthermore, its applicability is limited. While it's highly valuable for complex structures and numerous operations, it may be excessive for more straightforward situations. That's all about the visitor design pattern. See you in the next video."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_0_20.mp4",
    "text": " All right, it is the 14th of March. We're here with awesome students. So this is a reminder of what we talked about in the very beginning. The Stacey model is the bomb. It's the most important thing when you're coming newly into agile to understand why agile and it's simple."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_20_40.mp4",
    "text": " Right? You can either be in a simple zone working on predictive stuff that you can plan out to the max or you could find yourself going a little bit more complicated, but still be able to use predictive methods until you hit full on complex and eventually full on our key maybe on some projects and that's where you need"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_40_60.mp4",
    "text": " agile. Agile is going to work best, way you need to experiment and do things in iteration. So we've established that. Last time we met, we also talked about this framework, which I called the process group Pentagon. It's really PMI's five process groups, 10 knowledge areas. But I just dressed it up and called it the process group."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_60_80.mp4",
    "text": " you're painting and we talked about that. Now regarding agile, there's a question I always ask people, I'm like, what is agile? And the right answer to it, I'm gonna type it on the screen so that everyone remembers what agile is. Agile is not a set of processes or tools, but there's one thing that"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_80_100.mp4",
    "text": " you need before you use these tools that can help you in the world of agile. The thing you need the most and that is really what agile is, it is a mind set. First and foremost, it's a mindset. If anyone says what is agile, tell them it's a state of mind."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_100_120.mp4",
    "text": " It's a state of mind of the following things. Agility. Everyone has a plan until they're punched in the mouth. Then what good is your plan? Euclis. And that's a quote from Mike Tyson. Agility is a mindset. You must have the mindset I'm going to adapt. I'm going to evolve. I'm going to move."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_120_140.mp4",
    "text": " I'm going to pivot as soon as I see the need to I will pivot as soon as I see the need to I will move Another thing about agile what it is is customer focus like hyper focused on your customers competitive advantage these are the hallmarks of agile"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_140_160.mp4",
    "text": " Okay, so are there tools and techniques and methods that people could use like Giro or Confluence or are there frameworks like Scrum or Combo or methods. Yes, they are, but that is not what agile is primarily primarily. It is a mind"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_160_180.mp4",
    "text": " mindset mindset mindset mindset so we need to have that in the back of our mind when we understand it's a mindset and we're able to embrace the values individuals and interactions most important that's value number one the humans and you look at the principles the very first principle is a human"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_180_200.mp4",
    "text": " related principle. And it simply states our highest priority is to satisfy the customer. That's it. So going into your exam, you're going to be obsessed with customer satisfaction, adding value to the customer, helping the customer get to their"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_200_220.mp4",
    "text": " optimum, whatever that change that the customer is asking for, you're going to be laser focused on giving your customer value. Agile is also about adapting as an individual, not just as a company, so responding to change. When you take a look at the iron triangle, remember in the world of Agile we"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_220_240.mp4",
    "text": " flip the iron triangle on its head and scope becomes flexible. We also talked about how in the world of agile we rely more on pulling work to ourselves than pushing work on people. We also talked about the fact that in agile we plan at"
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_240_260.mp4",
    "text": " various levels, strategy level, portfolio level, product level, release level, iteration level, and all the way down to the daily scrum, right? The day level we're planning all throughout. Last time we met, we jumped into the world of scrum and we began talking about what scrum is."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_260_280.mp4",
    "text": " lightweight, easy to understand framework. So you take that framework and you keep the framework intact, but you can add things to it. So when you're practicing scrum, the idea is for you to use it in its entirety. Don't throw things out. Don't say, oh, that roll, forget that. Let's..."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_280_300.mp4",
    "text": " No, don't do that. Use it in its entirety. You're gonna learn the purity of scrum and you're gonna get better and better. Now, when the time comes, you're gonna be able to tailor it accordingly, but don't begin tailoring it today. Scrum is just one of the many frameworks."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_300_320.mp4",
    "text": " in the world of agile. First of all, agile is a mindset. We have the principles and the values, and then we have ways that we can interpret what we know in the world of the manifesto to make it more pragmatic. So here's a little part of..."
  },
  {
    "clip_path": "ui/clips/NNNKz9LcfVw_320_329.mp4",
    "text": " When we put this together, scrum was the leader. It's still the leader. You can see the big part of the pie is scrum."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_0_20.mp4",
    "text": " Hi friends, in today's video I'm going to be reading the audio version of the agile manifesto. If you're taking your PMP, Kappem, or Scrum Master certification, then this video is for you, so you can listen to this on the go anytime 24-7, whether you're in your car, running errands, or if you just find it much easier to learn."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_20_40.mp4",
    "text": " and observe material using audio. This will be extremely helpful so you can easily remember the key concepts that are applicable to the Agile framework. So let's get started. Before we get started..."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_40_60.mp4",
    "text": " We're going to first read through the Agile manifesto, followed by the 12 key principles behind the Agile manifesto. And if you'd like to follow along, you can access the Agile manifesto using the link that I'll be sharing down below at AgileMenefesto.org. As a disclaimer, the entire manifesto, which I'll be reading, is copyright of the following..."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_60_80.mp4",
    "text": " authors which I'm showing on the screen. No copyright infringement is intended as this is meant for educational purposes only. All credit goes to the individuals who created the Agile manifesto and I express my thanks and complete gratitude for the development of this for helping us lead our projects in much more effective ways using the"
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_80_100.mp4",
    "text": " agile mindset. Per the agile manifesto link at agilemanifesto.org, the declaration may be freely copied in any form, but only in its entirety through this notice. Here is the manifesto for agile software development. We are uncovering better ways of developing software by doing it and helping others do"
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_100_120.mp4",
    "text": " it. Through this work, we have come to value individuals and interactions over processes and tools working software over comprehensive documentation, customer collaboration over contract negotiation, responding to change over following a plan. That is"
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_120_140.mp4",
    "text": " While there is value on the items on the right, we value the items on the left more. Here are the 12 key principles behind the Agile manifesto. We follow these principles. Our highest priority is to satisfy the customer through early and continuous delivery of valuable software."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_140_160.mp4",
    "text": " even late in development. Agile processes harness change for the customers' competitive advantage. Deliver working software frequently from a couple of weeks to a couple of months with a preference to the shorter time scale. Business people and developers must work together daily throughout the project."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_160_180.mp4",
    "text": " Build projects around motivated individuals, give them the environment and the support they need, and trust them to get the job done. The most efficient and effective method of conveying information to and within a development team is face-to-face conversation. Working software is the primary..."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_180_200.mp4",
    "text": " measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. Continuous attention to technical excellence and good design enhances agility. Simplicity, the art of maximum"
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_200_220.mp4",
    "text": " optimizing the amount of work not done is essential. The best architectures, requirements, and designs emerge from self-organizing teams. At regular intervals, the team reflects on how to become more effective than tunes and adjust its behavior accordingly. Now that we've covered the entire..."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_220_240.mp4",
    "text": " Agile Manifesto and the key Agile Principles, here's the question of the day for you. Out of the 12 Agile Principles, which one sticks the most out and resonates with you the most? Now I hope you found this audio guide helpful and if you're looking to take your PMP exam, don't forget to sign up for my PMP exam Mastery Bootcamp course where I go and"
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_240_260.mp4",
    "text": " much more detail with how to create your study plan and master the basics and fundamentals of Agile 101. So make sure to sign up over at alvindapym.com for which slash bootcamp. To learn more about the top 30 Agile vocabulary and key terms that you need to master as well as how to study the Agile Practice Guide, just click on the link in the description."
  },
  {
    "clip_path": "ui/clips/uvrQjT8Hng4_260_269.mp4",
    "text": " or tap the screen over here. Otherwise, this is Alvin helping you become certified in project management. Thank you so much for watching and we'll talk soon in the next video."
  }
]